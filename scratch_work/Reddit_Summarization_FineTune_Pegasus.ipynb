{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Reddit_Summarization_FineTune Pegasus.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Load Dataset"
      ],
      "metadata": {
        "id": "OMsWQXiz0tfX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## The dataset consists of 3,848,330 posts with an average length of 270 words for content, and 28 words for the summary.\n",
        "## Min = 20 words Max = 50 words\n",
        "import tensorflow_datasets as tfds \n",
        "data = tfds.load(name='reddit')"
      ],
      "metadata": {
        "id": "Fn8IS_Z20wly"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Import and Parse Dataset"
      ],
      "metadata": {
        "id": "bQMTsq1g0zTP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# all imports\n",
        "import os\n",
        "import re\n",
        "import tensorflow as tf\n",
        "import numpy as np \n",
        "import pandas as pd\n",
        "from pprint import pprint\n",
        "from tqdm import tqdm\n",
        "\n",
        "import tensorflow_datasets as tfds \n",
        "from transformers import TFAutoModelForSequenceClassification\n",
        "from transformers import PegasusTokenizer, TFPegasusForConditionalGeneration\n"
      ],
      "metadata": {
        "id": "QFArMLHMkWBG"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q transformers"
      ],
      "metadata": {
        "id": "BvCFsSly3tBC"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q sentencepiece"
      ],
      "metadata": {
        "id": "YbuKt1Rj3viy"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q datasets"
      ],
      "metadata": {
        "id": "mRz77Kr93xYu"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# so now data is in here\n",
        "# the data sits on disk, and not memory\n",
        "path = '/root/tensorflow_datasets/reddit/1.0.0/'\n",
        "os.chdir(path)\n",
        "\n",
        "file_names = os.listdir(path)\n",
        "metadata_files = [i for i in file_names if re.search('^(?!reddit-train)', i)]\n",
        "train_records = [i for i in file_names if re.search('^reddit-train', i)]\n",
        "\n",
        "# create small subset for inspecting\n",
        "train_records_small = train_records[:10]"
      ],
      "metadata": {
        "id": "amopp_XGxZSV"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Parsing it into a json format\n",
        "raw_dataset = tf.data.TFRecordDataset(train_records_small)\n",
        "\n",
        "# limit 2 examples\n",
        "for i, raw_record in enumerate(raw_dataset.take(1)):\n",
        "  example = tf.train.Example()\n",
        "  example.ParseFromString(raw_record.numpy())\n",
        "  print(example)\n",
        "  if i > 1:\n",
        "    break\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xXMKOMa3jvuq",
        "outputId": "e90ea5eb-6b28-41b2-941f-361786d942df"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "features {\n",
            "  feature {\n",
            "    key: \"author\"\n",
            "    value {\n",
            "      bytes_list {\n",
            "        value: \"moleculariant\"\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "  feature {\n",
            "    key: \"body\"\n",
            "    value {\n",
            "      bytes_list {\n",
            "        value: \"Japan is basically a thin strip of islands on the edge of Asia. While rich in culture, it is important to understand that overpopulation is likely, given that Japanese heritage is not only coveted by cultures across the globe, it is famous. When I say famous, I mean to express the concept that during and after WW2, America came to know and love the culture of Japan in a way we never understood prior to the government\\'s clashing. It makes sense now that while Japan remains a sovereign government, western ideas and ideologies have been adopted, yet the land mass of Japan cannot sustain this America/Euro proclivity, so the Japanese government has promoted breeding, and as a result has led to overpopulation.\\n\\nTL;DR Japanese people rock, but there is only so much Japan.\"\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "  feature {\n",
            "    key: \"content\"\n",
            "    value {\n",
            "      bytes_list {\n",
            "        value: \"Japan is basically a thin strip of islands on the edge of Asia. While rich in culture, it is important to understand that overpopulation is likely, given that Japanese heritage is not only coveted by cultures across the globe, it is famous. When I say famous, I mean to express the concept that during and after WW2, America came to know and love the culture of Japan in a way we never understood prior to the government\\'s clashing. It makes sense now that while Japan remains a sovereign government, western ideas and ideologies have been adopted, yet the land mass of Japan cannot sustain this America/Euro proclivity, so the Japanese government has promoted breeding, and as a result has led to overpopulation.\"\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "  feature {\n",
            "    key: \"id\"\n",
            "    value {\n",
            "      bytes_list {\n",
            "        value: \"cnv3hgu\"\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "  feature {\n",
            "    key: \"normalizedBody\"\n",
            "    value {\n",
            "      bytes_list {\n",
            "        value: \"Japan is basically a thin strip of islands on the edge of Asia. While rich in culture, it is important to understand that overpopulation is likely, given that Japanese heritage is not only coveted by cultures across the globe, it is famous. When I say famous, I mean to express the concept that during and after WW2, America came to know and love the culture of Japan in a way we never understood prior to the government\\'s clashing. It makes sense now that while Japan remains a sovereign government, western ideas and ideologies have been adopted, yet the land mass of Japan cannot sustain this America/Euro proclivity, so the Japanese government has promoted breeding, and as a result has led to overpopulation. \\n TL;DR Japanese people rock, but there is only so much Japan. \\n\"\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "  feature {\n",
            "    key: \"subreddit\"\n",
            "    value {\n",
            "      bytes_list {\n",
            "        value: \"offbeat\"\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "  feature {\n",
            "    key: \"subreddit_id\"\n",
            "    value {\n",
            "      bytes_list {\n",
            "        value: \"t5_2qh11\"\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "  feature {\n",
            "    key: \"summary\"\n",
            "    value {\n",
            "      bytes_list {\n",
            "        value: \"Japanese people rock, but there is only so much Japan.\"\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "}\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# parse the output by defining the var types\n",
        "# this seems like the best way so far.\n",
        "def parse_file(serialized_example):\n",
        "\n",
        "  file_scruct = {\n",
        "      'author': tf.io.FixedLenFeature([], tf.string),\n",
        "      'body': tf.io.FixedLenFeature([], tf.string),\n",
        "      'content': tf.io.FixedLenFeature([], tf.string),\n",
        "      'id': tf.io.FixedLenFeature([], tf.string),\n",
        "      'normalizedBody': tf.io.FixedLenFeature([], tf.string),\n",
        "      'subreddit': tf.io.FixedLenFeature([], tf.string),\n",
        "      'subreddit_id': tf.io.FixedLenFeature([], tf.string),\n",
        "      'summary': tf.io.FixedLenFeature([], tf.string),\n",
        "  }\n",
        "\n",
        "  example1 = tf.io.parse_single_example(serialized_example, file_scruct)\n",
        "  return example1\n",
        "\n",
        "# parse:\n",
        "dataset = tf.data.TFRecordDataset(train_records_small).map(parse_file)\n",
        "\n",
        "\n",
        "# print it out\n",
        "# this prints out too many\n",
        "# for item in dataset:\n",
        "#   print(item['author'], item['summary'], item['subreddit'])\n",
        "\n",
        "# just print a few\n",
        "for i, item in enumerate(dataset):\n",
        "  tup1 = (item['author'], item['body'], item['content'], item['summary'], item['subreddit'])\n",
        "  print(tup1)\n",
        "  if i >= 10:\n",
        "    break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WfZQ_r9ckAiG",
        "outputId": "6e7c475e-2a74-425b-f496-1177552c6238"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(<tf.Tensor: shape=(), dtype=string, numpy=b'moleculariant'>, <tf.Tensor: shape=(), dtype=string, numpy=b\"Japan is basically a thin strip of islands on the edge of Asia. While rich in culture, it is important to understand that overpopulation is likely, given that Japanese heritage is not only coveted by cultures across the globe, it is famous. When I say famous, I mean to express the concept that during and after WW2, America came to know and love the culture of Japan in a way we never understood prior to the government's clashing. It makes sense now that while Japan remains a sovereign government, western ideas and ideologies have been adopted, yet the land mass of Japan cannot sustain this America/Euro proclivity, so the Japanese government has promoted breeding, and as a result has led to overpopulation.\\n\\nTL;DR Japanese people rock, but there is only so much Japan.\">, <tf.Tensor: shape=(), dtype=string, numpy=b\"Japan is basically a thin strip of islands on the edge of Asia. While rich in culture, it is important to understand that overpopulation is likely, given that Japanese heritage is not only coveted by cultures across the globe, it is famous. When I say famous, I mean to express the concept that during and after WW2, America came to know and love the culture of Japan in a way we never understood prior to the government's clashing. It makes sense now that while Japan remains a sovereign government, western ideas and ideologies have been adopted, yet the land mass of Japan cannot sustain this America/Euro proclivity, so the Japanese government has promoted breeding, and as a result has led to overpopulation.\">, <tf.Tensor: shape=(), dtype=string, numpy=b'Japanese people rock, but there is only so much Japan.'>, <tf.Tensor: shape=(), dtype=string, numpy=b'offbeat'>)\n",
            "(<tf.Tensor: shape=(), dtype=string, numpy=b'CarloGesualdo'>, <tf.Tensor: shape=(), dtype=string, numpy=b'Depends on how you measure. Adjusting levels to make the nip-outline a little more clear indicates that the right nipple (including what appears to be the areola) is a circle with a radius of 8 or 9 pixels. If we measure the radius of clinton\\'s eye from the center of the pupil to the outside of the iris, I find get a circle with a radius of 6 or 7 pixels. Of course there are confounding factors at play here. Portman\\'s nipples appear to be slightly closer to the camera, and I\\'m measuring the radius of the shadow on cast on the shirt which is merely a correlate of nipple size.\\n\\ntl;dr Natalie\\'s nips are (pending further investigation) slightly larger than Hillary\\'s irises\\n\\nEDIT: By \"right\" nipple, I mean right with respect to the viewer. In other words, Natalie Portman\\'s left nipple\\n\\nEDIT EDIT: Further research has turned up the following information. This [study]( of 37 women aged 20-64 found an average areolar diameter of 4.6 cm (+/- 1.14 cm). Even more interesting, they found a proportion of 1:3.4 for Breast/Areola size. Thus if someone could find the approximate size of Natalie Portman\\'s breasts, we could make a more accurate estimate of the size of her areola. Regardless, considering that the average diameter of the human iris is only 1.2cm, Natalie Portman\\'s nips (areola included) are almost certainly larger than Hillary Clinton\\'s Irises'>, <tf.Tensor: shape=(), dtype=string, numpy=b\"Depends on how you measure. Adjusting levels to make the nip-outline a little more clear indicates that the right nipple (including what appears to be the areola) is a circle with a radius of 8 or 9 pixels. If we measure the radius of clinton's eye from the center of the pupil to the outside of the iris, I find get a circle with a radius of 6 or 7 pixels. Of course there are confounding factors at play here. Portman's nipples appear to be slightly closer to the camera, and I'm measuring the radius of the shadow on cast on the shirt which is merely a correlate of nipple size.\">, <tf.Tensor: shape=(), dtype=string, numpy=b'Natalie\\'s nips are (pending further investigation) slightly larger than Hillary\\'s irises \\n EDIT: By \"right\" nipple, I mean right with respect to the viewer. In other words, Natalie Portman\\'s left nipple \\n EDIT EDIT: Further research has turned up the following information. This  study . Even more interesting, they found a proportion of 1:3.4 for Breast/Areola size. Thus if someone could find the approximate size of Natalie Portman\\'s breasts, we could make a more accurate estimate of the size of her areola. Regardless, considering that the average diameter of the human iris is only 1.2cm, Natalie Portman\\'s nips (areola included) are almost certainly larger than Hillary Clinton\\'s Irises'>, <tf.Tensor: shape=(), dtype=string, numpy=b'pics'>)\n",
            "(<tf.Tensor: shape=(), dtype=string, numpy=b'basisvector'>, <tf.Tensor: shape=(), dtype=string, numpy=b'&gt; I think belief is the wrong word to use for scientific knowledge\\n\\nWhy?  Any scientific conclusion is a justified belief vetted by the scientific method.  We rarely know whether future work will reveal our conclusions to be correct, incomplete, or wrong.  In light of this, do you think it is technically wrong to use the word belief in place of knowledge, or that the common usage of the word risks misunderstanding?\\n\\n&gt; I\\'d call them models that accurately describe what is known, given available information.\\n\\nWhat you\\'ve described can either be justified belief or knowledge depending on whether or not the models are actually true.  In the event the models are an accurate subset of a more comprehensive model, they constitute knowledge so long as the assumptions used in creating the models are not violated (knowingly or unknowingly).  In the event that the models are not an accurate subset, they do not constitute knowledge.  Instead, once additional evidence comes to light to reveal the flaws, a new model will replace, rather than extend, the old one.\\n\\n&gt; No one needs to believe them\\n\\nI agree reality is not altered by one\\'s willingness or refusal to believe in its governing principles.  I don\\'t believe I\\'ve insinuated this is the case, but I\\'ll take this opportunity to make it clear where I stand on the issue in case there\\'s some confusion.\\n\\n&gt; So the difference between a belief and scientific knowledge is that scientific theories (once fully peer reviewed and considered) are accurate to within the known, currently observed data set, whereas a belief may not be,\\n\\nThis is the crux of the matter.  What is knowledge, and what is belief?  A good example to consider is the model of the atom.  While early efforts to characterize the atom may have been accurate enough to describe the macro effects of interacting materials, they were wrong.  Not wrong in the sense that they were incomplete; they were completely wrong.  The lesson is that although we are able to describe all observable phenomenon with a given model, it may still be completely wrong.  Furthermore, despite having a model that is completely wrong, it is still possible to predict outcomes of reactions, even reactions that have never been performed.\\n\\nSo why not just call this knowledge?  In my opinion, there is value in understanding the limitations of all disciplines, including science.  Scientists themselves take great care to avoid premature declaration of fact.  So, while I don\\'t actually take issue with calling scientific theories \"knowledge\", as it is knowledge of the outcome of past tests, I do dislike when people insist that we cannot consider scientific theories to be \"beliefs\".  They are organized knowledge of past tests that provide justified belief in predictions regarding different tests in the future.  And I\\'m not talking about \"we can\\'t be sure if I drop this cup it will fall just because it did yesterday\".  I\\'m talking about using the models derived from prior tests to extrapolate for various purposes.\\n\\n&gt; e.g. I believe that the earth is 6,000 years old, or I believe that all animal species were created at the same time and sub-populations cannot become new species over time.\\n\\nThese are completely different as they are not justified beliefs, assuming panrationalism.  The distinction between radical and unsubstantiated claims made by religious institutions (or anyone) and those made by scientists should be justification, not truth.  That is really my only point.  \\n\\n**TL;DR:  By focusing on \"knowledge vs. belief\" instead of \"justified belief vs unjustified belief\", one is drawn into a discussion of truth rather than justification.  This is a move in the wrong direction since we can rarely know whether a belief is true, but we can often know whether a belief is justified.  Just my two cents.**'>, <tf.Tensor: shape=(), dtype=string, numpy=b'I think belief is the wrong word to use for scientific knowledge \\n Why?  Any scientific conclusion is a justified belief vetted by the scientific method.  We rarely know whether future work will reveal our conclusions to be correct, incomplete, or wrong.  In light of this, do you think it is technically wrong to use the word belief in place of knowledge, or that the common usage of the word risks misunderstanding? \\n > I\\'d call them models that accurately describe what is known, given available information. \\n What you\\'ve described can either be justified belief or knowledge depending on whether or not the models are actually true.  In the event the models are an accurate subset of a more comprehensive model, they constitute knowledge so long as the assumptions used in creating the models are not violated (knowingly or unknowingly).  In the event that the models are not an accurate subset, they do not constitute knowledge.  Instead, once additional evidence comes to light to reveal the flaws, a new model will replace, rather than extend, the old one. \\n > No one needs to believe them \\n I agree reality is not altered by one\\'s willingness or refusal to believe in its governing principles.  I don\\'t believe I\\'ve insinuated this is the case, but I\\'ll take this opportunity to make it clear where I stand on the issue in case there\\'s some confusion. \\n > So the difference between a belief and scientific knowledge is that scientific theories (once fully peer reviewed and considered) are accurate to within the known, currently observed data set, whereas a belief may not be, \\n This is the crux of the matter.  What is knowledge, and what is belief?  A good example to consider is the model of the atom.  While early efforts to characterize the atom may have been accurate enough to describe the macro effects of interacting materials, they were wrong.  Not wrong in the sense that they were incomplete; they were completely wrong.  The lesson is that although we are able to describe all observable phenomenon with a given model, it may still be completely wrong.  Furthermore, despite having a model that is completely wrong, it is still possible to predict outcomes of reactions, even reactions that have never been performed. \\n So why not just call this knowledge?  In my opinion, there is value in understanding the limitations of all disciplines, including science.  Scientists themselves take great care to avoid premature declaration of fact.  So, while I don\\'t actually take issue with calling scientific theories \"knowledge\", as it is knowledge of the outcome of past tests, I do dislike when people insist that we cannot consider scientific theories to be \"beliefs\".  They are organized knowledge of past tests that provide justified belief in predictions regarding different tests in the future.  And I\\'m not talking about \"we can\\'t be sure if I drop this cup it will fall just because it did yesterday\".  I\\'m talking about using the models derived from prior tests to extrapolate for various purposes. \\n > e.g. I believe that the earth is 6,000 years old, or I believe that all animal species were created at the same time and sub-populations cannot become new species over time. \\n These are completely different as they are not justified beliefs, assuming panrationalism.  The distinction between radical and unsubstantiated claims made by religious institutions (or anyone) and those made by scientists should be justification, not truth.  That is really my only point.'>, <tf.Tensor: shape=(), dtype=string, numpy=b'By focusing on \"knowledge vs. belief\" instead of \"justified belief vs unjustified belief\", one is drawn into a discussion of truth rather than justification.  This is a move in the wrong direction since we can rarely know whether a belief is true, but we can often know whether a belief is justified.  Just my two cents.'>, <tf.Tensor: shape=(), dtype=string, numpy=b'DebateReligion'>)\n",
            "(<tf.Tensor: shape=(), dtype=string, numpy=b'Coopster034'>, <tf.Tensor: shape=(), dtype=string, numpy=b\"I do, all the time. The reason I'm asking here is two fold. \\n\\nOne is because I read through the thread that I linked in the original post and thought it was very interesting when it comes to a male's perspective. \\n\\nTwo is because my girlfriend is a little bit timid/shy/quiet. She routinely describes it as her biggest problem. We've been dating for 8 months now and she has been so much more open and talkative since we started dating, but IMO she still has a way to go. I'm willing to talk to her and ask her about about anything as well as willing to listen to her about anything. I would like to be able to bring things to her though. To make her enjoy and or appreciate something that she likes or would want, but she hasn't ever mentioned or thought of. \\n\\ntl;dr - I do talk to my girlfriend, but she's not overly open. I want others opinion so I can be the man she deserves to be with.\">, <tf.Tensor: shape=(), dtype=string, numpy=b\"I do, all the time. The reason I'm asking here is two fold. \\n One is because I read through the thread that I linked in the original post and thought it was very interesting when it comes to a male's perspective. \\n Two is because my girlfriend is a little bit timid/shy/quiet. She routinely describes it as her biggest problem. We've been dating for 8 months now and she has been so much more open and talkative since we started dating, but IMO she still has a way to go. I'm willing to talk to her and ask her about about anything as well as willing to listen to her about anything. I would like to be able to bring things to her though. To make her enjoy and or appreciate something that she likes or would want, but she hasn't ever mentioned or thought of.\">, <tf.Tensor: shape=(), dtype=string, numpy=b\"I do talk to my girlfriend, but she's not overly open. I want others opinion so I can be the man she deserves to be with.\">, <tf.Tensor: shape=(), dtype=string, numpy=b'AskReddit'>)\n",
            "(<tf.Tensor: shape=(), dtype=string, numpy=b'justwhatiwishedfor'>, <tf.Tensor: shape=(), dtype=string, numpy=b\"I've recently built my PC and due to that, my funds are a bit short. I don't think I can afford BF1 and with BF on sale right now, I was thinking I should get either BF4 or BFHardline. However, I wanted to know, is now a terrible time to buy either of those games with the thinking that the multiplayer base will drop significantly? \\n\\nI know there's still people playing BF2 / BF3, so I was thinking perhaps people will still be playing this when BF1 comes out? And if it is a good idea to buy BF4, should I also consider buying hardline? \\n\\ntl;dr: Will there still be a good multiplayer base for BF4 after bf1 comes out? And if yes, should I choose hardline or BF4? \">, <tf.Tensor: shape=(), dtype=string, numpy=b\"I've recently built my PC and due to that, my funds are a bit short. I don't think I can afford BF1 and with BF on sale right now, I was thinking I should get either BF4 or BFHardline. However, I wanted to know, is now a terrible time to buy either of those games with the thinking that the multiplayer base will drop significantly? \\n I know there's still people playing BF2 / BF3, so I was thinking perhaps people will still be playing this when BF1 comes out? And if it is a good idea to buy BF4, should I also consider buying hardline?\">, <tf.Tensor: shape=(), dtype=string, numpy=b'Will there still be a good multiplayer base for BF4 after bf1 comes out? And if yes, should I choose hardline or BF4?'>, <tf.Tensor: shape=(), dtype=string, numpy=b'battlefield_4'>)\n",
            "(<tf.Tensor: shape=(), dtype=string, numpy=b'maliknyc'>, <tf.Tensor: shape=(), dtype=string, numpy=b\"I'll make this brief,\\n\\nToday I am a Business Solutions Architect for a major company within the US (Fortune 50). I work primarily with CxO's and build programs and solutions that assist organizations in executing against the strategic business goals and imperatives. I am very experienced in Benefits Dependency Networking, Business Capability Mapping and Enterprise Architecture Frameworks (Zachman, TOGAF and DODAF). I have absolutely no problem in engaging with my clients in terms of gaining credibility and trust. I have probably touched ~50 Fortune 500 companies in the last year all with very positive feedback on my work.\\n\\nSo my questions is this: Is getting an MBA worth it for me? I have currently been accepted into my company's HiPo (High Performance) program where they pair us up with Executive for advanced one on one mentoring. My manager says he is fully supportive of me pursuing my MBA if I chose to do so. I don't think it will be beneficial now however if one day I plan to take the leap to Business Development within my company I may need it just to have a shot at the position. What do you guys think? Would this be worth my time?\\n\\nAny advice, criticism or feedback would be greatly appreciated\\xe2\\x80\\xa6\\n\\ntldr; I work with business executives today on strategic planning initiatives. I don't have an MBA and it has never come up as an issue. I great really positive feedback on my work. Should I still look to get my MBA? \">, <tf.Tensor: shape=(), dtype=string, numpy=b\"I'll make this brief, \\n Today I am a Business Solutions Architect for a major company within the US (Fortune 50). I work primarily with CxO's and build programs and solutions that assist organizations in executing against the strategic business goals and imperatives. I am very experienced in Benefits Dependency Networking, Business Capability Mapping and Enterprise Architecture Frameworks (Zachman, TOGAF and DODAF). I have absolutely no problem in engaging with my clients in terms of gaining credibility and trust. I have probably touched ~50 Fortune 500 companies in the last year all with very positive feedback on my work. \\n So my questions is this: Is getting an MBA worth it for me? I have currently been accepted into my company's HiPo (High Performance) program where they pair us up with Executive for advanced one on one mentoring. My manager says he is fully supportive of me pursuing my MBA if I chose to do so. I don't think it will be beneficial now however if one day I plan to take the leap to Business Development within my company I may need it just to have a shot at the position. What do you guys think? Would this be worth my time? \\n Any advice, criticism or feedback would be greatly appreciated\\xe2\\x80\\xa6\">, <tf.Tensor: shape=(), dtype=string, numpy=b\"I work with business executives today on strategic planning initiatives. I don't have an MBA and it has never come up as an issue. I great really positive feedback on my work. Should I still look to get my MBA?\">, <tf.Tensor: shape=(), dtype=string, numpy=b'MBA'>)\n",
            "(<tf.Tensor: shape=(), dtype=string, numpy=b'[deleted]'>, <tf.Tensor: shape=(), dtype=string, numpy=b\"So many matches and I have yet to play with a team that actually plays the objective! For starters, capture and defend some god damn flags when you're playing Control. You'll get kills along the way. There's another game mode for you if you're one of those people that don't even attempt to cap a flag the entire game. Also, please DO NOT flip the spawns! By this, I mean don't be greedy and try to cap the third flag just because you shat on the other team. You're going to end up leading your teammate that was defending the first flag to a surprise Guardian Gangbang while losing a flag in the process. And PLEASEEEEE use your radar. I have had so many unnecessary deaths because a teammate of mine simply jumped away when the enemy is literally whispering in their ear telling them their entire elaborate plan to slit my neck and hulk smash me to the Reef. The radar isn't there just for show. My kill streaks are in vain because my team and myself are unable to coordinate and capitalize on the situation.\\n\\nTL;DR I'm salty as fuck because I can't carry my teams 24/7. Please carry with me, guys. Team work is necessary.\">, <tf.Tensor: shape=(), dtype=string, numpy=b\"So many matches and I have yet to play with a team that actually plays the objective! For starters, capture and defend some god damn flags when you're playing Control. You'll get kills along the way. There's another game mode for you if you're one of those people that don't even attempt to cap a flag the entire game. Also, please DO NOT flip the spawns! By this, I mean don't be greedy and try to cap the third flag just because you shat on the other team. You're going to end up leading your teammate that was defending the first flag to a surprise Guardian Gangbang while losing a flag in the process. And PLEASEEEEE use your radar. I have had so many unnecessary deaths because a teammate of mine simply jumped away when the enemy is literally whispering in their ear telling them their entire elaborate plan to slit my neck and hulk smash me to the Reef. The radar isn't there just for show. My kill streaks are in vain because my team and myself are unable to coordinate and capitalize on the situation.\">, <tf.Tensor: shape=(), dtype=string, numpy=b\"I'm salty as fuck because I can't carry my teams 24/7. Please carry with me, guys. Team work is necessary.\">, <tf.Tensor: shape=(), dtype=string, numpy=b'DestinyTheGame'>)\n",
            "(<tf.Tensor: shape=(), dtype=string, numpy=b'bluesox'>, <tf.Tensor: shape=(), dtype=string, numpy=b'I feel just as sickened as the lot of you due to the condescending tone of most of the comments here. A great deal of effort went into making the image photorealistic, and it took 5 minutes to write a critique. By criticizing the work, you are validating its objective. It has obviously evoked some thought and emotion within us to elicit a response, no matter which direction your moral compass is pointing.\\n\\nPersonally, I feel that it reflects the current obsession with instant gratification and constant push toward hedonism that permeate American culture. It cuts out the romance and goes straight for the end product. The apparent tastelessness and lack of merit are indicative of the times we live in. There\\'s no longer any need to have character. We can just Google whatever we want to know. We don\\'t have to refine our memory or our personality anymore. The phone knows everything, so we are more free to pursue our basic desires. In doing so, we become empty inside like the gaping maw of the recipient. We yearn for substance, but have none ourselves. Our source of information, talking points, and entertainment is something we wear outside, in a pocket or on a belt clip. We spew \"knowledge\" that we don\\'t need to remember, reducing it to a sound bite we can use to impress our friends and acquaintances. The majority of interactions we have on a daily basis are as empty as the one implied in the painting.\\n\\nI can see that I disagree with the majority of opinions in this thread, but consider the works of Toulouse-Latrec. \"Whores\" have been a popular subject of artist for eons, and the styles used to depict them reflect the social atmosphere of the times. Porn is the biggest business in the world right now. It drives technological advances! Betamax and VHS each became obsolete when porn companies updated their means of distribution. Digital streaming and DLC didn\\'t start with YouTube and iTunes. I could go on, but I\\'ve said enough already.\\n\\nTL;DR: \"Open your mouth and close your eyes, and I\\'ll give you a big surprise!\"'>, <tf.Tensor: shape=(), dtype=string, numpy=b'I feel just as sickened as the lot of you due to the condescending tone of most of the comments here. A great deal of effort went into making the image photorealistic, and it took 5 minutes to write a critique. By criticizing the work, you are validating its objective. It has obviously evoked some thought and emotion within us to elicit a response, no matter which direction your moral compass is pointing. \\n Personally, I feel that it reflects the current obsession with instant gratification and constant push toward hedonism that permeate American culture. It cuts out the romance and goes straight for the end product. The apparent tastelessness and lack of merit are indicative of the times we live in. There\\'s no longer any need to have character. We can just Google whatever we want to know. We don\\'t have to refine our memory or our personality anymore. The phone knows everything, so we are more free to pursue our basic desires. In doing so, we become empty inside like the gaping maw of the recipient. We yearn for substance, but have none ourselves. Our source of information, talking points, and entertainment is something we wear outside, in a pocket or on a belt clip. We spew \"knowledge\" that we don\\'t need to remember, reducing it to a sound bite we can use to impress our friends and acquaintances. The majority of interactions we have on a daily basis are as empty as the one implied in the painting. \\n I can see that I disagree with the majority of opinions in this thread, but consider the works of Toulouse-Latrec. \"Whores\" have been a popular subject of artist for eons, and the styles used to depict them reflect the social atmosphere of the times. Porn is the biggest business in the world right now. It drives technological advances! Betamax and VHS each became obsolete when porn companies updated their means of distribution. Digital streaming and DLC didn\\'t start with YouTube and iTunes. I could go on, but I\\'ve said enough already.'>, <tf.Tensor: shape=(), dtype=string, numpy=b'Open your mouth and close your eyes, and I\\'ll give you a big surprise!\"'>, <tf.Tensor: shape=(), dtype=string, numpy=b'Art'>)\n",
            "(<tf.Tensor: shape=(), dtype=string, numpy=b'Greybeard29'>, <tf.Tensor: shape=(), dtype=string, numpy=b'I\\'m the dumb guy in this story. I drew up a table survey thing about how much sleep people got... I changed the overall question from weekly to nightly, but I didn\\'t change the increments... Teacher circles my last answer as \"Did you get 40 hours sleep last night\" and was absolutely wetting herself... So was the whole class, that was a fun lesson... TL;DR: I get more than enough sleep '>, <tf.Tensor: shape=(), dtype=string, numpy=b'I\\'m the dumb guy in this story. I drew up a table survey thing about how much sleep people got... I changed the overall question from weekly to nightly, but I didn\\'t change the increments... Teacher circles my last answer as \"Did you get 40 hours sleep last night\" and was absolutely wetting herself... So was the whole class, that was a fun lesson...'>, <tf.Tensor: shape=(), dtype=string, numpy=b'I get more than enough sleep'>, <tf.Tensor: shape=(), dtype=string, numpy=b'AskReddit'>)\n",
            "(<tf.Tensor: shape=(), dtype=string, numpy=b'Synthus'>, <tf.Tensor: shape=(), dtype=string, numpy=b\"I thought Control had the least space magic involved. It's just a signal pulse that requires further propagation by use of the mass relays. They deplete their eezo cores to do so and might suffer internal damage, but they maintain structural integrity.\\n\\nIn the context of the MEverse, I'd argue that it's a little difficult to separate intrinsic scientific development and development resulting from Reaper direction.\\n\\nOn Reaper technological inflience, I view it like having the answer sheet to a practice test. You know that looking at the answers will prevent you from learning nearly as much as you could, but it's extremely hard to resist the urge anyway. Access to the model answers also reduces the incentive to jump through the mental hoops involved in considering alternatives and eliminating options.\\n\\nI thought the splinter group deal was the Reapers trying to fragment any organized resistance and giving them a source of troops and intelligence that could also operate undercover. It makes sense from a military perspective, and it's no suprise that the Reapers would attempt this every cycle.\\n\\nI'm a masochist that way. The only tl;dr post that I perused on ME3's ending was the pile of tripe that someone splatted out on the Escapist forums defending the ending. It basically went 'lol literature major, 2deep4u' and claimed that the asspull of an ending sequence was thoroughly foreshadowed and was thematically coherent.\">, <tf.Tensor: shape=(), dtype=string, numpy=b\"I thought Control had the least space magic involved. It's just a signal pulse that requires further propagation by use of the mass relays. They deplete their eezo cores to do so and might suffer internal damage, but they maintain structural integrity. \\n In the context of the MEverse, I'd argue that it's a little difficult to separate intrinsic scientific development and development resulting from Reaper direction. \\n On Reaper technological inflience, I view it like having the answer sheet to a practice test. You know that looking at the answers will prevent you from learning nearly as much as you could, but it's extremely hard to resist the urge anyway. Access to the model answers also reduces the incentive to jump through the mental hoops involved in considering alternatives and eliminating options. \\n I thought the splinter group deal was the Reapers trying to fragment any organized resistance and giving them a source of troops and intelligence that could also operate undercover. It makes sense from a military perspective, and it's no suprise that the Reapers would attempt this every cycle. \\n I'm a masochist that way. The only\">, <tf.Tensor: shape=(), dtype=string, numpy=b\"post that I perused on ME3's ending was the pile of tripe that someone splatted out on the Escapist forums defending the ending. It basically went 'lol literature major, 2deep4u' and claimed that the asspull of an ending sequence was thoroughly foreshadowed and was thematically coherent.\">, <tf.Tensor: shape=(), dtype=string, numpy=b'masseffect'>)\n",
            "(<tf.Tensor: shape=(), dtype=string, numpy=b'scottvicious'>, <tf.Tensor: shape=(), dtype=string, numpy=b\"I mean honestly each game is so close and so exciting. Sure I don't know most of the players (except for the ex-Korean pros and Vasilli) but each game is a spectacle. I beg of you guys to check it out if you are awake at this time. Not to mention the casting is on point. \\n\\ntl;dr: LPL is the most entertaining LoL split this Spring, hands down.\">, <tf.Tensor: shape=(), dtype=string, numpy=b\"I mean honestly each game is so close and so exciting. Sure I don't know most of the players (except for the ex-Korean pros and Vasilli) but each game is a spectacle. I beg of you guys to check it out if you are awake at this time. Not to mention the casting is on point.\">, <tf.Tensor: shape=(), dtype=string, numpy=b'LPL is the most entertaining LoL split this Spring, hands down.'>, <tf.Tensor: shape=(), dtype=string, numpy=b'leagueoflegends'>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# get data ready for summarizing\n",
        "\n",
        "dataset = tf.data.TFRecordDataset(train_records_small).map(parse_file)\n",
        "dataset_dict = {'content': [], 'summary': []}\n",
        "\n",
        "decode_string = np.vectorize(lambda x: x.decode('utf-8'))\n",
        "\n",
        "for item in dataset: \n",
        "  dataset_dict['content'].append(decode_string(item['content'].numpy()))\n",
        "  dataset_dict['summary'].append(decode_string(item['summary'].numpy()))"
      ],
      "metadata": {
        "id": "9f0MBSnjkHWE"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Fine-tune Pegasus Model"
      ],
      "metadata": {
        "id": "rhCi7mze0-Ho"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Tokenize dataset to fine-tune model (https://huggingface.co/transformers/v4.9.2/training.html)\n",
        "tokenizer = PegasusTokenizer.from_pretrained(\"google/pegasus-xsum\")"
      ],
      "metadata": {
        "id": "27WpMQ1Xk_DJ"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##Is this step necessary?\n",
        "#def tokenize_function(examples):\n",
        "#    return tokenizer(examples[\"content\"], padding=\"max_length\", truncation=True)\n",
        "\n",
        "#tokenized_datasets = dataset_dict.map(tokenize_function)"
      ],
      "metadata": {
        "id": "mK_Bu6fNmFqg"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##Pretrain and compile model\n",
        "import tensorflow as tf\n",
        "from transformers import TFAutoModelForSequenceClassification\n",
        "from transformers import PegasusTokenizer, TFPegasusForConditionalGeneration\n",
        "\n",
        "pega_model = TFPegasusForConditionalGeneration.from_pretrained(\"google/pegasus-xsum\")\n",
        "tokenizer = PegasusTokenizer.from_pretrained(\"google/pegasus-xsum\")\n",
        "\n",
        "pega_model.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(learning_rate=5e-5),\n",
        "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "    metrics=tf.metrics.SparseCategoricalAccuracy(),\n",
        ")\n",
        "\n",
        "pega_model.fit(dataset_dict['content'], validation_data=dataset_dict['summary'], epochs=3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8EJL6MeTnxuZ",
        "outputId": "73294b3e-a784-45de-c627-2eec4259e204"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "All model checkpoint layers were used when initializing TFPegasusForConditionalGeneration.\n",
            "\n",
            "All the layers of TFPegasusForConditionalGeneration were initialized from the model checkpoint at google/pegasus-xsum.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFPegasusForConditionalGeneration for predictions without further training.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pega_model.save_pretrained(\"my_pega_model\")"
      ],
      "metadata": {
        "id": "lIcyPAEhzvEq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pega_model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YNh1lKftzGKr",
        "outputId": "0a736818-b1b0-45a8-d14b-4430031d5165"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"tf_pegasus_for_conditional_generation_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " model (TFPegasusMainLayer)  multiple                  569748480 \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 569,844,583\n",
            "Trainable params: 569,748,480\n",
            "Non-trainable params: 96,103\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Test Model"
      ],
      "metadata": {
        "id": "iYqeVoNn9JUZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = tokenizer(np.array2string(dataset_dict['content'][1]), max_length=1024, truncation=True, return_tensors=\"tf\")\n",
        "inputs['input_ids'].shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "44eEc-PB9KCL",
        "outputId": "58bb18d3-6384-4947-8839-390cfa22ac12"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TensorShape([1, 138])"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate Summary\n",
        "summary_ids = pega_model.generate(inputs[\"input_ids\"], \n",
        "                              num_beams=4,\n",
        "                              no_repeat_ngram_size=2,\n",
        "                              min_length=20,\n",
        "                              max_length=50)\n",
        "print(\"content:\")\n",
        "print(dataset_dict['content'][1])\n",
        "\n",
        "print(\"\\n\\true:\")\n",
        "print(dataset_dict['summary'][1])\n",
        "\n",
        "print(\"\\n\\nprediction:\")\n",
        "pprint(tokenizer.batch_decode(summary_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0], compact=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iUS8o4eS9SNL",
        "outputId": "57641d33-6d94-4c36-c585-a7350b448e3a"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\\content:\n",
            "Depends on how you measure. Adjusting levels to make the nip-outline a little more clear indicates that the right nipple (including what appears to be the areola) is a circle with a radius of 8 or 9 pixels. If we measure the radius of clinton's eye from the center of the pupil to the outside of the iris, I find get a circle with a radius of 6 or 7 pixels. Of course there are confounding factors at play here. Portman's nipples appear to be slightly closer to the camera, and I'm measuring the radius of the shadow on cast on the shirt which is merely a correlate of nipple size.\n",
            "\n",
            "\true:\n",
            "Natalie's nips are (pending further investigation) slightly larger than Hillary's irises \n",
            " EDIT: By \"right\" nipple, I mean right with respect to the viewer. In other words, Natalie Portman's left nipple \n",
            " EDIT EDIT: Further research has turned up the following information. This  study . Even more interesting, they found a proportion of 1:3.4 for Breast/Areola size. Thus if someone could find the approximate size of Natalie Portman's breasts, we could make a more accurate estimate of the size of her areola. Regardless, considering that the average diameter of the human iris is only 1.2cm, Natalie Portman's nips (areola included) are almost certainly larger than Hillary Clinton's Irises\n",
            "\n",
            "\n",
            "prediction:\n",
            "(\"What do you make of Natalie Portman's nipple shadow on the shirt of her \"\n",
            " 'latest film, Black Swan?')\n"
          ]
        }
      ]
    }
  ]
}