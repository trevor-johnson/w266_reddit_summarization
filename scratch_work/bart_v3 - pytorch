{"cells":[{"cell_type":"markdown","metadata":{"id":"X4cRE8IbIrIV"},"source":["If you're opening this Notebook on colab, you will probably need to install ðŸ¤— Transformers and ðŸ¤— Datasets as well as other dependencies. Uncomment the following cell and run it."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MOsHUjgdIrIW"},"outputs":[],"source":["from IPython.display import clear_output\n","! pip install datasets transformers rouge-score nltk\n","clear_output()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1549,"status":"ok","timestamp":1658117852241,"user":{"displayName":"Trevor Johnson","userId":"10875792761423186902"},"user_tz":420},"id":"mmmwZMOKNRNH","outputId":"a1e6f0b3-69af-4253-802a-758eeb44f7be"},"outputs":[{"name":"stderr","output_type":"stream","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n"]},{"data":{"text/plain":["True"]},"execution_count":2,"metadata":{},"output_type":"execute_result"}],"source":["import nltk\n","nltk.download('punkt')"]},{"cell_type":"markdown","metadata":{"id":"Ljlw-9kfLG2R"},"source":["If you're opening this notebook locally, make sure your environment has an install from the last version of those libraries.\n","\n","To be able to share your model with the community and generate results like the one shown in the picture below via the inference API, there are a few more steps to follow.\n","\n","First you have to store your authentication token from the Hugging Face website (sign up [here](https://huggingface.co/join) if you haven't already!) then execute the following cell and input your username and password:"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":299},"executionInfo":{"elapsed":254,"status":"ok","timestamp":1658117878613,"user":{"displayName":"Trevor Johnson","userId":"10875792761423186902"},"user_tz":420},"id":"rRrsQN2ULG2S","outputId":"5c8e1c84-54b1-4e89-9a78-6bd25d88791f"},"outputs":[{"name":"stdout","output_type":"stream","text":["Login successful\n","Your token has been saved to /root/.huggingface/token\n","\u001b[1m\u001b[31mAuthenticated through git-credential store but this isn't the helper defined on your machine.\n","You might have to re-authenticate when pushing to the Hugging Face Hub. Run the following command in your terminal in case you want to set this credential helper as the default\n","\n","git config --global credential.helper store\u001b[0m\n"]}],"source":["from huggingface_hub import notebook_login\n","\n","notebook_login()"]},{"cell_type":"markdown","metadata":{"id":"fyOW9HipLG2S"},"source":["Then you need to install Git-LFS. Uncomment the following instructions:"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2085,"status":"ok","timestamp":1658117890273,"user":{"displayName":"Trevor Johnson","userId":"10875792761423186902"},"user_tz":420},"id":"hGFgs9eKLG2S","outputId":"30588139-e685-4116-a163-6c80b1921bd1"},"outputs":[{"name":"stdout","output_type":"stream","text":["Reading package lists... Done\n","Building dependency tree       \n","Reading state information... Done\n","git-lfs is already the newest version (2.3.4-1).\n","The following package was automatically installed and is no longer required:\n","  libnvidia-common-460\n","Use 'apt autoremove' to remove it.\n","0 upgraded, 0 newly installed, 0 to remove and 49 not upgraded.\n"]}],"source":["!apt install git-lfs"]},{"cell_type":"markdown","metadata":{"id":"GjVHCq47LG2T"},"source":["Make sure your version of Transformers is at least 4.11.0 since the functionality was introduced in that version:"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":12,"status":"ok","timestamp":1658117890274,"user":{"displayName":"Trevor Johnson","userId":"10875792761423186902"},"user_tz":420},"id":"z4J6bR0bLG2T","outputId":"9b329188-fa6f-46a4-9834-556244d4f3b9"},"outputs":[{"name":"stdout","output_type":"stream","text":["4.20.1\n"]}],"source":["import transformers\n","\n","print(transformers.__version__)"]},{"cell_type":"markdown","metadata":{"id":"HFASsisvIrIb"},"source":["You can find a script version of this notebook to fine-tune your model in a distributed fashion using multiple GPUs or TPUs [here](https://github.com/huggingface/transformers/tree/master/examples/seq2seq)."]},{"cell_type":"markdown","metadata":{"id":"rEJBSTyZIrIb"},"source":["# Fine-tuning a model on a summarization task"]},{"cell_type":"markdown","metadata":{"id":"kTCFado4IrIc"},"source":["In this notebook, we will see how to fine-tune one of the [ðŸ¤— Transformers](https://github.com/huggingface/transformers) model for a summarization task. We will use the [XSum dataset](https://arxiv.org/pdf/1808.08745.pdf) (for extreme summarization) which contains BBC articles accompanied with single-sentence summaries.\n","\n","![Widget inference on a summarization task](https://github.com/huggingface/notebooks/blob/main/examples/images/summarization.png?raw=1)\n","\n","We will see how to easily load the dataset for this task using ðŸ¤— Datasets and how to fine-tune a model on it using the `Trainer` API."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sazsWTyhLG2U"},"outputs":[],"source":["# model_checkpoint = \"t5-small\"\n","model_checkpoint = 'facebook/bart-base' "]},{"cell_type":"markdown","metadata":{"id":"4RRkXuteIrIh"},"source":["This notebook is built to run  with any model checkpoint from the [Model Hub](https://huggingface.co/models) as long as that model has a sequence-to-sequence version in the Transformers library. Here we picked the [`t5-small`](https://huggingface.co/t5-small) checkpoint. "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DqqC_-ISid__"},"outputs":[],"source":[""]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6597,"status":"ok","timestamp":1658118099241,"user":{"displayName":"Trevor Johnson","userId":"10875792761423186902"},"user_tz":420},"id":"_KcZftQQho9-","outputId":"dbb2c2c6-9168-4309-d9b9-c007ea0d539d"},"outputs":[{"name":"stdout","output_type":"stream","text":["Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"]}],"source":["# skip the steps below and run my custom data setup\n","import os, re\n","import pandas as pd\n","from google.colab import drive\n","from sklearn.model_selection import train_test_split\n","import datasets\n","from datasets.dataset_dict import DatasetDict\n","from datasets import Dataset, load_metric, load_dataset\n","\n","\n","drive.mount('/content/gdrive')\n","# data_path =\"/content/gdrive/MyDrive/Classes/W266_NLP/w266_reddit_summarization/data/reddit_parquet/\"\n","data_path =\"/content/gdrive/MyDrive/w266/w266_reddit_summarization/data/reddit_parquet/\"\n","\n","os.chdir(data_path)\n","files = [i for i in os.listdir(data_path) if re.search(\"reddit_data\", i)]\n","df = pd.read_parquet(files[0])\n","df = df.iloc[:55000]\n","train, test = train_test_split(df, test_size=5/55, random_state=1)\n","\n","def group_subreddit(subreddit):\n","\n","  if subreddit in ['buildapc', 'LifeProTips', 'IAmA', 'DoesAnybodyElse',\n","                   ] or re.search('advice|ask|relationship|explain|question', subreddit.lower()):\n","    x = 'advice'\n","  elif subreddit in ['leagueoflegends', 'DotA2', 'starcraft', 'magicTCG', \n","                     'Guildwars2', 'DestinyTheGame', 'pcmasterrace', \n","                     'Planetside', 'rpg', 'pokemon', 'smashbros', 'swtor', \n","                     'runescape', 'battlefield3', 'DarkSouls2', 'LeagueofLegendsMeta', \n","                     'WorldofTanks', 'darksouls', 'gamedev', 'Minecraft', 'Diablo', \n","                     'DnD', 'skyrim', 'halo', 'PS4', 'xboxone', 'battlefield_4', \n","                     'ShouldIbuythisgame', 'Pathfinder_RPG', 'elderscrollsonline', \n","                     'Fallout', 'GrandTheftAutoV'\n","                     ] or re.search('gaming|games', subreddit.lower()):\n","    x = 'gaming'\n","  elif subreddit in ['tifu', 'TwoXChromosomes', 'offmychest', 'todayilearned', \n","                     'fffffffuuuuuuuuuuuu', 'TalesFromRetail', 'JusticePorn', \n","                     'confession']:\n","    x = 'story'\n","  elif re.search('funny|comedy|changemyview|news|politic|atheis|religion|christian|islam|mormon', subreddit.lower()):\n","    x = 'news/life'\n","  elif re.search('sport|baseball|soccer|golf|football|basketball|nfl|nba|mlb', subreddit.lower()):\n","    x = 'sports'\n","  elif re.search('pics|videos', subreddit.lower()):\n","    x = 'pics/videos'\n","  else:\n","    x = 'other'\n","  \n","  return x\n","\n","train['subreddit_group'] = train['subreddit'].map(group_subreddit)\n","test['subreddit_group'] = test['subreddit'].map(group_subreddit)\n","\n","all_data = DatasetDict({\n","    'train': Dataset.from_dict({\n","        'content': train['content'],\n","        'summary': train['summary'],\n","        'subreddit': train['subreddit'],\n","        'subreddit_group': train['subreddit_group']\n","    }), \n","\n","    'test': Dataset.from_dict({\n","        'content': test['content'],\n","        'summary': test['summary'],\n","        'subreddit': test['subreddit'],\n","        'subreddit_group': test['subreddit_group']\n","    })\n","})\n","\n","raw_datasets = all_data"]},{"cell_type":"markdown","metadata":{"id":"whPRbBNbIrIl"},"source":["## Loading the dataset"]},{"cell_type":"markdown","metadata":{"id":"W7QYTpxXIrIl"},"source":["We will use the [ðŸ¤— Datasets](https://github.com/huggingface/datasets) library to download the data and get the metric we need to use for evaluation (to compare our model to the benchmark). This can be easily done with the functions `load_dataset` and `load_metric`.  "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IreSlFmlIrIm"},"outputs":[],"source":["from datasets import load_dataset, load_metric\n","\n","# raw_datasets = load_dataset(\"xsum\")\n","# metric = load_metric(\"rouge\")"]},{"cell_type":"markdown","metadata":{"id":"RzfPtOMoIrIu"},"source":["The `dataset` object itself is [`DatasetDict`](https://huggingface.co/docs/datasets/package_reference/main_classes.html#datasetdict), which contains one key for the training, validation and test set:"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":171,"status":"ok","timestamp":1658118116772,"user":{"displayName":"Trevor Johnson","userId":"10875792761423186902"},"user_tz":420},"id":"GWiVUF0jIrIv","outputId":"d6e05c59-c86a-4d0b-d9ba-24dae1cc75be"},"outputs":[{"data":{"text/plain":["DatasetDict({\n","    train: Dataset({\n","        features: ['content', 'summary', 'subreddit', 'subreddit_group'],\n","        num_rows: 50000\n","    })\n","    test: Dataset({\n","        features: ['content', 'summary', 'subreddit', 'subreddit_group'],\n","        num_rows: 5000\n","    })\n","})"]},"execution_count":11,"metadata":{},"output_type":"execute_result"}],"source":["raw_datasets"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OzsC6KclMWwl"},"outputs":[],"source":["# SUBSET FOR NOW\n","# from datasets.dataset_dict import DatasetDict\n","# raw_datasets=DatasetDict({\n","#     'train': raw_datasets['train'].select(range(200)),\n","#     'validation': raw_datasets['validation'].select(range(20)),\n","#     'test': raw_datasets['test'].select(range(20))\n","# })\n"]},{"cell_type":"markdown","metadata":{"id":"u3EtYfeHIrIz"},"source":["To access an actual element, you need to select a split first, then give an index:"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":27,"status":"ok","timestamp":1658117247927,"user":{"displayName":"Trevor Johnson","userId":"10875792761423186902"},"user_tz":420},"id":"ZtdCvEVKMWKM","outputId":"f689fb1b-fdc8-4069-d053-ef35ef7ea37e"},"outputs":[{"data":{"text/plain":["DatasetDict({\n","    train: Dataset({\n","        features: ['document', 'summary', 'id'],\n","        num_rows: 200\n","    })\n","    validation: Dataset({\n","        features: ['document', 'summary', 'id'],\n","        num_rows: 20\n","    })\n","    test: Dataset({\n","        features: ['document', 'summary', 'id'],\n","        num_rows: 20\n","    })\n","})"]},"execution_count":11,"metadata":{},"output_type":"execute_result"}],"source":["raw_datasets"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":11,"status":"ok","timestamp":1658116818060,"user":{"displayName":"Trevor Johnson","userId":"10875792761423186902"},"user_tz":420},"id":"X6HrpprwIrIz","outputId":"63eb4bf7-7b52-4b8e-d44b-f93898da449d"},"outputs":[{"data":{"text/plain":["{'document': 'The full cost of damage in Newton Stewart, one of the areas worst affected, is still being assessed.\\nRepair work is ongoing in Hawick and many roads in Peeblesshire remain badly affected by standing water.\\nTrains on the west coast mainline face disruption due to damage at the Lamington Viaduct.\\nMany businesses and householders were affected by flooding in Newton Stewart after the River Cree overflowed into the town.\\nFirst Minister Nicola Sturgeon visited the area to inspect the damage.\\nThe waters breached a retaining wall, flooding many commercial properties on Victoria Street - the main shopping thoroughfare.\\nJeanette Tate, who owns the Cinnamon Cafe which was badly affected, said she could not fault the multi-agency response once the flood hit.\\nHowever, she said more preventative work could have been carried out to ensure the retaining wall did not fail.\\n\"It is difficult but I do think there is so much publicity for Dumfries and the Nith - and I totally appreciate that - but it is almost like we\\'re neglected or forgotten,\" she said.\\n\"That may not be true but it is perhaps my perspective over the last few days.\\n\"Why were you not ready to help us a bit more when the warning and the alarm alerts had gone out?\"\\nMeanwhile, a flood alert remains in place across the Borders because of the constant rain.\\nPeebles was badly hit by problems, sparking calls to introduce more defences in the area.\\nScottish Borders Council has put a list on its website of the roads worst affected and drivers have been urged not to ignore closure signs.\\nThe Labour Party\\'s deputy Scottish leader Alex Rowley was in Hawick on Monday to see the situation first hand.\\nHe said it was important to get the flood protection plan right but backed calls to speed up the process.\\n\"I was quite taken aback by the amount of damage that has been done,\" he said.\\n\"Obviously it is heart-breaking for people who have been forced out of their homes and the impact on businesses.\"\\nHe said it was important that \"immediate steps\" were taken to protect the areas most vulnerable and a clear timetable put in place for flood prevention plans.\\nHave you been affected by flooding in Dumfries and Galloway or the Borders? Tell us about your experience of the situation and how it was handled. Email us on selkirk.news@bbc.co.uk or dumfries@bbc.co.uk.',\n"," 'id': '35232142',\n"," 'summary': 'Clean-up operations are continuing across the Scottish Borders and Dumfries and Galloway after flooding caused by Storm Frank.'}"]},"execution_count":12,"metadata":{},"output_type":"execute_result"}],"source":["raw_datasets[\"train\"][0]"]},{"cell_type":"markdown","metadata":{"id":"WHUmphG3IrI3"},"source":["To get a sense of what the data looks like, the following function will show some examples picked randomly in the dataset."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"i3j8APAoIrI3"},"outputs":[],"source":["import datasets\n","import random\n","import pandas as pd\n","from IPython.display import display, HTML\n","\n","def show_random_elements(dataset, num_examples=5):\n","    assert num_examples <= len(dataset), \"Can't pick more elements than there are in the dataset.\"\n","    picks = []\n","    for _ in range(num_examples):\n","        pick = random.randint(0, len(dataset)-1)\n","        while pick in picks:\n","            pick = random.randint(0, len(dataset)-1)\n","        picks.append(pick)\n","    \n","    df = pd.DataFrame(dataset[picks])\n","    for column, typ in dataset.features.items():\n","        if isinstance(typ, datasets.ClassLabel):\n","            df[column] = df[column].transform(lambda i: typ.names[i])\n","    display(HTML(df.to_html()))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"elapsed":167,"status":"ok","timestamp":1658118152824,"user":{"displayName":"Trevor Johnson","userId":"10875792761423186902"},"user_tz":420},"id":"SZy5tRB_IrI7","outputId":"0d8ee0d2-efe8-41e5-dde4-334dd3a2d48a"},"outputs":[{"data":{"text/html":["<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>content</th>\n","      <th>summary</th>\n","      <th>subreddit</th>\n","      <th>subreddit_group</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>I know this seems odd, but consider the reasons why he ran after he hit you. From what it appears he did not intentionally hit you with his car, although he did intentionally flea the scene. If he did this with the intent of avoiding being held responsible for the unintentional damage caused, then you could argue he should be held criminally responsible (although in scale with the damage and harm caused). However, if he was running, instead, from the the police, as opposed to being held responsible for damages, because he was concerned about having to be thrown in the cage for something unethical the police would do to him if he had police contact. For instance, suspended driver's license, drug related warrants, unpaid traffic tickets (where no harm was caused), etc. While it isn't wise to run from the police, you could understand and even excuse the behavior of someone who fled an accident to avoid unjust state force against him. In disposition, you are being compensated for his damages, but if his intent was not to harm you, damage your property or evade compensating you for incidental damage/harm, then you probably should not assist in having him arrested.</td>\n","      <td>Consider that which is not obvious regarding why he fled and determine weather to help the police based on those grounds. \\n Edit: I accidentally a verb.</td>\n","      <td>Anarcho_Capitalism</td>\n","      <td>other</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>You know, you should be saving your money, especially if you are lucky enough to have no real financial responsibilties at the moment.  The younger you start saving, the better.  And I don't mean that in a, \"you're young so you have time but it would be cool if you started saving for emergencies or for your future!\" but in like a you should be saving your money now kind of way.  I'm really tired I hope that makes sense.</td>\n","      <td>don't spend 15 dollars on caffeine for one day.  Try putting just a couple bucks a week into your savings account or even a percentage of all your tips.  Low enough so that you won't miss the amount, but enough that it will be significant eventually, especially after you start adding more. \\n But also I know you won't actually do it cause I was 18 once too so whatevs</td>\n","      <td>KitchenConfidential</td>\n","      <td>other</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>My grandma once called my uncle at work to tell him \"my computer is on tv\". He had to go through questions like: do you mean they are selling your kind of computer on tv (she loves QVC) and when she kept insisting he had to problem solve a bit more. She was saying her screen was on the tv, \"it has the same trash bin thingy and I have that same picture too\" \\n True problem: she lives in a really small town, and the local cable tv guide station is run from some guys computer...I guess the tv guide program had crashed so it was displaying his destop the standard windows xp grassy hill scene. \\n Hehe gotta love grandmas...even if they constantly forward those gross chain emails with ppts attached ::sigh::</td>\n","      <td>grandma thought her computer screen magically was displaying on her tv...it was the local cable tv guide program that had crashed and was displaying a windows xp desktop instead</td>\n","      <td>AskReddit</td>\n","      <td>advice</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>you don't want to remember yourself as the person who denied a dying woman her last rites \\n I honestly don't believe that at all. Your actions have consequences and dying shouldn't mean you get cleared of all wrongdoings. She screwed him so badly that he had to go to Afganistan to make a living! I don't know what she did but he's going to be paying for what she did for awhile. \\n Also considering she's dying I doubt her trying to clear the mud will do most good. Remember this assumes she will actually try to look these people up, contact them and convince them what a heartless person she was. He has almost no way to check if she does it or not unless he talks to the people on the phone and thats not likely to happen. They all assume he's guilty and if she mentions she's dying they will just think he's heartless for making her do this. \\n He can't win. The only way he can really do anything good is to take himself out of the game she made which is what he is doing. The reply he sent her is all that was needed. He was honest but considering how people think if they confess their sins that it absolves them, I don't think she will suffer too much from guilt. She's dying and thats what's really making her suffer. \\n Really think about it. It shouldn't take you lying on your deathbed to try to make amends for people you wronged BUT then again she just wanted forgiveness. She probably didn't even think of trying to actually put in any effort to undo the hurt she caused.</td>\n","      <td>He has nothing to feel guilty about. Forgiveness is something you earn. It is NOT something you are automatically entitled to.</td>\n","      <td>AskReddit</td>\n","      <td>advice</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>Glad you liked it! I hope to be going after my M.S. in a year or so at the most. \\n I guess I'll just give you a guided tour to my thought process. As long winded as it is: \\n I remember the volcano in Indonesia from a prior post a couple months ago on some other subreddit and thought /r/geology would appreciate it. But I never really had much luck posting my own content so I forgot about it. But then yesterday I realized it was my cakeday and said to myself \"If ever there was a day you could do this, rumor is that would be today.\" After uploading a pic of that if I realized if I was going to go off about a pretty blue volcano, I could do better than just repost from another board. So I dug up some other pretty interesting geology facts I enjoyed learning. \\n My petrology professor discussed the carbonate volcano in Africa, and I found it very fascinating. It was always a cool geology fact that unfortunately can't be used on many people for a quick attention grabber - as a great deal of people I know couldn't tell you the difference between lava and magma...'so carbonate lava...is that supposed to mean something?' So then you try and explain the different kinds of lava composition and watch their eyes quickly glaze over. I knew you guys would like it, even if you already knew about it. \\n I've always thought it cool to think that there is an oceanic plate dragging the underside of our continent as it plunges into the mantle. It was a huge revelation for me in my structure class when we discussed it - I mean, I already knew about tectonics, but I think a lot of people forget about a plate once it goes under. That the subduction of a tectonic plate was half the story was a big shift in how I saw plate tectonics. \\n Antarctica was just a random factoid that popped into my head. Some seem to think that Antarctica was always frozen, when in fact for a great deal of its history it was much like modern-day Alaska. And then, of course, comes the follow up question as to why it isn't like that now. And Australia is a big reason for that, but not exclusively the culprit, I know. \\n And then we get to both the Vredefort and Sudbury impacts. I knew of them for some time, as I love astronomy as much as I love geology...and who doesn't love to hear about gigantic bollide impacts? To be perfectly honest, I would have pursued a B.S. in the space sciences if my school had offered a degree plan. But, it didn't, and I quickly found I loved geology just as much. That being said, I saw no reason I couldn't incorporate the two into my academic career. Whenever I was given liberty on a term paper or seminar project, I would always steer it towards a more planetary science topic.</td>\n","      <td>Learned most of it while an undergrad geology student, combined with a very healthy love of astronomy.</td>\n","      <td>geology</td>\n","      <td>other</td>\n","    </tr>\n","  </tbody>\n","</table>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"}],"source":["show_random_elements(raw_datasets[\"train\"])"]},{"cell_type":"markdown","metadata":{"id":"lnjDIuQ3IrI-"},"source":["The metric is an instance of [`datasets.Metric`](https://huggingface.co/docs/datasets/package_reference/main_classes.html#datasets.Metric):"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":692,"referenced_widgets":["90e33f97fe1b444094b1c03632b2182d","73c255048a854e63b44e2e25c314283f","1baea1173d664ccdb97e490d83a5f90a","602835e28b624c7fa751f4982f73b725","60f530870d104fb4887daa6ee8320ef6","bfd0f0886d734008a3e1fdafef5d4d10","7d8baea866f74727b9415b6ffc703e17","ba71dc30691d4324962655cafcfa0e28","92f9a608f48548a2b5c591f58629c87b","8754a37c0c014fa0955a5f717c90a722","a95970b5709448e7904fda70805d757f"]},"executionInfo":{"elapsed":634,"status":"ok","timestamp":1658118185983,"user":{"displayName":"Trevor Johnson","userId":"10875792761423186902"},"user_tz":420},"id":"5o4rUteaIrI_","outputId":"f320b9e4-25c9-4ff1-c79e-6ac3bcb72ed5"},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"90e33f97fe1b444094b1c03632b2182d","version_major":2,"version_minor":0},"text/plain":["Downloading builder script:   0%|          | 0.00/2.16k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"text/plain":["Metric(name: \"rouge\", features: {'predictions': Value(dtype='string', id='sequence'), 'references': Value(dtype='string', id='sequence')}, usage: \"\"\"\n","Calculates average rouge scores for a list of hypotheses and references\n","Args:\n","    predictions: list of predictions to score. Each prediction\n","        should be a string with tokens separated by spaces.\n","    references: list of reference for each prediction. Each\n","        reference should be a string with tokens separated by spaces.\n","    rouge_types: A list of rouge types to calculate.\n","        Valid names:\n","        `\"rouge{n}\"` (e.g. `\"rouge1\"`, `\"rouge2\"`) where: {n} is the n-gram based scoring,\n","        `\"rougeL\"`: Longest common subsequence based scoring.\n","        `\"rougeLSum\"`: rougeLsum splits text using `\"\n","\"`.\n","        See details in https://github.com/huggingface/datasets/issues/617\n","    use_stemmer: Bool indicating whether Porter stemmer should be used to strip word suffixes.\n","    use_aggregator: Return aggregates if this is set to True\n","Returns:\n","    rouge1: rouge_1 (precision, recall, f1),\n","    rouge2: rouge_2 (precision, recall, f1),\n","    rougeL: rouge_l (precision, recall, f1),\n","    rougeLsum: rouge_lsum (precision, recall, f1)\n","Examples:\n","\n","    >>> rouge = datasets.load_metric('rouge')\n","    >>> predictions = [\"hello there\", \"general kenobi\"]\n","    >>> references = [\"hello there\", \"general kenobi\"]\n","    >>> results = rouge.compute(predictions=predictions, references=references)\n","    >>> print(list(results.keys()))\n","    ['rouge1', 'rouge2', 'rougeL', 'rougeLsum']\n","    >>> print(results[\"rouge1\"])\n","    AggregateScore(low=Score(precision=1.0, recall=1.0, fmeasure=1.0), mid=Score(precision=1.0, recall=1.0, fmeasure=1.0), high=Score(precision=1.0, recall=1.0, fmeasure=1.0))\n","    >>> print(results[\"rouge1\"].mid.fmeasure)\n","    1.0\n","\"\"\", stored examples: 0)"]},"execution_count":16,"metadata":{},"output_type":"execute_result"}],"source":["from datasets import metric\n","metric = load_metric(\"rouge\")\n","metric"]},{"cell_type":"markdown","metadata":{"id":"jAWdqcUBIrJC"},"source":["You can call its `compute` method with your predictions and labels, which need to be list of decoded strings:"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":464,"status":"ok","timestamp":1658118190027,"user":{"displayName":"Trevor Johnson","userId":"10875792761423186902"},"user_tz":420},"id":"6XN1Rq0aIrJC","outputId":"ee05985e-f871-454e-86ca-556eede994a4"},"outputs":[{"data":{"text/plain":["{'rouge1': AggregateScore(low=Score(precision=1.0, recall=1.0, fmeasure=1.0), mid=Score(precision=1.0, recall=1.0, fmeasure=1.0), high=Score(precision=1.0, recall=1.0, fmeasure=1.0)),\n"," 'rouge2': AggregateScore(low=Score(precision=1.0, recall=1.0, fmeasure=1.0), mid=Score(precision=1.0, recall=1.0, fmeasure=1.0), high=Score(precision=1.0, recall=1.0, fmeasure=1.0)),\n"," 'rougeL': AggregateScore(low=Score(precision=1.0, recall=1.0, fmeasure=1.0), mid=Score(precision=1.0, recall=1.0, fmeasure=1.0), high=Score(precision=1.0, recall=1.0, fmeasure=1.0)),\n"," 'rougeLsum': AggregateScore(low=Score(precision=1.0, recall=1.0, fmeasure=1.0), mid=Score(precision=1.0, recall=1.0, fmeasure=1.0), high=Score(precision=1.0, recall=1.0, fmeasure=1.0))}"]},"execution_count":17,"metadata":{},"output_type":"execute_result"}],"source":["fake_preds = [\"hello there\", \"general kenobi\"]\n","fake_labels = [\"hello there\", \"general kenobi\"]\n","metric.compute(predictions=fake_preds, references=fake_labels)"]},{"cell_type":"markdown","metadata":{"id":"n9qywopnIrJH"},"source":["## Preprocessing the data"]},{"cell_type":"markdown","metadata":{"id":"YVx71GdAIrJH"},"source":["Before we can feed those texts to our model, we need to preprocess them. This is done by a ðŸ¤— Transformers `Tokenizer` which will (as the name indicates) tokenize the inputs (including converting the tokens to their corresponding IDs in the pretrained vocabulary) and put it in a format the model expects, as well as generate the other inputs that the model requires.\n","\n","To do all of this, we instantiate our tokenizer with the `AutoTokenizer.from_pretrained` method, which will ensure:\n","\n","- we get a tokenizer that corresponds to the model architecture we want to use,\n","- we download the vocabulary used when pretraining this specific checkpoint.\n","\n","That vocabulary will be cached, so it's not downloaded again the next time we run the cell."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":145,"referenced_widgets":["b4bbc234da26489da00dcc09d2bc0dbd","d5cfee387a9e44c8976acada7fb31c7e","9370161533694c829a54f79409503465","a2915cb4aee74ef4a1eeeb70ed120eac","69058e9ac98b47af8fd1101ddc79fec1","09efbe9b92414586ab625e33fa6b2af6","ccc4f3dceaee4975a39da2ffb483de82","588dc141f2d5412586adfa7d9c076524","9c6cdac8b9ab4442b4ebd927467cb8e1","f4e6b38f227641578e78444461d184fa","f57fcb803c1744fea3141cb55f380920","2b96b99bb9dc4639b2779e2bf2846760","930b7956e36c45e7af272715f12f82d1","faa20ab5681a4bb3a638a40192de3046","dd3e9bbdce8342959b40100e153fc393","dc08100d8c8144cc8975a039f7e589a7","19539ed5a5654ef187ecd7ef80e7db68","739fec51035f4b668c99f65bf711c86a","2cba9577673640b292ecbbd820e91d99","4c8d41de286946e6982f452b0eb4faff","e9679f20489e4764895e3fc30afe728d","26849dedd55644c09eba9b2d6e7f4477","eae923c93e9446dfb705c9ab2110fb26","83d7222a1db34f82ac7c2cde32b6d1db","d1821956c1474176a86871969d801ce8","ff433f3d436f47a1b5970d9d63f085f0","b506824d5cd449fe875ae3745eb58313","2b2d4c24206b4bf7b5aed9c03ce519b2","88050daa3d014ceb8b1abd683abca402","ac532ef32f6e47f797df68d691a4eb7e","3536eeb6a7214ccf8e3982e06ff4800c","b9620572988342f5bce9648df91400e0","ae3dba8427634d5d9864eb5e4873d433","8a0cabb89fb04265bc88d0bbe4a690c3","10316121353a490e88bb46fad9e62bbc","aad27a3ef2e2499db3afc65d33cbf1a2","225f970e5fcb4223b4ef3ab0b6949cdc","e222e3d7b8e84d71a5a2aa088db50efc","3ad40ca7262440e6861d4fc1d0cfdd3b","0ae125422de040f8b5ac548f0baa169d","fe45eb5edb4047209da8c657c8a53894","dc5c22843be8492781364748c9fdf27b","2865381884714810bcd9625e87e0aae1","2e90bbcedbb641a5b997738799617542"]},"executionInfo":{"elapsed":7469,"status":"ok","timestamp":1658118201791,"user":{"displayName":"Trevor Johnson","userId":"10875792761423186902"},"user_tz":420},"id":"eXNLu_-nIrJI","outputId":"bde03613-6a9a-40bf-9da9-88fff0f5cd89"},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"b4bbc234da26489da00dcc09d2bc0dbd","version_major":2,"version_minor":0},"text/plain":["Downloading:   0%|          | 0.00/1.68k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"2b96b99bb9dc4639b2779e2bf2846760","version_major":2,"version_minor":0},"text/plain":["Downloading:   0%|          | 0.00/878k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"eae923c93e9446dfb705c9ab2110fb26","version_major":2,"version_minor":0},"text/plain":["Downloading:   0%|          | 0.00/446k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"8a0cabb89fb04265bc88d0bbe4a690c3","version_major":2,"version_minor":0},"text/plain":["Downloading:   0%|          | 0.00/1.29M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"}],"source":["from transformers import AutoTokenizer\n","\n","tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)"]},{"cell_type":"markdown","metadata":{"id":"Vl6IidfdIrJK"},"source":["By default, the call above will use one of the fast tokenizers (backed by Rust) from the ðŸ¤— Tokenizers library."]},{"cell_type":"markdown","metadata":{"id":"rowT4iCLIrJK"},"source":["You can directly call this tokenizer on one sentence or a pair of sentences:"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":829,"status":"ok","timestamp":1658116850164,"user":{"displayName":"Trevor Johnson","userId":"10875792761423186902"},"user_tz":420},"id":"a5hBlsrHIrJL","outputId":"47333914-f862-4a0b-fa4d-e1952ce4d032"},"outputs":[{"data":{"text/plain":["{'input_ids': [0, 31414, 6, 42, 65, 3645, 328, 2], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1]}"]},"execution_count":15,"metadata":{},"output_type":"execute_result"}],"source":["tokenizer(\"Hello, this one sentence!\")"]},{"cell_type":"markdown","metadata":{"id":"qo_0B1M2IrJM"},"source":["Depending on the model you selected, you will see different keys in the dictionary returned by the cell above. They don't matter much for what we're doing here (just know they are required by the model we will instantiate later), you can learn more about them in [this tutorial](https://huggingface.co/transformers/preprocessing.html) if you're interested.\n","\n","Instead of one sentence, we can pass along a list of sentences:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IQlaBpELLG2X","outputId":"fb4894ba-809b-4965-b043-1a69fca4e732"},"outputs":[{"data":{"text/plain":["{'input_ids': [[8774, 6, 48, 80, 7142, 55, 1], [100, 19, 430, 7142, 5, 1]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1]]}"]},"execution_count":null,"metadata":{},"output_type":"execute_result"}],"source":["tokenizer([\"Hello, this one sentence!\", \"This is another sentence.\"])"]},{"cell_type":"markdown","metadata":{"id":"HLdcOFq_LG2X"},"source":["To prepare the targets for our model, we need to tokenize them inside the `as_target_tokenizer` context manager. This will make sure the tokenizer uses the special tokens corresponding to the targets:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"D_1JeiATLG2Y","outputId":"95ddc6fe-8415-4248-a6a0-79787f447a3e"},"outputs":[{"name":"stdout","output_type":"stream","text":["{'input_ids': [[8774, 6, 48, 80, 7142, 55, 1], [100, 19, 430, 7142, 5, 1]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1]]}\n"]}],"source":["with tokenizer.as_target_tokenizer():\n","    print(tokenizer([\"Hello, this one sentence!\", \"This is another sentence.\"]))"]},{"cell_type":"markdown","metadata":{"id":"2C0hcmp9IrJQ"},"source":["If you are using one of the five T5 checkpoints we have to prefix the inputs with \"summarize:\" (the model can also translate and it needs the prefix to know which task it has to perform)."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"M-4nlCUJLG2Y"},"outputs":[],"source":["if model_checkpoint in [\"t5-small\", \"t5-base\", \"t5-larg\", \"t5-3b\", \"t5-11b\"]:\n","    prefix = \"summarize: \"\n","else:\n","    prefix = \"\""]},{"cell_type":"markdown","metadata":{"id":"e_CmNC9WLG2Y"},"source":["We can then write the function that will preprocess our samples. We just feed them to the `tokenizer` with the argument `truncation=True`. This will ensure that an input longer that what the model selected can handle will be truncated to the maximum length accepted by the model. The padding will be dealt with later on (in a data collator) so we pad examples to the longest length in the batch and not the whole dataset."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vc0BSBLIIrJQ"},"outputs":[],"source":["max_input_length = 1024\n","max_target_length = 128\n","\n","def preprocess_function(examples):\n","    inputs = [prefix + doc for doc in examples[\"content\"]]\n","    model_inputs = tokenizer(inputs, max_length=max_input_length, truncation=True)\n","\n","    # Setup the tokenizer for targets\n","    with tokenizer.as_target_tokenizer():\n","        labels = tokenizer(examples[\"summary\"], max_length=max_target_length, truncation=True)\n","\n","    model_inputs[\"labels\"] = labels[\"input_ids\"]\n","    return model_inputs"]},{"cell_type":"markdown","metadata":{"id":"0lm8ozrJIrJR"},"source":["This function works with one or several examples. In the case of several examples, the tokenizer will return a list of lists for each key:"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":175,"status":"ok","timestamp":1658118251992,"user":{"displayName":"Trevor Johnson","userId":"10875792761423186902"},"user_tz":420},"id":"-b70jh26IrJS","outputId":"2138e014-7aa2-4cb6-d239-0c74030669f0"},"outputs":[{"data":{"text/plain":["{'input_ids': [[0, 100, 56, 1099, 36020, 22926, 77, 939, 21, 504, 734, 939, 74, 1877, 1481, 2838, 142, 9, 5, 1280, 9, 1925, 12741, 31, 127, 43870, 734, 206, 13625, 9316, 7933, 111, 996, 4, 125, 127, 3795, 156, 162, 213, 7, 5, 3299, 13, 10, 17735, 17591, 16572, 142, 79, 21, 3915, 24, 429, 28, 10, 16570, 50, 103, 1026, 9, 1668, 4, 2647, 6, 25, 5, 3299, 16, 784, 1792, 6721, 62, 69, 10802, 939, 553, 114, 24, 21, 164, 7, 28, 8661, 734, 79, 26, 10, 891, 9, 11248, 7, 1109, 225, 62, 5, 6711, 8, 146, 162, 12327, 10, 410, 4, 407, 79, 13593, 69, 865, 11, 53, 79, 64, 75, 120, 55, 87, 5, 4767, 9, 69, 8411, 11, 4, 407, 79, 4719, 2758, 162, 7, 12327, 8, 185, 1844, 31997, 5065, 150, 202, 667, 7, 120, 162, 7, 12327, 4, 264, 26, 103, 11248, 8, 939, 300, 55, 3473, 615, 14, 79, 21, 10, 455, 8411, 11, 4, 287, 939, 524, 39204, 2577, 11, 2400, 939, 802, 24, 74, 28, 6269, 7, 224, 10, 33228, 8018, 7, 244, 1108, 5, 11789, 939, 33, 127, 8411, 11, 110, 53, 7308, 4, 407, 5, 129, 631, 14, 115, 283, 88, 1508, 21, 584, 734, 22, 47123, 14, 2653, 205, 113, 1308, 19323, 15345, 9305, 6, 5, 3299, 16, 11, 4817, 8, 939, 524, 26458, 3804, 81, 11, 11708, 19124, 2], [0, 5234, 2459, 2115, 1050, 27109, 113, 25, 10, 936, 6, 142, 14, 16, 5700, 41, 23783, 21099, 169, 9, 2057, 10, 2430, 6287, 15, 932, 4, 345, 34, 7533, 57, 10, 881, 1800, 569, 177, 11, 750, 14, 630, 75, 14958, 1050, 16797, 4, 1437, 50118, 29339, 14, 189, 28, 1528, 6, 38, 95, 64, 75, 11390, 14, 82, 18644, 2115, 1050, 16797, 16, 10, 205, 631, 4, 1534, 89, 103, 2345, 9, 2394, 14, 97, 621, 679, 11, 116, 22375, 4, 5534, 6, 47, 64, 14958, 82, 18, 26253, 53, 45, 49, 40154, 6601, 116, 4820, 18, 5, 516, 7, 28, 4777, 116, 1437, 50118, 359, 282, 39596, 131, 1437, 50118, 38, 938, 75, 269, 5056, 7, 932, 11, 1989, 59, 5, 16334, 631, 6, 4420, 24, 16, 41, 696, 111, 53, 38, 21, 164, 7, 342, 10, 650, 2892, 15, 24, 4, 318, 47, 109, 215, 10, 631, 6, 2540, 95, 342, 66, 99, 110, 164, 7, 109, 4, 1806, 101, 573, 8, 5443, 6, 45, 383, 11, 26278, 8, 4983, 4, 1491, 95, 35, 5534, 6, 24, 40, 490, 1010, 13445, 4, 1437, 50118, 3029, 44218, 35, 50118, 34790, 6, 24, 21, 95, 10, 8306, 4, 3128, 38, 33, 1982, 62, 42, 233, 38, 5658, 492, 1533, 2643, 14, 38, 33, 802, 9, 4, 1525, 768, 6, 4318, 189, 73, 12488, 45, 28, 10676, 6, 4577, 50, 3680, 8, 82, 5658, 1169, 526, 19, 162, 6, 477, 66, 16855, 50, 11390, 24, 70, 561, 111, 8, 14, 18, 2051, 6, 38, 437, 45, 584, 38, 437, 5, 34150, 8, 33, 5, 275, 2956, 4, 38, 524, 7105, 102, 17107, 8, 42, 16, 1819, 65, 9, 5, 934, 8, 3544, 11111, 936, 7, 4190, 11, 1771, 26061, 4, 85, 40, 45, 28, 1365, 4, 1437, 50118, 440, 6, 47, 32, 45, 567, 160, 25, 10, 2378, 9902, 6, 5261, 16, 205, 6, 42, 16, 141, 2643, 283, 7, 1323, 6, 38, 33, 16855, 11, 127, 2053, 111, 97, 82, 1765, 106, 66, 64, 4190, 106, 4, 152, 889, 34, 57, 11, 5, 442, 13, 10, 150, 6, 8, 129, 5568, 13072, 142, 9, 5, 48596, 145, 98, 37540, 11, 128, 4783, 34821, 108, 8, 5, 17491, 530, 12673, 467, 4, 38, 2813, 209, 748, 37624, 45, 7, 28, 10, 2430, 631, 6, 98, 38, 524, 6404, 7, 5952, 464, 383, 51, 33, 3544, 1687, 4, 1437, 50118, 38, 619, 11, 6548, 25, 157, 25, 2302, 6, 683, 488, 50, 1022, 33, 57, 156, 6, 51, 32, 1341, 543, 7, 464, 50, 7213, 6, 25, 52, 33, 450, 19, 468, 8538, 4, 20, 435, 10279, 21, 9206, 448, 39951, 8, 56, 7, 28, 13, 10, 2007, 464, 4, 2]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], 'labels': [[0, 37460, 17735, 17591, 16572, 939, 802, 7, 1108, 5, 11789, 1825, 30, 224, 22, 9332, 673, 14, 2653, 205, 113, 2], [0, 22763, 4048, 2115, 82, 18, 22, 406, 29469, 113, 16, 45, 3035, 11, 127, 1040, 4, 1437, 50118, 3029, 26637, 782, 7, 28, 1415, 23, 6, 40, 1701, 110, 332, 111, 42, 21, 129, 10, 16007, 477, 8, 45, 2342, 293, 4183, 66, 4, 1437, 50118, 43789, 36773, 16, 92, 6, 2540, 1137, 110, 472, 10649, 13709, 14079, 141, 24, 40, 173, 45, 95, 17232, 1669, 6, 24, 18, 10, 2755, 4, 2]]}"]},"execution_count":23,"metadata":{},"output_type":"execute_result"}],"source":["preprocess_function(raw_datasets['train'][:2])"]},{"cell_type":"markdown","metadata":{"id":"zS-6iXTkIrJT"},"source":["To apply this function on all the pairs of sentences in our dataset, we just use the `map` method of our `dataset` object we created earlier. This will apply the function on all the elements of all the splits in `dataset`, so our training, validation and testing data will be preprocessed in one single command."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":81,"referenced_widgets":["7e1a77ff532749d8a7f8f53431a55c28","8811032c6666436b864ae30e7442b6ed","48f9f7eccb9d4b3692db22c5d2c5d216","6644ad1d2be048baa5636c6dda1f6094","75f03da0ae7e4043a5d42e767bef3d45","074cd3a234934af18b8678b29ae6fe19","ebda5f7f396447c785f72362df7aac58","2bab8c69de604ca6b224c3fd078cce2d","37879dc340a0498ebfab7671b01fe98c","4dc5c51f71124330bd6fccb9707d56e2","3ecdca77a28c4e36b834bcc7b5391335","bd00df4b769d4401b1264dc0ad4236e6","6cfb530dba6d466a969cd003e0ae805f","bb74f2bab2c2454fab5fef220ca79326","413bfa9619c7423cae3c5e10f838e7c2","6005b3f2c8e34608b0d6f10362200303","eb57b8243bfe4f84b682ac5c16ecdb8d","a01a52716f394e3aaa99d10cc7d5aee2","dee93bd19733408685197995af31acad","e52ea568032d431fb5d9054c67cbacfa","89cead58fd2e48b79478d72035617896","a82fc31aa62a4240be842fb9a0b1916e"]},"executionInfo":{"elapsed":19262,"status":"ok","timestamp":1658118273785,"user":{"displayName":"Trevor Johnson","userId":"10875792761423186902"},"user_tz":420},"id":"DDtsaJeVIrJT","outputId":"74962a56-c73e-43e5-9929-fb3322d1f557"},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"7e1a77ff532749d8a7f8f53431a55c28","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/50 [00:00<?, ?ba/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"bd00df4b769d4401b1264dc0ad4236e6","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/5 [00:00<?, ?ba/s]"]},"metadata":{},"output_type":"display_data"}],"source":["tokenized_datasets = raw_datasets.map(preprocess_function, batched=True)"]},{"cell_type":"markdown","metadata":{"id":"voWiw8C7IrJV"},"source":["Even better, the results are automatically cached by the ðŸ¤— Datasets library to avoid spending time on this step the next time you run your notebook. The ðŸ¤— Datasets library is normally smart enough to detect when the function you pass to map has changed (and thus requires to not use the cache data). For instance, it will properly detect if you change the task in the first cell and rerun the notebook. ðŸ¤— Datasets warns you when it uses cached files, you can pass `load_from_cache_file=False` in the call to `map` to not use the cached files and force the preprocessing to be applied again.\n","\n","Note that we passed `batched=True` to encode the texts by batches together. This is to leverage the full benefit of the fast tokenizer we loaded earlier, which will use multi-threading to treat the texts in a batch concurrently."]},{"cell_type":"markdown","metadata":{"id":"545PP3o8IrJV"},"source":["## Fine-tuning the model"]},{"cell_type":"markdown","metadata":{"id":"FBiW8UpKIrJW"},"source":["Now that our data is ready, we can download the pretrained model and fine-tune it. Since our task is of the sequence-to-sequence kind, we use the `AutoModelForSeq2SeqLM` class. Like with the tokenizer, the `from_pretrained` method will download and cache the model for us."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":49,"referenced_widgets":["f417cb142ede4722b5ad82b8ee357781","a4a35e25ddd54bee876a0e53f06947fe","e0bf2123f98a4985a2a51c206ff03879","fbb33edf1c424c6f8e23253403802cb0","c42e8555f98e4c70a726f168c7ccdf32","56d197b2009d4da4a04d9fe0ae2e1be8","a7eb94e977884dac9b3ce6efb5b511b9","ad0c51fbc2e44d7a8b802c957f075789","6e9f10e376004f4691ff1fa357ef3b4e","96f82a34b5f04c76badb222361acd337","16e76a6a0dfe4a299f19c8b38b7f4fcf"]},"executionInfo":{"elapsed":13116,"status":"ok","timestamp":1658118290601,"user":{"displayName":"Trevor Johnson","userId":"10875792761423186902"},"user_tz":420},"id":"TlqNaB8jIrJW","outputId":"a5134db5-395a-4222-f40d-0f34ab91a3a7"},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"f417cb142ede4722b5ad82b8ee357781","version_major":2,"version_minor":0},"text/plain":["Downloading:   0%|          | 0.00/532M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"}],"source":["from transformers import AutoModelForSeq2SeqLM, DataCollatorForSeq2Seq, Seq2SeqTrainingArguments, Seq2SeqTrainer\n","\n","model = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint)"]},{"cell_type":"markdown","metadata":{"id":"CczA5lJlIrJX"},"source":["Note that  we don't get a warning like in our classification example. This means we used all the weights of the pretrained model and there is no randomly initialized head in this case."]},{"cell_type":"markdown","metadata":{"id":"_N8urzhyIrJY"},"source":["To instantiate a `Seq2SeqTrainer`, we will need to define three more things. The most important is the [`Seq2SeqTrainingArguments`](https://huggingface.co/transformers/main_classes/trainer.html#transformers.Seq2SeqTrainingArguments), which is a class that contains all the attributes to customize the training. It requires one folder name, which will be used to save the checkpoints of the model, and all other arguments are optional:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"s3ifACgZLG2Z"},"outputs":[],"source":["batch_size = 4 # 16\n","model_name = model_checkpoint.split(\"/\")[-1]\n","args = Seq2SeqTrainingArguments(\n","    f\"{model_name}-finetuned-bart-round1\",\n","    evaluation_strategy = \"epoch\",\n","    learning_rate=2e-5,\n","    per_device_train_batch_size=batch_size,\n","    per_device_eval_batch_size=batch_size,\n","    weight_decay=0.01,\n","    save_total_limit=3,\n","    num_train_epochs=1,\n","    predict_with_generate=True,\n","    fp16=True,\n","    push_to_hub=True,\n",")"]},{"cell_type":"markdown","metadata":{"id":"km3pGVdTIrJc"},"source":["Here we set the evaluation to be done at the end of each epoch, tweak the learning rate, use the `batch_size` defined at the top of the cell and customize the weight decay. Since the `Seq2SeqTrainer` will save the model regularly and our dataset is quite large, we tell it to make three saves maximum. Lastly, we use the `predict_with_generate` option (to properly generate summaries) and activate mixed precision training (to go a bit faster).\n","\n","The last argument to setup everything so we can push the model to the [Hub](https://huggingface.co/models) regularly during training. Remove it if you didn't follow the installation steps at the top of the notebook. If you want to save your model locally in a name that is different than the name of the repository it will be pushed, or if you want to push your model under an organization and not your name space, use the `hub_model_id` argument to set the repo name (it needs to be the full name, including your namespace: for instance `\"sgugger/t5-finetuned-xsum\"` or `\"huggingface/t5-finetuned-xsum\"`).\n","\n","Then, we need a special kind of data collator, which will not only pad the inputs to the maximum length in the batch, but also the labels:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XmtXNRWZLG2Z"},"outputs":[],"source":["data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)"]},{"cell_type":"markdown","metadata":{"id":"7sZOdRlRIrJd"},"source":["The last thing to define for our `Seq2SeqTrainer` is how to compute the metrics from the predictions. We need to define a function for this, which will just use the `metric` we loaded earlier, and we have to do a bit of pre-processing to decode the predictions into texts:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UmvbnJ9JIrJd"},"outputs":[],"source":["import nltk\n","import numpy as np\n","\n","def compute_metrics(eval_pred):\n","    predictions, labels = eval_pred\n","    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n","    # Replace -100 in the labels as we can't decode them.\n","    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n","    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n","    \n","    # Rouge expects a newline after each sentence\n","    decoded_preds = [\"\\n\".join(nltk.sent_tokenize(pred.strip())) for pred in decoded_preds]\n","    decoded_labels = [\"\\n\".join(nltk.sent_tokenize(label.strip())) for label in decoded_labels]\n","    \n","    result = metric.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)\n","    # Extract a few results\n","    result = {key: value.mid.fmeasure * 100 for key, value in result.items()}\n","    \n","    # Add mean generated length\n","    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in predictions]\n","    result[\"gen_len\"] = np.mean(prediction_lens)\n","    \n","    return {k: round(v, 4) for k, v in result.items()}"]},{"cell_type":"markdown","metadata":{"id":"rXuFTAzDIrJe"},"source":["Then we just need to pass all of this along with our datasets to the `Seq2SeqTrainer`:"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":14527,"status":"ok","timestamp":1658118330830,"user":{"displayName":"Trevor Johnson","userId":"10875792761423186902"},"user_tz":420},"id":"imY1oC3SIrJf","outputId":"75a1f247-ffe0-4d54-a62c-09102e02109f"},"outputs":[{"name":"stderr","output_type":"stream","text":["Cloning https://huggingface.co/trevorj/bart-base-finetuned-bart-round1 into local empty directory.\n","Using cuda_amp half precision backend\n"]}],"source":["trainer = Seq2SeqTrainer(\n","    model,\n","    args,\n","    train_dataset=tokenized_datasets[\"train\"],\n","    eval_dataset=tokenized_datasets[\"test\"],\n","    data_collator=data_collator,\n","    tokenizer=tokenizer,\n","    compute_metrics=compute_metrics\n",")"]},{"cell_type":"markdown","metadata":{"id":"CdzABDVcIrJg"},"source":["We can now finetune our model by just calling the `train` method:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PTWaylcgjqXO"},"outputs":[],"source":["# started at 9:25pm. took about 1 hr\n","# trianing on 50k obs"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"elapsed":3213094,"status":"ok","timestamp":1658121546945,"user":{"displayName":"Trevor Johnson","userId":"10875792761423186902"},"user_tz":420},"id":"uNx5pyRlIrJh","outputId":"b37cc513-0af7-465b-8316-3a8011b8783e","scrolled":false},"outputs":[{"name":"stderr","output_type":"stream","text":["The following columns in the training set don't have a corresponding argument in `BartForConditionalGeneration.forward` and have been ignored: subreddit, content, subreddit_group, summary. If subreddit, content, subreddit_group, summary are not expected by `BartForConditionalGeneration.forward`,  you can safely ignore this message.\n","/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  FutureWarning,\n","***** Running training *****\n","  Num examples = 50000\n","  Num Epochs = 1\n","  Instantaneous batch size per device = 4\n","  Total train batch size (w. parallel, distributed & accumulation) = 4\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 12500\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='12500' max='12500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [12500/12500 53:30, Epoch 1/1]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Rouge1</th>\n","      <th>Rouge2</th>\n","      <th>Rougel</th>\n","      <th>Rougelsum</th>\n","      <th>Gen Len</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>3.523800</td>\n","      <td>3.262908</td>\n","      <td>15.669700</td>\n","      <td>4.155800</td>\n","      <td>13.341400</td>\n","      <td>13.780500</td>\n","      <td>15.843000</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Saving model checkpoint to bart-base-finetuned-bart-round1/checkpoint-500\n","Configuration saved in bart-base-finetuned-bart-round1/checkpoint-500/config.json\n","Model weights saved in bart-base-finetuned-bart-round1/checkpoint-500/pytorch_model.bin\n","tokenizer config file saved in bart-base-finetuned-bart-round1/checkpoint-500/tokenizer_config.json\n","Special tokens file saved in bart-base-finetuned-bart-round1/checkpoint-500/special_tokens_map.json\n","tokenizer config file saved in bart-base-finetuned-bart-round1/tokenizer_config.json\n","Special tokens file saved in bart-base-finetuned-bart-round1/special_tokens_map.json\n","Saving model checkpoint to bart-base-finetuned-bart-round1/checkpoint-1000\n","Configuration saved in bart-base-finetuned-bart-round1/checkpoint-1000/config.json\n","Model weights saved in bart-base-finetuned-bart-round1/checkpoint-1000/pytorch_model.bin\n","tokenizer config file saved in bart-base-finetuned-bart-round1/checkpoint-1000/tokenizer_config.json\n","Special tokens file saved in bart-base-finetuned-bart-round1/checkpoint-1000/special_tokens_map.json\n","Saving model checkpoint to bart-base-finetuned-bart-round1/checkpoint-1500\n","Configuration saved in bart-base-finetuned-bart-round1/checkpoint-1500/config.json\n","Model weights saved in bart-base-finetuned-bart-round1/checkpoint-1500/pytorch_model.bin\n","tokenizer config file saved in bart-base-finetuned-bart-round1/checkpoint-1500/tokenizer_config.json\n","Special tokens file saved in bart-base-finetuned-bart-round1/checkpoint-1500/special_tokens_map.json\n","Saving model checkpoint to bart-base-finetuned-bart-round1/checkpoint-2000\n","Configuration saved in bart-base-finetuned-bart-round1/checkpoint-2000/config.json\n","Model weights saved in bart-base-finetuned-bart-round1/checkpoint-2000/pytorch_model.bin\n","tokenizer config file saved in bart-base-finetuned-bart-round1/checkpoint-2000/tokenizer_config.json\n","Special tokens file saved in bart-base-finetuned-bart-round1/checkpoint-2000/special_tokens_map.json\n","Deleting older checkpoint [bart-base-finetuned-bart-round1/checkpoint-500] due to args.save_total_limit\n","Saving model checkpoint to bart-base-finetuned-bart-round1/checkpoint-2500\n","Configuration saved in bart-base-finetuned-bart-round1/checkpoint-2500/config.json\n","Model weights saved in bart-base-finetuned-bart-round1/checkpoint-2500/pytorch_model.bin\n","tokenizer config file saved in bart-base-finetuned-bart-round1/checkpoint-2500/tokenizer_config.json\n","Special tokens file saved in bart-base-finetuned-bart-round1/checkpoint-2500/special_tokens_map.json\n","Deleting older checkpoint [bart-base-finetuned-bart-round1/checkpoint-1000] due to args.save_total_limit\n","Saving model checkpoint to bart-base-finetuned-bart-round1/checkpoint-3000\n","Configuration saved in bart-base-finetuned-bart-round1/checkpoint-3000/config.json\n","Model weights saved in bart-base-finetuned-bart-round1/checkpoint-3000/pytorch_model.bin\n","tokenizer config file saved in bart-base-finetuned-bart-round1/checkpoint-3000/tokenizer_config.json\n","Special tokens file saved in bart-base-finetuned-bart-round1/checkpoint-3000/special_tokens_map.json\n","Deleting older checkpoint [bart-base-finetuned-bart-round1/checkpoint-1500] due to args.save_total_limit\n","Saving model checkpoint to bart-base-finetuned-bart-round1/checkpoint-3500\n","Configuration saved in bart-base-finetuned-bart-round1/checkpoint-3500/config.json\n","Model weights saved in bart-base-finetuned-bart-round1/checkpoint-3500/pytorch_model.bin\n","tokenizer config file saved in bart-base-finetuned-bart-round1/checkpoint-3500/tokenizer_config.json\n","Special tokens file saved in bart-base-finetuned-bart-round1/checkpoint-3500/special_tokens_map.json\n","Deleting older checkpoint [bart-base-finetuned-bart-round1/checkpoint-2000] due to args.save_total_limit\n","Saving model checkpoint to bart-base-finetuned-bart-round1/checkpoint-4000\n","Configuration saved in bart-base-finetuned-bart-round1/checkpoint-4000/config.json\n","Model weights saved in bart-base-finetuned-bart-round1/checkpoint-4000/pytorch_model.bin\n","tokenizer config file saved in bart-base-finetuned-bart-round1/checkpoint-4000/tokenizer_config.json\n","Special tokens file saved in bart-base-finetuned-bart-round1/checkpoint-4000/special_tokens_map.json\n","Deleting older checkpoint [bart-base-finetuned-bart-round1/checkpoint-2500] due to args.save_total_limit\n","Saving model checkpoint to bart-base-finetuned-bart-round1/checkpoint-4500\n","Configuration saved in bart-base-finetuned-bart-round1/checkpoint-4500/config.json\n","Model weights saved in bart-base-finetuned-bart-round1/checkpoint-4500/pytorch_model.bin\n","tokenizer config file saved in bart-base-finetuned-bart-round1/checkpoint-4500/tokenizer_config.json\n","Special tokens file saved in bart-base-finetuned-bart-round1/checkpoint-4500/special_tokens_map.json\n","Deleting older checkpoint [bart-base-finetuned-bart-round1/checkpoint-3000] due to args.save_total_limit\n","Saving model checkpoint to bart-base-finetuned-bart-round1/checkpoint-5000\n","Configuration saved in bart-base-finetuned-bart-round1/checkpoint-5000/config.json\n","Model weights saved in bart-base-finetuned-bart-round1/checkpoint-5000/pytorch_model.bin\n","tokenizer config file saved in bart-base-finetuned-bart-round1/checkpoint-5000/tokenizer_config.json\n","Special tokens file saved in bart-base-finetuned-bart-round1/checkpoint-5000/special_tokens_map.json\n","Deleting older checkpoint [bart-base-finetuned-bart-round1/checkpoint-3500] due to args.save_total_limit\n","Saving model checkpoint to bart-base-finetuned-bart-round1/checkpoint-5500\n","Configuration saved in bart-base-finetuned-bart-round1/checkpoint-5500/config.json\n","Model weights saved in bart-base-finetuned-bart-round1/checkpoint-5500/pytorch_model.bin\n","tokenizer config file saved in bart-base-finetuned-bart-round1/checkpoint-5500/tokenizer_config.json\n","Special tokens file saved in bart-base-finetuned-bart-round1/checkpoint-5500/special_tokens_map.json\n","Deleting older checkpoint [bart-base-finetuned-bart-round1/checkpoint-4000] due to args.save_total_limit\n","Saving model checkpoint to bart-base-finetuned-bart-round1/checkpoint-6000\n","Configuration saved in bart-base-finetuned-bart-round1/checkpoint-6000/config.json\n","Model weights saved in bart-base-finetuned-bart-round1/checkpoint-6000/pytorch_model.bin\n","tokenizer config file saved in bart-base-finetuned-bart-round1/checkpoint-6000/tokenizer_config.json\n","Special tokens file saved in bart-base-finetuned-bart-round1/checkpoint-6000/special_tokens_map.json\n","Deleting older checkpoint [bart-base-finetuned-bart-round1/checkpoint-4500] due to args.save_total_limit\n","Saving model checkpoint to bart-base-finetuned-bart-round1/checkpoint-6500\n","Configuration saved in bart-base-finetuned-bart-round1/checkpoint-6500/config.json\n","Model weights saved in bart-base-finetuned-bart-round1/checkpoint-6500/pytorch_model.bin\n","tokenizer config file saved in bart-base-finetuned-bart-round1/checkpoint-6500/tokenizer_config.json\n","Special tokens file saved in bart-base-finetuned-bart-round1/checkpoint-6500/special_tokens_map.json\n","Deleting older checkpoint [bart-base-finetuned-bart-round1/checkpoint-5000] due to args.save_total_limit\n","Saving model checkpoint to bart-base-finetuned-bart-round1/checkpoint-7000\n","Configuration saved in bart-base-finetuned-bart-round1/checkpoint-7000/config.json\n","Model weights saved in bart-base-finetuned-bart-round1/checkpoint-7000/pytorch_model.bin\n","tokenizer config file saved in bart-base-finetuned-bart-round1/checkpoint-7000/tokenizer_config.json\n","Special tokens file saved in bart-base-finetuned-bart-round1/checkpoint-7000/special_tokens_map.json\n","Deleting older checkpoint [bart-base-finetuned-bart-round1/checkpoint-5500] due to args.save_total_limit\n","Saving model checkpoint to bart-base-finetuned-bart-round1/checkpoint-7500\n","Configuration saved in bart-base-finetuned-bart-round1/checkpoint-7500/config.json\n","Model weights saved in bart-base-finetuned-bart-round1/checkpoint-7500/pytorch_model.bin\n","tokenizer config file saved in bart-base-finetuned-bart-round1/checkpoint-7500/tokenizer_config.json\n","Special tokens file saved in bart-base-finetuned-bart-round1/checkpoint-7500/special_tokens_map.json\n","Deleting older checkpoint [bart-base-finetuned-bart-round1/checkpoint-6000] due to args.save_total_limit\n","Saving model checkpoint to bart-base-finetuned-bart-round1/checkpoint-8000\n","Configuration saved in bart-base-finetuned-bart-round1/checkpoint-8000/config.json\n","Model weights saved in bart-base-finetuned-bart-round1/checkpoint-8000/pytorch_model.bin\n","tokenizer config file saved in bart-base-finetuned-bart-round1/checkpoint-8000/tokenizer_config.json\n","Special tokens file saved in bart-base-finetuned-bart-round1/checkpoint-8000/special_tokens_map.json\n","Deleting older checkpoint [bart-base-finetuned-bart-round1/checkpoint-6500] due to args.save_total_limit\n","Saving model checkpoint to bart-base-finetuned-bart-round1/checkpoint-8500\n","Configuration saved in bart-base-finetuned-bart-round1/checkpoint-8500/config.json\n","Model weights saved in bart-base-finetuned-bart-round1/checkpoint-8500/pytorch_model.bin\n","tokenizer config file saved in bart-base-finetuned-bart-round1/checkpoint-8500/tokenizer_config.json\n","Special tokens file saved in bart-base-finetuned-bart-round1/checkpoint-8500/special_tokens_map.json\n","Deleting older checkpoint [bart-base-finetuned-bart-round1/checkpoint-7000] due to args.save_total_limit\n","Saving model checkpoint to bart-base-finetuned-bart-round1/checkpoint-9000\n","Configuration saved in bart-base-finetuned-bart-round1/checkpoint-9000/config.json\n","Model weights saved in bart-base-finetuned-bart-round1/checkpoint-9000/pytorch_model.bin\n","tokenizer config file saved in bart-base-finetuned-bart-round1/checkpoint-9000/tokenizer_config.json\n","Special tokens file saved in bart-base-finetuned-bart-round1/checkpoint-9000/special_tokens_map.json\n","Deleting older checkpoint [bart-base-finetuned-bart-round1/checkpoint-7500] due to args.save_total_limit\n","Saving model checkpoint to bart-base-finetuned-bart-round1/checkpoint-9500\n","Configuration saved in bart-base-finetuned-bart-round1/checkpoint-9500/config.json\n","Model weights saved in bart-base-finetuned-bart-round1/checkpoint-9500/pytorch_model.bin\n","tokenizer config file saved in bart-base-finetuned-bart-round1/checkpoint-9500/tokenizer_config.json\n","Special tokens file saved in bart-base-finetuned-bart-round1/checkpoint-9500/special_tokens_map.json\n","Deleting older checkpoint [bart-base-finetuned-bart-round1/checkpoint-8000] due to args.save_total_limit\n","Saving model checkpoint to bart-base-finetuned-bart-round1/checkpoint-10000\n","Configuration saved in bart-base-finetuned-bart-round1/checkpoint-10000/config.json\n","Model weights saved in bart-base-finetuned-bart-round1/checkpoint-10000/pytorch_model.bin\n","tokenizer config file saved in bart-base-finetuned-bart-round1/checkpoint-10000/tokenizer_config.json\n","Special tokens file saved in bart-base-finetuned-bart-round1/checkpoint-10000/special_tokens_map.json\n","Deleting older checkpoint [bart-base-finetuned-bart-round1/checkpoint-8500] due to args.save_total_limit\n","Saving model checkpoint to bart-base-finetuned-bart-round1/checkpoint-10500\n","Configuration saved in bart-base-finetuned-bart-round1/checkpoint-10500/config.json\n","Model weights saved in bart-base-finetuned-bart-round1/checkpoint-10500/pytorch_model.bin\n","tokenizer config file saved in bart-base-finetuned-bart-round1/checkpoint-10500/tokenizer_config.json\n","Special tokens file saved in bart-base-finetuned-bart-round1/checkpoint-10500/special_tokens_map.json\n","Deleting older checkpoint [bart-base-finetuned-bart-round1/checkpoint-9000] due to args.save_total_limit\n","Saving model checkpoint to bart-base-finetuned-bart-round1/checkpoint-11000\n","Configuration saved in bart-base-finetuned-bart-round1/checkpoint-11000/config.json\n","Model weights saved in bart-base-finetuned-bart-round1/checkpoint-11000/pytorch_model.bin\n","tokenizer config file saved in bart-base-finetuned-bart-round1/checkpoint-11000/tokenizer_config.json\n","Special tokens file saved in bart-base-finetuned-bart-round1/checkpoint-11000/special_tokens_map.json\n","Deleting older checkpoint [bart-base-finetuned-bart-round1/checkpoint-9500] due to args.save_total_limit\n","Saving model checkpoint to bart-base-finetuned-bart-round1/checkpoint-11500\n","Configuration saved in bart-base-finetuned-bart-round1/checkpoint-11500/config.json\n","Model weights saved in bart-base-finetuned-bart-round1/checkpoint-11500/pytorch_model.bin\n","tokenizer config file saved in bart-base-finetuned-bart-round1/checkpoint-11500/tokenizer_config.json\n","Special tokens file saved in bart-base-finetuned-bart-round1/checkpoint-11500/special_tokens_map.json\n","Deleting older checkpoint [bart-base-finetuned-bart-round1/checkpoint-10000] due to args.save_total_limit\n","Saving model checkpoint to bart-base-finetuned-bart-round1/checkpoint-12000\n","Configuration saved in bart-base-finetuned-bart-round1/checkpoint-12000/config.json\n","Model weights saved in bart-base-finetuned-bart-round1/checkpoint-12000/pytorch_model.bin\n","tokenizer config file saved in bart-base-finetuned-bart-round1/checkpoint-12000/tokenizer_config.json\n","Special tokens file saved in bart-base-finetuned-bart-round1/checkpoint-12000/special_tokens_map.json\n","Deleting older checkpoint [bart-base-finetuned-bart-round1/checkpoint-10500] due to args.save_total_limit\n","Saving model checkpoint to bart-base-finetuned-bart-round1/checkpoint-12500\n","Configuration saved in bart-base-finetuned-bart-round1/checkpoint-12500/config.json\n","Model weights saved in bart-base-finetuned-bart-round1/checkpoint-12500/pytorch_model.bin\n","tokenizer config file saved in bart-base-finetuned-bart-round1/checkpoint-12500/tokenizer_config.json\n","Special tokens file saved in bart-base-finetuned-bart-round1/checkpoint-12500/special_tokens_map.json\n","Deleting older checkpoint [bart-base-finetuned-bart-round1/checkpoint-11000] due to args.save_total_limit\n","The following columns in the evaluation set don't have a corresponding argument in `BartForConditionalGeneration.forward` and have been ignored: subreddit, content, subreddit_group, summary. If subreddit, content, subreddit_group, summary are not expected by `BartForConditionalGeneration.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 5000\n","  Batch size = 4\n","\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n"]},{"data":{"text/plain":["TrainOutput(global_step=12500, training_loss=3.59469181640625, metrics={'train_runtime': 3212.8622, 'train_samples_per_second': 15.562, 'train_steps_per_second': 3.891, 'total_flos': 1.441009464201216e+16, 'train_loss': 3.59469181640625, 'epoch': 1.0})"]},"execution_count":30,"metadata":{},"output_type":"execute_result"}],"source":["trainer.train()"]},{"cell_type":"markdown","metadata":{"id":"c6TuiN-ULG2a"},"source":["You can now upload the result of the training to the Hub, just execute this instruction:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UKw-4Ol-24iu"},"outputs":[],"source":["# started at 1030 pm"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"q7GJVinALG2a","outputId":"d253816a-d02b-4647-bde0-de44f05182e8"},"outputs":[{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["Saving model checkpoint to bart-base-finetuned-bart-round1\n","Configuration saved in bart-base-finetuned-bart-round1/config.json\n","Model weights saved in bart-base-finetuned-bart-round1/pytorch_model.bin\n","tokenizer config file saved in bart-base-finetuned-bart-round1/tokenizer_config.json\n","Special tokens file saved in bart-base-finetuned-bart-round1/special_tokens_map.json\n","Several commits (2) will be pushed upstream.\n","The progress bars may be unreliable.\n"]}],"source":["trainer.push_to_hub()"]},{"cell_type":"markdown","metadata":{"id":"CUP4dk8KLG2a"},"source":["You can now share this model with all your friends, family, favorite pets: they can all load it with the identifier `\"your-username/the-name-you-picked\"` so for instance:\n","\n","```python\n","from transformers import AutoModelForSeq2SeqLM\n","\n","model = AutoModelForSeq2SeqLM.from_pretrained(\"sgugger/my-awesome-model\")\n","```"]},{"cell_type":"markdown","metadata":{"id":"QJp97NxTyJiT"},"source":["# Test it out:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tT9gD3N5LG2a"},"outputs":[],"source":["from transformers import AutoModelForSeq2SeqLM\n","\n","my_model = AutoModelForSeq2SeqLM.from_pretrained('trevorj/bart-base-finetuned-bart-round1')"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"machine_shape":"hm","name":"Summarization (PT - BART, reddit)","provenance":[{"file_id":"https://github.com/huggingface/notebooks/blob/main/examples/summarization.ipynb","timestamp":1658111950007}]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"074cd3a234934af18b8678b29ae6fe19":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"09efbe9b92414586ab625e33fa6b2af6":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0ae125422de040f8b5ac548f0baa169d":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"10316121353a490e88bb46fad9e62bbc":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_3ad40ca7262440e6861d4fc1d0cfdd3b","placeholder":"â€‹","style":"IPY_MODEL_0ae125422de040f8b5ac548f0baa169d","value":"Downloading: 100%"}},"16e76a6a0dfe4a299f19c8b38b7f4fcf":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"19539ed5a5654ef187ecd7ef80e7db68":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1baea1173d664ccdb97e490d83a5f90a":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_ba71dc30691d4324962655cafcfa0e28","max":2160,"min":0,"orientation":"horizontal","style":"IPY_MODEL_92f9a608f48548a2b5c591f58629c87b","value":2160}},"225f970e5fcb4223b4ef3ab0b6949cdc":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_2865381884714810bcd9625e87e0aae1","placeholder":"â€‹","style":"IPY_MODEL_2e90bbcedbb641a5b997738799617542","value":" 1.29M/1.29M [00:00&lt;00:00, 3.08MB/s]"}},"26849dedd55644c09eba9b2d6e7f4477":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"2865381884714810bcd9625e87e0aae1":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2b2d4c24206b4bf7b5aed9c03ce519b2":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2b96b99bb9dc4639b2779e2bf2846760":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_930b7956e36c45e7af272715f12f82d1","IPY_MODEL_faa20ab5681a4bb3a638a40192de3046","IPY_MODEL_dd3e9bbdce8342959b40100e153fc393"],"layout":"IPY_MODEL_dc08100d8c8144cc8975a039f7e589a7"}},"2bab8c69de604ca6b224c3fd078cce2d":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2cba9577673640b292ecbbd820e91d99":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2e90bbcedbb641a5b997738799617542":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"3536eeb6a7214ccf8e3982e06ff4800c":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"37879dc340a0498ebfab7671b01fe98c":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"3ad40ca7262440e6861d4fc1d0cfdd3b":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3ecdca77a28c4e36b834bcc7b5391335":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"413bfa9619c7423cae3c5e10f838e7c2":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_89cead58fd2e48b79478d72035617896","placeholder":"â€‹","style":"IPY_MODEL_a82fc31aa62a4240be842fb9a0b1916e","value":" 5/5 [00:01&lt;00:00,  2.97ba/s]"}},"48f9f7eccb9d4b3692db22c5d2c5d216":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_2bab8c69de604ca6b224c3fd078cce2d","max":50,"min":0,"orientation":"horizontal","style":"IPY_MODEL_37879dc340a0498ebfab7671b01fe98c","value":50}},"4c8d41de286946e6982f452b0eb4faff":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"4dc5c51f71124330bd6fccb9707d56e2":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"56d197b2009d4da4a04d9fe0ae2e1be8":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"588dc141f2d5412586adfa7d9c076524":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6005b3f2c8e34608b0d6f10362200303":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"602835e28b624c7fa751f4982f73b725":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_8754a37c0c014fa0955a5f717c90a722","placeholder":"â€‹","style":"IPY_MODEL_a95970b5709448e7904fda70805d757f","value":" 5.60k/? [00:00&lt;00:00, 195kB/s]"}},"60f530870d104fb4887daa6ee8320ef6":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6644ad1d2be048baa5636c6dda1f6094":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_4dc5c51f71124330bd6fccb9707d56e2","placeholder":"â€‹","style":"IPY_MODEL_3ecdca77a28c4e36b834bcc7b5391335","value":" 50/50 [00:17&lt;00:00,  2.74ba/s]"}},"69058e9ac98b47af8fd1101ddc79fec1":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6cfb530dba6d466a969cd003e0ae805f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_eb57b8243bfe4f84b682ac5c16ecdb8d","placeholder":"â€‹","style":"IPY_MODEL_a01a52716f394e3aaa99d10cc7d5aee2","value":"100%"}},"6e9f10e376004f4691ff1fa357ef3b4e":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"739fec51035f4b668c99f65bf711c86a":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"73c255048a854e63b44e2e25c314283f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_bfd0f0886d734008a3e1fdafef5d4d10","placeholder":"â€‹","style":"IPY_MODEL_7d8baea866f74727b9415b6ffc703e17","value":"Downloading builder script: "}},"75f03da0ae7e4043a5d42e767bef3d45":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7d8baea866f74727b9415b6ffc703e17":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"7e1a77ff532749d8a7f8f53431a55c28":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_8811032c6666436b864ae30e7442b6ed","IPY_MODEL_48f9f7eccb9d4b3692db22c5d2c5d216","IPY_MODEL_6644ad1d2be048baa5636c6dda1f6094"],"layout":"IPY_MODEL_75f03da0ae7e4043a5d42e767bef3d45"}},"83d7222a1db34f82ac7c2cde32b6d1db":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_2b2d4c24206b4bf7b5aed9c03ce519b2","placeholder":"â€‹","style":"IPY_MODEL_88050daa3d014ceb8b1abd683abca402","value":"Downloading: 100%"}},"8754a37c0c014fa0955a5f717c90a722":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"88050daa3d014ceb8b1abd683abca402":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"8811032c6666436b864ae30e7442b6ed":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_074cd3a234934af18b8678b29ae6fe19","placeholder":"â€‹","style":"IPY_MODEL_ebda5f7f396447c785f72362df7aac58","value":"100%"}},"89cead58fd2e48b79478d72035617896":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8a0cabb89fb04265bc88d0bbe4a690c3":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_10316121353a490e88bb46fad9e62bbc","IPY_MODEL_aad27a3ef2e2499db3afc65d33cbf1a2","IPY_MODEL_225f970e5fcb4223b4ef3ab0b6949cdc"],"layout":"IPY_MODEL_e222e3d7b8e84d71a5a2aa088db50efc"}},"90e33f97fe1b444094b1c03632b2182d":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_73c255048a854e63b44e2e25c314283f","IPY_MODEL_1baea1173d664ccdb97e490d83a5f90a","IPY_MODEL_602835e28b624c7fa751f4982f73b725"],"layout":"IPY_MODEL_60f530870d104fb4887daa6ee8320ef6"}},"92f9a608f48548a2b5c591f58629c87b":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"930b7956e36c45e7af272715f12f82d1":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_19539ed5a5654ef187ecd7ef80e7db68","placeholder":"â€‹","style":"IPY_MODEL_739fec51035f4b668c99f65bf711c86a","value":"Downloading: 100%"}},"9370161533694c829a54f79409503465":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_588dc141f2d5412586adfa7d9c076524","max":1716,"min":0,"orientation":"horizontal","style":"IPY_MODEL_9c6cdac8b9ab4442b4ebd927467cb8e1","value":1716}},"96f82a34b5f04c76badb222361acd337":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9c6cdac8b9ab4442b4ebd927467cb8e1":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"a01a52716f394e3aaa99d10cc7d5aee2":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"a2915cb4aee74ef4a1eeeb70ed120eac":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f4e6b38f227641578e78444461d184fa","placeholder":"â€‹","style":"IPY_MODEL_f57fcb803c1744fea3141cb55f380920","value":" 1.68k/1.68k [00:00&lt;00:00, 59.0kB/s]"}},"a4a35e25ddd54bee876a0e53f06947fe":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_56d197b2009d4da4a04d9fe0ae2e1be8","placeholder":"â€‹","style":"IPY_MODEL_a7eb94e977884dac9b3ce6efb5b511b9","value":"Downloading: 100%"}},"a7eb94e977884dac9b3ce6efb5b511b9":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"a82fc31aa62a4240be842fb9a0b1916e":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"a95970b5709448e7904fda70805d757f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"aad27a3ef2e2499db3afc65d33cbf1a2":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_fe45eb5edb4047209da8c657c8a53894","max":1355863,"min":0,"orientation":"horizontal","style":"IPY_MODEL_dc5c22843be8492781364748c9fdf27b","value":1355863}},"ac532ef32f6e47f797df68d691a4eb7e":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ad0c51fbc2e44d7a8b802c957f075789":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ae3dba8427634d5d9864eb5e4873d433":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"b4bbc234da26489da00dcc09d2bc0dbd":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_d5cfee387a9e44c8976acada7fb31c7e","IPY_MODEL_9370161533694c829a54f79409503465","IPY_MODEL_a2915cb4aee74ef4a1eeeb70ed120eac"],"layout":"IPY_MODEL_69058e9ac98b47af8fd1101ddc79fec1"}},"b506824d5cd449fe875ae3745eb58313":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b9620572988342f5bce9648df91400e0":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ba71dc30691d4324962655cafcfa0e28":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"bb74f2bab2c2454fab5fef220ca79326":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_dee93bd19733408685197995af31acad","max":5,"min":0,"orientation":"horizontal","style":"IPY_MODEL_e52ea568032d431fb5d9054c67cbacfa","value":5}},"bd00df4b769d4401b1264dc0ad4236e6":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_6cfb530dba6d466a969cd003e0ae805f","IPY_MODEL_bb74f2bab2c2454fab5fef220ca79326","IPY_MODEL_413bfa9619c7423cae3c5e10f838e7c2"],"layout":"IPY_MODEL_6005b3f2c8e34608b0d6f10362200303"}},"bfd0f0886d734008a3e1fdafef5d4d10":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c42e8555f98e4c70a726f168c7ccdf32":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ccc4f3dceaee4975a39da2ffb483de82":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"d1821956c1474176a86871969d801ce8":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_ac532ef32f6e47f797df68d691a4eb7e","max":456318,"min":0,"orientation":"horizontal","style":"IPY_MODEL_3536eeb6a7214ccf8e3982e06ff4800c","value":456318}},"d5cfee387a9e44c8976acada7fb31c7e":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_09efbe9b92414586ab625e33fa6b2af6","placeholder":"â€‹","style":"IPY_MODEL_ccc4f3dceaee4975a39da2ffb483de82","value":"Downloading: 100%"}},"dc08100d8c8144cc8975a039f7e589a7":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"dc5c22843be8492781364748c9fdf27b":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"dd3e9bbdce8342959b40100e153fc393":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e9679f20489e4764895e3fc30afe728d","placeholder":"â€‹","style":"IPY_MODEL_26849dedd55644c09eba9b2d6e7f4477","value":" 878k/878k [00:00&lt;00:00, 982kB/s]"}},"dee93bd19733408685197995af31acad":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e0bf2123f98a4985a2a51c206ff03879":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_ad0c51fbc2e44d7a8b802c957f075789","max":557771387,"min":0,"orientation":"horizontal","style":"IPY_MODEL_6e9f10e376004f4691ff1fa357ef3b4e","value":557771387}},"e222e3d7b8e84d71a5a2aa088db50efc":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e52ea568032d431fb5d9054c67cbacfa":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"e9679f20489e4764895e3fc30afe728d":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"eae923c93e9446dfb705c9ab2110fb26":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_83d7222a1db34f82ac7c2cde32b6d1db","IPY_MODEL_d1821956c1474176a86871969d801ce8","IPY_MODEL_ff433f3d436f47a1b5970d9d63f085f0"],"layout":"IPY_MODEL_b506824d5cd449fe875ae3745eb58313"}},"eb57b8243bfe4f84b682ac5c16ecdb8d":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ebda5f7f396447c785f72362df7aac58":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"f417cb142ede4722b5ad82b8ee357781":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_a4a35e25ddd54bee876a0e53f06947fe","IPY_MODEL_e0bf2123f98a4985a2a51c206ff03879","IPY_MODEL_fbb33edf1c424c6f8e23253403802cb0"],"layout":"IPY_MODEL_c42e8555f98e4c70a726f168c7ccdf32"}},"f4e6b38f227641578e78444461d184fa":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f57fcb803c1744fea3141cb55f380920":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"faa20ab5681a4bb3a638a40192de3046":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_2cba9577673640b292ecbbd820e91d99","max":898823,"min":0,"orientation":"horizontal","style":"IPY_MODEL_4c8d41de286946e6982f452b0eb4faff","value":898823}},"fbb33edf1c424c6f8e23253403802cb0":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_96f82a34b5f04c76badb222361acd337","placeholder":"â€‹","style":"IPY_MODEL_16e76a6a0dfe4a299f19c8b38b7f4fcf","value":" 532M/532M [00:08&lt;00:00, 65.7MB/s]"}},"fe45eb5edb4047209da8c657c8a53894":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ff433f3d436f47a1b5970d9d63f085f0":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_b9620572988342f5bce9648df91400e0","placeholder":"â€‹","style":"IPY_MODEL_ae3dba8427634d5d9864eb5e4873d433","value":" 446k/446k [00:00&lt;00:00, 893kB/s]"}}}}},"nbformat":4,"nbformat_minor":0}