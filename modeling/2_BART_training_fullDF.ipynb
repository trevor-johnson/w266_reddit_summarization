{"cells":[{"cell_type":"markdown","metadata":{"id":"LmCQLmSHDrXg"},"source":["# Fine tune single BART by combining all the subgroups together"]},{"cell_type":"markdown","metadata":{"id":"SaoqggbYD3F_"},"source":["# Setups"]},{"cell_type":"code","execution_count":1,"metadata":{"executionInfo":{"elapsed":28514,"status":"ok","timestamp":1658607634362,"user":{"displayName":"Trevor Johnson","userId":"10875792761423186902"},"user_tz":420},"id":"X2__lmSSDmsG"},"outputs":[],"source":["from IPython.display import clear_output\n","\n","!pip install datasets transformers rouge_score rouge-score nltk\n","# rouge-score is the google version\n","!pip install pyarrow\n","!pip install -q sentencepiece\n","\n","clear_output()"]},{"cell_type":"code","execution_count":2,"metadata":{"executionInfo":{"elapsed":14277,"status":"ok","timestamp":1658607648634,"user":{"displayName":"Trevor Johnson","userId":"10875792761423186902"},"user_tz":420},"id":"J0pxN5RGD5Zs"},"outputs":[],"source":["import os\n","import re\n","import time\n","from tqdm.notebook import trange, tqdm\n","import pandas as pd\n","import numpy as np\n","from pprint import pprint\n","import matplotlib.pyplot as plt\n","\n","# nlp stuff\n","import nltk\n","nltk.download('punkt')\n","\n","# tf stuff\n","import tensorflow_datasets as tfds \n","import tensorflow as tf\n","from transformers import PegasusTokenizer, TFPegasusForConditionalGeneration # pegasus\n","from transformers import BartTokenizer, TFBartForConditionalGeneration # bart\n","\n","# pytorch dataset types\n","import datasets\n","from datasets.dataset_dict import DatasetDict\n","from datasets import Dataset, load_metric, load_dataset\n","\n","# pytorch bart stuff\n","import torch\n","from transformers import AutoModelForSeq2SeqLM, DataCollatorForSeq2Seq, Seq2SeqTrainingArguments, Seq2SeqTrainer\n","from transformers import AutoTokenizer\n","\n","clear_output()"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":299,"referenced_widgets":["e4916d03793e44619a54548216c4381c","804592cb94544a74b1cab4f8d14b6fb3","874161a6ba6447cabac32f7b57e5c9b2","91515a2cf31345e4a8c536bbc30ce423","63f1381b2ad14a05a56178c1d87be274","863712b14c73466aad32c14b8b6d10e0","5786fb1e5ea041d5a1fd7338487f84f0","06682a88e16a4f50bf231c6207310638","3b4eca3725634087a7840e0a92722c6e","89af5249d84649f0aa2a74032e88da47","4d515b67918f46158926276af4879f0a","334b2b2e0a6f4269b6640cb3456a27e0","3ac7db03816d4f13915b006fce5c1786","e3d7cb3140374328af657c177b889807"]},"executionInfo":{"elapsed":189,"status":"ok","timestamp":1658607648796,"user":{"displayName":"Trevor Johnson","userId":"10875792761423186902"},"user_tz":420},"id":"asC0CF-lGDKW","outputId":"e6b7792c-4779-493f-9ade-991e74a74a7a"},"outputs":[{"name":"stdout","output_type":"stream","text":["Login successful\n","Your token has been saved to /root/.huggingface/token\n","\u001b[1m\u001b[31mAuthenticated through git-credential store but this isn't the helper defined on your machine.\n","You might have to re-authenticate when pushing to the Hugging Face Hub. Run the following command in your terminal in case you want to set this credential helper as the default\n","\n","git config --global credential.helper store\u001b[0m\n"]}],"source":["# sign into huggingface: https://huggingface.co/settings/tokens\n","from huggingface_hub import notebook_login\n","notebook_login()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ms8ZCQjjGeV1"},"outputs":[],"source":["#!apt install git-lfs"]},{"cell_type":"markdown","metadata":{"id":"tyI0wrDdD_VW"},"source":["# Load data"]},{"cell_type":"code","execution_count":4,"metadata":{"executionInfo":{"elapsed":165,"status":"ok","timestamp":1658607654601,"user":{"displayName":"Trevor Johnson","userId":"10875792761423186902"},"user_tz":420},"id":"2Kqkg1SZEKqN"},"outputs":[],"source":["# specify your path to the repo here:\n","repo_path = '/content/gdrive/MyDrive/w266/w266_reddit_summarization'"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":18939,"status":"ok","timestamp":1658607673960,"user":{"displayName":"Trevor Johnson","userId":"10875792761423186902"},"user_tz":420},"id":"r6MWx5d_D_7b","outputId":"e68f3d76-ba23-4ec7-a610-cbb0390cbb09"},"outputs":[{"name":"stdout","output_type":"stream","text":["Mounted at /content/gdrive\n","CPU times: user 1.17 s, sys: 357 ms, total: 1.53 s\n","Wall time: 19 s\n"]}],"source":["%%time\n","from google.colab import drive\n","drive.mount('/content/gdrive')\n","data_path = os.path.join(repo_path, 'data/reddit_parquet/train_test_split_v2')\n","os.chdir(data_path)\n","files = [i for i in os.listdir(data_path) if re.search(\"reddit\", i)]\n","\n","train = pd.read_parquet('reddit_train.parquet')\n","test = pd.read_parquet('reddit_test.parquet')\n","valid = pd.read_parquet('reddit_validation.parquet')"]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6,"status":"ok","timestamp":1658607673960,"user":{"displayName":"Trevor Johnson","userId":"10875792761423186902"},"user_tz":420},"id":"ZPGpW4IKE-1Y","outputId":"dae6f2af-e333-4943-fd13-04a951d66b87"},"outputs":[{"name":"stdout","output_type":"stream","text":["train\n","advice/story              15000\n","gaming                    15000\n","media/lifestyle/sports    15000\n","other                     15000\n","Name: subreddit_group, dtype: int64\n","\n","\n","test:\n","advice/story              1000\n","gaming                    1000\n","media/lifestyle/sports    1000\n","other                     1000\n","Name: subreddit_group, dtype: int64\n","\n","\n","valid:\n"]},{"data":{"text/plain":["advice/story              1000\n","gaming                    1000\n","media/lifestyle/sports    1000\n","other                     1000\n","Name: subreddit_group, dtype: int64"]},"execution_count":6,"metadata":{},"output_type":"execute_result"}],"source":["print(\"train\")\n","print(train['subreddit_group'].value_counts())\n","\n","print(\"\\n\\ntest:\")\n","print(test['subreddit_group'].value_counts())\n","\n","print(\"\\n\\nvalid:\")\n","valid['subreddit_group'].value_counts()"]},{"cell_type":"markdown","metadata":{"id":"ZV5-6hQiGi-B"},"source":["# Modeling"]},{"cell_type":"code","execution_count":7,"metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1658607675161,"user":{"displayName":"Trevor Johnson","userId":"10875792761423186902"},"user_tz":420},"id":"fgwErjJDZ70J"},"outputs":[],"source":["# bunch of diff checkpoints to consider\n","\n","# bart checkpoints\n","# model_checkpoint = 'facebook/bart-base' # keep returning the first sentence for me, extractive.\n","# model_checkpoint = 'facebook/bart-large-mnli' # same as above, only returns first sentences. extractive.\n","# model_checkpoint = 'sshleifer/distilbart-cnn-12-6' # works a bit better, but seems to produce extractive summaries still. \n","# model_checkpoint = 'sshleifer/distilbart-xsum-6-6' # was recommended. produces abstractive summaries p well. so far works the best of the above. \n","model_checkpoint = 'sshleifer/distilbart-xsum-6-6' # trained on both xsum and cnn/dm\n","\n","# pegasus checkpoints:\n","# model_checkpoint = \"google/pegasus-xsum\" # works really well\n","# model_checkpoint = 'google/pegasus-reddit_tifu' # also works really well"]},{"cell_type":"code","execution_count":8,"metadata":{"executionInfo":{"elapsed":17778,"status":"ok","timestamp":1658607693119,"user":{"displayName":"Trevor Johnson","userId":"10875792761423186902"},"user_tz":420},"id":"mdeOrmfCFM02"},"outputs":[],"source":["# load model, tokenizer, and rouge metric\n","# load diff checkpoint after it stopped fitting halfway through\n","tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n","model = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint)\n","metric = load_metric(\"rouge\")\n","\n","clear_output()"]},{"cell_type":"code","execution_count":9,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":209,"status":"ok","timestamp":1658607693323,"user":{"displayName":"Trevor Johnson","userId":"10875792761423186902"},"user_tz":420},"id":"JaH79IneFNXJ","outputId":"32cb64aa-5d49-440b-d77d-b986b177d634"},"outputs":[{"data":{"text/plain":["DatasetDict({\n","    train: Dataset({\n","        features: ['content', 'summary', 'subreddit'],\n","        num_rows: 60000\n","    })\n","    test: Dataset({\n","        features: ['content', 'summary', 'subreddit'],\n","        num_rows: 4000\n","    })\n","    valid: Dataset({\n","        features: ['content', 'summary', 'subreddit'],\n","        num_rows: 4000\n","    })\n","})"]},"execution_count":9,"metadata":{},"output_type":"execute_result"}],"source":["# convert data to torch Dataset.\n","raw_datasets = DatasetDict({\n","    'train': Dataset.from_dict({\n","        'content': train['content'],\n","        'summary': train['summary'],\n","        'subreddit': train['subreddit']\n","    }), \n","    'test': Dataset.from_dict({\n","        'content': test['content'],\n","        'summary': test['summary'],\n","        'subreddit': test['subreddit']\n","    }), \n","    'valid': Dataset.from_dict({\n","        'content': valid['content'],\n","        'summary': valid['summary'],\n","        'subreddit': valid['subreddit']\n","    })\n","})\n","\n","raw_datasets"]},{"cell_type":"code","execution_count":10,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":151,"referenced_widgets":["3fda19722e59409597470dd0110efc08","232f0a75ffc843218d3ce903bf69a756","8ac28d9522f647ab84d3785510f85cfe","ba52e8b08abe41c69cf8904014a12bc6","af94b304158d451f8224240f55ccd2cb","c04b3b3a3d7a403b90b919eb611ee925","b256eae206544d1bab1282d326debec7","f27234bffb604ebcb08871a1f651a3bc","334b1cbc12f14eb48b1f906f13eb5d1a","9250295910064effbb9ac75724721d62","7a4e6315b7514e6289d53eb6bf1a4675","3d7ab64789e645778443d3b16dd89685","29b082b7ef2d461ab6884d300990d7a0","7cbf1b99dc9e4a518a8906f88ed348ba","492de51cec0c484facd2c35c38c31154","84e260ce602e43c8a42b0cc87fc4fefa","d16c036d8d9d4c3c892edb7551b31e44","93a26a45055249a99e2b1537a90454d2","77d1121e412246a9add84d16a406384d","c82822cd5613489bba88b1ef81ee6b08","9b18efeac7d84f8e9d182129d8787a8d","97b585d3fdd8480b8789cfc2b30be260","7e976547c2aa4a44ac55a6e8907786a0","27ee43e81888408aa874a569a0de57e0","9974be5e870e48d181b8b7eb0f37a2e8","a3201ef5d93e4a758bb96d18c84abf44","4ec242f9663b47ab89beaf10d505a22b","4a31a7ef703e46b3b562c82148c526b4","3c2bb074cf394352b6a7c0c5083be919","4957975f68a645d8b0b6386c2ec5d873","ec86077a2ff54d23985501daabffe813","414449a196a44a6094ca6d7496ccd4de","4d3eefdcf1db43bf9a723d49ebcdf5a6"]},"executionInfo":{"elapsed":51561,"status":"ok","timestamp":1658607744874,"user":{"displayName":"Trevor Johnson","userId":"10875792761423186902"},"user_tz":420},"id":"3vyhmnSZIIjS","outputId":"81597330-459a-4d6a-8bac-8afef954565a"},"outputs":[{"name":"stderr","output_type":"stream","text":["Parameter 'function'=<function preprocess_function at 0x7fd33c624200> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"3fda19722e59409597470dd0110efc08","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/60 [00:00<?, ?ba/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"3d7ab64789e645778443d3b16dd89685","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/4 [00:00<?, ?ba/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"7e976547c2aa4a44ac55a6e8907786a0","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/4 [00:00<?, ?ba/s]"]},"metadata":{},"output_type":"display_data"}],"source":["# tokenize everything\n","max_input_length = 1024\n","max_target_length = 128\n","\n","def preprocess_function(examples):\n","    inputs = [doc for doc in examples[\"content\"]]\n","    model_inputs = tokenizer(inputs, max_length=max_input_length, truncation=True)\n","\n","    # Setup the tokenizer for targets\n","    with tokenizer.as_target_tokenizer():\n","        labels = tokenizer(examples[\"summary\"], max_length=max_target_length, truncation=True)\n","\n","    model_inputs[\"labels\"] = labels[\"input_ids\"]\n","    return model_inputs\n","\n","tokenized_datasets = raw_datasets.map(preprocess_function, batched=True)"]},{"cell_type":"code","execution_count":11,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":15,"status":"ok","timestamp":1658607744875,"user":{"displayName":"Trevor Johnson","userId":"10875792761423186902"},"user_tz":420},"id":"xWqYi08uwanD","outputId":"07ed93a3-7b0f-4142-a3ba-31783527ec9f"},"outputs":[{"data":{"text/plain":["DatasetDict({\n","    train: Dataset({\n","        features: ['content', 'summary', 'subreddit', 'input_ids', 'attention_mask', 'labels'],\n","        num_rows: 60000\n","    })\n","    test: Dataset({\n","        features: ['content', 'summary', 'subreddit', 'input_ids', 'attention_mask', 'labels'],\n","        num_rows: 4000\n","    })\n","    valid: Dataset({\n","        features: ['content', 'summary', 'subreddit', 'input_ids', 'attention_mask', 'labels'],\n","        num_rows: 4000\n","    })\n","})"]},"execution_count":11,"metadata":{},"output_type":"execute_result"}],"source":["tokenized_datasets"]},{"cell_type":"code","execution_count":12,"metadata":{"executionInfo":{"elapsed":13,"status":"ok","timestamp":1658607744875,"user":{"displayName":"Trevor Johnson","userId":"10875792761423186902"},"user_tz":420},"id":"PLMagSErJLmh"},"outputs":[],"source":["data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)\n","\n","def compute_metrics(eval_pred):\n","    predictions, labels = eval_pred\n","    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n","    # Replace -100 in the labels as we can't decode them.\n","    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n","    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n","    \n","    # Rouge expects a newline after each sentence\n","    decoded_preds = [\"\\n\".join(nltk.sent_tokenize(pred.strip())) for pred in decoded_preds]\n","    decoded_labels = [\"\\n\".join(nltk.sent_tokenize(label.strip())) for label in decoded_labels]\n","    \n","    result = metric.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)\n","    # Extract a few results\n","    result = {key: value.mid.fmeasure * 100 for key, value in result.items()}\n","    \n","    # Add mean generated length\n","    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in predictions]\n","    result[\"gen_len\"] = np.mean(prediction_lens)\n","    \n","    return {k: round(v, 4) for k, v in result.items()}"]},{"cell_type":"markdown","metadata":{"id":"khYnVUR0_rBZ"},"source":["# Train full dataset\n","- epochs=3, obs=60k, batch_size=8. Took 4:50 hrs"]},{"cell_type":"code","execution_count":16,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"elapsed":17404983,"status":"ok","timestamp":1658625353708,"user":{"displayName":"Trevor Johnson","userId":"10875792761423186902"},"user_tz":420},"id":"10C1FSESKUt7","outputId":"516c0660-a527-4e4f-9db0-de90d8cadb11"},"outputs":[{"name":"stderr","output_type":"stream","text":["PyTorch: setting up devices\n","The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n","The following columns in the training set don't have a corresponding argument in `BartForConditionalGeneration.forward` and have been ignored: summary, subreddit, content. If summary, subreddit, content are not expected by `BartForConditionalGeneration.forward`,  you can safely ignore this message.\n","/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  FutureWarning,\n","***** Running training *****\n","  Num examples = 60000\n","  Num Epochs = 3\n","  Instantaneous batch size per device = 8\n","  Total train batch size (w. parallel, distributed & accumulation) = 8\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 22500\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='22500' max='22500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [22500/22500 4:50:04, Epoch 3/3]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Rouge1</th>\n","      <th>Rouge2</th>\n","      <th>Rougel</th>\n","      <th>Rougelsum</th>\n","      <th>Gen Len</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>2.957900</td>\n","      <td>3.445648</td>\n","      <td>18.898900</td>\n","      <td>5.068900</td>\n","      <td>15.485100</td>\n","      <td>16.315500</td>\n","      <td>19.442000</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>2.743700</td>\n","      <td>3.479094</td>\n","      <td>18.881800</td>\n","      <td>4.977500</td>\n","      <td>15.408400</td>\n","      <td>16.289200</td>\n","      <td>19.113000</td>\n","    </tr>\n","    <tr>\n","      <td>3</td>\n","      <td>2.566600</td>\n","      <td>3.520019</td>\n","      <td>18.894100</td>\n","      <td>4.952900</td>\n","      <td>15.299800</td>\n","      <td>16.196500</td>\n","      <td>19.820800</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Saving model checkpoint to BART_reddit/checkpoint-500\n","Configuration saved in BART_reddit/checkpoint-500/config.json\n","Model weights saved in BART_reddit/checkpoint-500/pytorch_model.bin\n","tokenizer config file saved in BART_reddit/checkpoint-500/tokenizer_config.json\n","Special tokens file saved in BART_reddit/checkpoint-500/special_tokens_map.json\n","Saving model checkpoint to BART_reddit/checkpoint-1000\n","Configuration saved in BART_reddit/checkpoint-1000/config.json\n","Model weights saved in BART_reddit/checkpoint-1000/pytorch_model.bin\n","tokenizer config file saved in BART_reddit/checkpoint-1000/tokenizer_config.json\n","Special tokens file saved in BART_reddit/checkpoint-1000/special_tokens_map.json\n","Deleting older checkpoint [BART_reddit/checkpoint-500] due to args.save_total_limit\n","Saving model checkpoint to BART_reddit/checkpoint-1500\n","Configuration saved in BART_reddit/checkpoint-1500/config.json\n","Model weights saved in BART_reddit/checkpoint-1500/pytorch_model.bin\n","tokenizer config file saved in BART_reddit/checkpoint-1500/tokenizer_config.json\n","Special tokens file saved in BART_reddit/checkpoint-1500/special_tokens_map.json\n","Deleting older checkpoint [BART_reddit/checkpoint-1000] due to args.save_total_limit\n","Saving model checkpoint to BART_reddit/checkpoint-2000\n","Configuration saved in BART_reddit/checkpoint-2000/config.json\n","Model weights saved in BART_reddit/checkpoint-2000/pytorch_model.bin\n","tokenizer config file saved in BART_reddit/checkpoint-2000/tokenizer_config.json\n","Special tokens file saved in BART_reddit/checkpoint-2000/special_tokens_map.json\n","Deleting older checkpoint [BART_reddit/checkpoint-1500] due to args.save_total_limit\n","Saving model checkpoint to BART_reddit/checkpoint-2500\n","Configuration saved in BART_reddit/checkpoint-2500/config.json\n","Model weights saved in BART_reddit/checkpoint-2500/pytorch_model.bin\n","tokenizer config file saved in BART_reddit/checkpoint-2500/tokenizer_config.json\n","Special tokens file saved in BART_reddit/checkpoint-2500/special_tokens_map.json\n","Deleting older checkpoint [BART_reddit/checkpoint-2000] due to args.save_total_limit\n","Saving model checkpoint to BART_reddit/checkpoint-3000\n","Configuration saved in BART_reddit/checkpoint-3000/config.json\n","Model weights saved in BART_reddit/checkpoint-3000/pytorch_model.bin\n","tokenizer config file saved in BART_reddit/checkpoint-3000/tokenizer_config.json\n","Special tokens file saved in BART_reddit/checkpoint-3000/special_tokens_map.json\n","Deleting older checkpoint [BART_reddit/checkpoint-2500] due to args.save_total_limit\n","Saving model checkpoint to BART_reddit/checkpoint-3500\n","Configuration saved in BART_reddit/checkpoint-3500/config.json\n","Model weights saved in BART_reddit/checkpoint-3500/pytorch_model.bin\n","tokenizer config file saved in BART_reddit/checkpoint-3500/tokenizer_config.json\n","Special tokens file saved in BART_reddit/checkpoint-3500/special_tokens_map.json\n","Deleting older checkpoint [BART_reddit/checkpoint-3000] due to args.save_total_limit\n","Saving model checkpoint to BART_reddit/checkpoint-4000\n","Configuration saved in BART_reddit/checkpoint-4000/config.json\n","Model weights saved in BART_reddit/checkpoint-4000/pytorch_model.bin\n","tokenizer config file saved in BART_reddit/checkpoint-4000/tokenizer_config.json\n","Special tokens file saved in BART_reddit/checkpoint-4000/special_tokens_map.json\n","Deleting older checkpoint [BART_reddit/checkpoint-3500] due to args.save_total_limit\n","Saving model checkpoint to BART_reddit/checkpoint-4500\n","Configuration saved in BART_reddit/checkpoint-4500/config.json\n","Model weights saved in BART_reddit/checkpoint-4500/pytorch_model.bin\n","tokenizer config file saved in BART_reddit/checkpoint-4500/tokenizer_config.json\n","Special tokens file saved in BART_reddit/checkpoint-4500/special_tokens_map.json\n","Deleting older checkpoint [BART_reddit/checkpoint-4000] due to args.save_total_limit\n","Saving model checkpoint to BART_reddit/checkpoint-5000\n","Configuration saved in BART_reddit/checkpoint-5000/config.json\n","Model weights saved in BART_reddit/checkpoint-5000/pytorch_model.bin\n","tokenizer config file saved in BART_reddit/checkpoint-5000/tokenizer_config.json\n","Special tokens file saved in BART_reddit/checkpoint-5000/special_tokens_map.json\n","Deleting older checkpoint [BART_reddit/checkpoint-4500] due to args.save_total_limit\n","Saving model checkpoint to BART_reddit/checkpoint-5500\n","Configuration saved in BART_reddit/checkpoint-5500/config.json\n","Model weights saved in BART_reddit/checkpoint-5500/pytorch_model.bin\n","tokenizer config file saved in BART_reddit/checkpoint-5500/tokenizer_config.json\n","Special tokens file saved in BART_reddit/checkpoint-5500/special_tokens_map.json\n","Deleting older checkpoint [BART_reddit/checkpoint-5000] due to args.save_total_limit\n","Saving model checkpoint to BART_reddit/checkpoint-6000\n","Configuration saved in BART_reddit/checkpoint-6000/config.json\n","Model weights saved in BART_reddit/checkpoint-6000/pytorch_model.bin\n","tokenizer config file saved in BART_reddit/checkpoint-6000/tokenizer_config.json\n","Special tokens file saved in BART_reddit/checkpoint-6000/special_tokens_map.json\n","Deleting older checkpoint [BART_reddit/checkpoint-5500] due to args.save_total_limit\n","Saving model checkpoint to BART_reddit/checkpoint-6500\n","Configuration saved in BART_reddit/checkpoint-6500/config.json\n","Model weights saved in BART_reddit/checkpoint-6500/pytorch_model.bin\n","tokenizer config file saved in BART_reddit/checkpoint-6500/tokenizer_config.json\n","Special tokens file saved in BART_reddit/checkpoint-6500/special_tokens_map.json\n","Deleting older checkpoint [BART_reddit/checkpoint-6000] due to args.save_total_limit\n","Saving model checkpoint to BART_reddit/checkpoint-7000\n","Configuration saved in BART_reddit/checkpoint-7000/config.json\n","Model weights saved in BART_reddit/checkpoint-7000/pytorch_model.bin\n","tokenizer config file saved in BART_reddit/checkpoint-7000/tokenizer_config.json\n","Special tokens file saved in BART_reddit/checkpoint-7000/special_tokens_map.json\n","Deleting older checkpoint [BART_reddit/checkpoint-6500] due to args.save_total_limit\n","Saving model checkpoint to BART_reddit/checkpoint-7500\n","Configuration saved in BART_reddit/checkpoint-7500/config.json\n","Model weights saved in BART_reddit/checkpoint-7500/pytorch_model.bin\n","tokenizer config file saved in BART_reddit/checkpoint-7500/tokenizer_config.json\n","Special tokens file saved in BART_reddit/checkpoint-7500/special_tokens_map.json\n","Deleting older checkpoint [BART_reddit/checkpoint-7000] due to args.save_total_limit\n","The following columns in the evaluation set don't have a corresponding argument in `BartForConditionalGeneration.forward` and have been ignored: summary, subreddit, content. If summary, subreddit, content are not expected by `BartForConditionalGeneration.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 4000\n","  Batch size = 8\n","Saving model checkpoint to BART_reddit/checkpoint-8000\n","Configuration saved in BART_reddit/checkpoint-8000/config.json\n","Model weights saved in BART_reddit/checkpoint-8000/pytorch_model.bin\n","tokenizer config file saved in BART_reddit/checkpoint-8000/tokenizer_config.json\n","Special tokens file saved in BART_reddit/checkpoint-8000/special_tokens_map.json\n","Deleting older checkpoint [BART_reddit/checkpoint-7500] due to args.save_total_limit\n","Saving model checkpoint to BART_reddit/checkpoint-8500\n","Configuration saved in BART_reddit/checkpoint-8500/config.json\n","Model weights saved in BART_reddit/checkpoint-8500/pytorch_model.bin\n","tokenizer config file saved in BART_reddit/checkpoint-8500/tokenizer_config.json\n","Special tokens file saved in BART_reddit/checkpoint-8500/special_tokens_map.json\n","Deleting older checkpoint [BART_reddit/checkpoint-8000] due to args.save_total_limit\n","Saving model checkpoint to BART_reddit/checkpoint-9000\n","Configuration saved in BART_reddit/checkpoint-9000/config.json\n","Model weights saved in BART_reddit/checkpoint-9000/pytorch_model.bin\n","tokenizer config file saved in BART_reddit/checkpoint-9000/tokenizer_config.json\n","Special tokens file saved in BART_reddit/checkpoint-9000/special_tokens_map.json\n","Deleting older checkpoint [BART_reddit/checkpoint-8500] due to args.save_total_limit\n","Saving model checkpoint to BART_reddit/checkpoint-9500\n","Configuration saved in BART_reddit/checkpoint-9500/config.json\n","Model weights saved in BART_reddit/checkpoint-9500/pytorch_model.bin\n","tokenizer config file saved in BART_reddit/checkpoint-9500/tokenizer_config.json\n","Special tokens file saved in BART_reddit/checkpoint-9500/special_tokens_map.json\n","Deleting older checkpoint [BART_reddit/checkpoint-9000] due to args.save_total_limit\n","Saving model checkpoint to BART_reddit/checkpoint-10000\n","Configuration saved in BART_reddit/checkpoint-10000/config.json\n","Model weights saved in BART_reddit/checkpoint-10000/pytorch_model.bin\n","tokenizer config file saved in BART_reddit/checkpoint-10000/tokenizer_config.json\n","Special tokens file saved in BART_reddit/checkpoint-10000/special_tokens_map.json\n","Deleting older checkpoint [BART_reddit/checkpoint-9500] due to args.save_total_limit\n","Saving model checkpoint to BART_reddit/checkpoint-10500\n","Configuration saved in BART_reddit/checkpoint-10500/config.json\n","Model weights saved in BART_reddit/checkpoint-10500/pytorch_model.bin\n","tokenizer config file saved in BART_reddit/checkpoint-10500/tokenizer_config.json\n","Special tokens file saved in BART_reddit/checkpoint-10500/special_tokens_map.json\n","Deleting older checkpoint [BART_reddit/checkpoint-10000] due to args.save_total_limit\n","Saving model checkpoint to BART_reddit/checkpoint-11000\n","Configuration saved in BART_reddit/checkpoint-11000/config.json\n","Model weights saved in BART_reddit/checkpoint-11000/pytorch_model.bin\n","tokenizer config file saved in BART_reddit/checkpoint-11000/tokenizer_config.json\n","Special tokens file saved in BART_reddit/checkpoint-11000/special_tokens_map.json\n","Deleting older checkpoint [BART_reddit/checkpoint-10500] due to args.save_total_limit\n","Saving model checkpoint to BART_reddit/checkpoint-11500\n","Configuration saved in BART_reddit/checkpoint-11500/config.json\n","Model weights saved in BART_reddit/checkpoint-11500/pytorch_model.bin\n","tokenizer config file saved in BART_reddit/checkpoint-11500/tokenizer_config.json\n","Special tokens file saved in BART_reddit/checkpoint-11500/special_tokens_map.json\n","Deleting older checkpoint [BART_reddit/checkpoint-11000] due to args.save_total_limit\n","Saving model checkpoint to BART_reddit/checkpoint-12000\n","Configuration saved in BART_reddit/checkpoint-12000/config.json\n","Model weights saved in BART_reddit/checkpoint-12000/pytorch_model.bin\n","tokenizer config file saved in BART_reddit/checkpoint-12000/tokenizer_config.json\n","Special tokens file saved in BART_reddit/checkpoint-12000/special_tokens_map.json\n","Deleting older checkpoint [BART_reddit/checkpoint-11500] due to args.save_total_limit\n","Saving model checkpoint to BART_reddit/checkpoint-12500\n","Configuration saved in BART_reddit/checkpoint-12500/config.json\n","Model weights saved in BART_reddit/checkpoint-12500/pytorch_model.bin\n","tokenizer config file saved in BART_reddit/checkpoint-12500/tokenizer_config.json\n","Special tokens file saved in BART_reddit/checkpoint-12500/special_tokens_map.json\n","Deleting older checkpoint [BART_reddit/checkpoint-12000] due to args.save_total_limit\n","Saving model checkpoint to BART_reddit/checkpoint-13000\n","Configuration saved in BART_reddit/checkpoint-13000/config.json\n","Model weights saved in BART_reddit/checkpoint-13000/pytorch_model.bin\n","tokenizer config file saved in BART_reddit/checkpoint-13000/tokenizer_config.json\n","Special tokens file saved in BART_reddit/checkpoint-13000/special_tokens_map.json\n","Deleting older checkpoint [BART_reddit/checkpoint-12500] due to args.save_total_limit\n","Saving model checkpoint to BART_reddit/checkpoint-13500\n","Configuration saved in BART_reddit/checkpoint-13500/config.json\n","Model weights saved in BART_reddit/checkpoint-13500/pytorch_model.bin\n","tokenizer config file saved in BART_reddit/checkpoint-13500/tokenizer_config.json\n","Special tokens file saved in BART_reddit/checkpoint-13500/special_tokens_map.json\n","Deleting older checkpoint [BART_reddit/checkpoint-13000] due to args.save_total_limit\n","Saving model checkpoint to BART_reddit/checkpoint-14000\n","Configuration saved in BART_reddit/checkpoint-14000/config.json\n","Model weights saved in BART_reddit/checkpoint-14000/pytorch_model.bin\n","tokenizer config file saved in BART_reddit/checkpoint-14000/tokenizer_config.json\n","Special tokens file saved in BART_reddit/checkpoint-14000/special_tokens_map.json\n","Deleting older checkpoint [BART_reddit/checkpoint-13500] due to args.save_total_limit\n","Saving model checkpoint to BART_reddit/checkpoint-14500\n","Configuration saved in BART_reddit/checkpoint-14500/config.json\n","Model weights saved in BART_reddit/checkpoint-14500/pytorch_model.bin\n","tokenizer config file saved in BART_reddit/checkpoint-14500/tokenizer_config.json\n","Special tokens file saved in BART_reddit/checkpoint-14500/special_tokens_map.json\n","Deleting older checkpoint [BART_reddit/checkpoint-14000] due to args.save_total_limit\n","Saving model checkpoint to BART_reddit/checkpoint-15000\n","Configuration saved in BART_reddit/checkpoint-15000/config.json\n","Model weights saved in BART_reddit/checkpoint-15000/pytorch_model.bin\n","tokenizer config file saved in BART_reddit/checkpoint-15000/tokenizer_config.json\n","Special tokens file saved in BART_reddit/checkpoint-15000/special_tokens_map.json\n","Deleting older checkpoint [BART_reddit/checkpoint-14500] due to args.save_total_limit\n","The following columns in the evaluation set don't have a corresponding argument in `BartForConditionalGeneration.forward` and have been ignored: summary, subreddit, content. If summary, subreddit, content are not expected by `BartForConditionalGeneration.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 4000\n","  Batch size = 8\n","Saving model checkpoint to BART_reddit/checkpoint-15500\n","Configuration saved in BART_reddit/checkpoint-15500/config.json\n","Model weights saved in BART_reddit/checkpoint-15500/pytorch_model.bin\n","tokenizer config file saved in BART_reddit/checkpoint-15500/tokenizer_config.json\n","Special tokens file saved in BART_reddit/checkpoint-15500/special_tokens_map.json\n","Deleting older checkpoint [BART_reddit/checkpoint-15000] due to args.save_total_limit\n","Saving model checkpoint to BART_reddit/checkpoint-16000\n","Configuration saved in BART_reddit/checkpoint-16000/config.json\n","Model weights saved in BART_reddit/checkpoint-16000/pytorch_model.bin\n","tokenizer config file saved in BART_reddit/checkpoint-16000/tokenizer_config.json\n","Special tokens file saved in BART_reddit/checkpoint-16000/special_tokens_map.json\n","Deleting older checkpoint [BART_reddit/checkpoint-15500] due to args.save_total_limit\n","Saving model checkpoint to BART_reddit/checkpoint-16500\n","Configuration saved in BART_reddit/checkpoint-16500/config.json\n","Model weights saved in BART_reddit/checkpoint-16500/pytorch_model.bin\n","tokenizer config file saved in BART_reddit/checkpoint-16500/tokenizer_config.json\n","Special tokens file saved in BART_reddit/checkpoint-16500/special_tokens_map.json\n","Deleting older checkpoint [BART_reddit/checkpoint-16000] due to args.save_total_limit\n","Saving model checkpoint to BART_reddit/checkpoint-17000\n","Configuration saved in BART_reddit/checkpoint-17000/config.json\n","Model weights saved in BART_reddit/checkpoint-17000/pytorch_model.bin\n","tokenizer config file saved in BART_reddit/checkpoint-17000/tokenizer_config.json\n","Special tokens file saved in BART_reddit/checkpoint-17000/special_tokens_map.json\n","Deleting older checkpoint [BART_reddit/checkpoint-16500] due to args.save_total_limit\n","Saving model checkpoint to BART_reddit/checkpoint-17500\n","Configuration saved in BART_reddit/checkpoint-17500/config.json\n","Model weights saved in BART_reddit/checkpoint-17500/pytorch_model.bin\n","tokenizer config file saved in BART_reddit/checkpoint-17500/tokenizer_config.json\n","Special tokens file saved in BART_reddit/checkpoint-17500/special_tokens_map.json\n","Deleting older checkpoint [BART_reddit/checkpoint-17000] due to args.save_total_limit\n","Saving model checkpoint to BART_reddit/checkpoint-18000\n","Configuration saved in BART_reddit/checkpoint-18000/config.json\n","Model weights saved in BART_reddit/checkpoint-18000/pytorch_model.bin\n","tokenizer config file saved in BART_reddit/checkpoint-18000/tokenizer_config.json\n","Special tokens file saved in BART_reddit/checkpoint-18000/special_tokens_map.json\n","Deleting older checkpoint [BART_reddit/checkpoint-17500] due to args.save_total_limit\n","Saving model checkpoint to BART_reddit/checkpoint-18500\n","Configuration saved in BART_reddit/checkpoint-18500/config.json\n","Model weights saved in BART_reddit/checkpoint-18500/pytorch_model.bin\n","tokenizer config file saved in BART_reddit/checkpoint-18500/tokenizer_config.json\n","Special tokens file saved in BART_reddit/checkpoint-18500/special_tokens_map.json\n","Deleting older checkpoint [BART_reddit/checkpoint-18000] due to args.save_total_limit\n","Saving model checkpoint to BART_reddit/checkpoint-19000\n","Configuration saved in BART_reddit/checkpoint-19000/config.json\n","Model weights saved in BART_reddit/checkpoint-19000/pytorch_model.bin\n","tokenizer config file saved in BART_reddit/checkpoint-19000/tokenizer_config.json\n","Special tokens file saved in BART_reddit/checkpoint-19000/special_tokens_map.json\n","Deleting older checkpoint [BART_reddit/checkpoint-18500] due to args.save_total_limit\n","Saving model checkpoint to BART_reddit/checkpoint-19500\n","Configuration saved in BART_reddit/checkpoint-19500/config.json\n","Model weights saved in BART_reddit/checkpoint-19500/pytorch_model.bin\n","tokenizer config file saved in BART_reddit/checkpoint-19500/tokenizer_config.json\n","Special tokens file saved in BART_reddit/checkpoint-19500/special_tokens_map.json\n","Deleting older checkpoint [BART_reddit/checkpoint-19000] due to args.save_total_limit\n","Saving model checkpoint to BART_reddit/checkpoint-20000\n","Configuration saved in BART_reddit/checkpoint-20000/config.json\n","Model weights saved in BART_reddit/checkpoint-20000/pytorch_model.bin\n","tokenizer config file saved in BART_reddit/checkpoint-20000/tokenizer_config.json\n","Special tokens file saved in BART_reddit/checkpoint-20000/special_tokens_map.json\n","Deleting older checkpoint [BART_reddit/checkpoint-19500] due to args.save_total_limit\n","Saving model checkpoint to BART_reddit/checkpoint-20500\n","Configuration saved in BART_reddit/checkpoint-20500/config.json\n","Model weights saved in BART_reddit/checkpoint-20500/pytorch_model.bin\n","tokenizer config file saved in BART_reddit/checkpoint-20500/tokenizer_config.json\n","Special tokens file saved in BART_reddit/checkpoint-20500/special_tokens_map.json\n","Deleting older checkpoint [BART_reddit/checkpoint-20000] due to args.save_total_limit\n","Saving model checkpoint to BART_reddit/checkpoint-21000\n","Configuration saved in BART_reddit/checkpoint-21000/config.json\n","Model weights saved in BART_reddit/checkpoint-21000/pytorch_model.bin\n","tokenizer config file saved in BART_reddit/checkpoint-21000/tokenizer_config.json\n","Special tokens file saved in BART_reddit/checkpoint-21000/special_tokens_map.json\n","Deleting older checkpoint [BART_reddit/checkpoint-20500] due to args.save_total_limit\n","Saving model checkpoint to BART_reddit/checkpoint-21500\n","Configuration saved in BART_reddit/checkpoint-21500/config.json\n","Model weights saved in BART_reddit/checkpoint-21500/pytorch_model.bin\n","tokenizer config file saved in BART_reddit/checkpoint-21500/tokenizer_config.json\n","Special tokens file saved in BART_reddit/checkpoint-21500/special_tokens_map.json\n","Deleting older checkpoint [BART_reddit/checkpoint-21000] due to args.save_total_limit\n","Saving model checkpoint to BART_reddit/checkpoint-22000\n","Configuration saved in BART_reddit/checkpoint-22000/config.json\n","Model weights saved in BART_reddit/checkpoint-22000/pytorch_model.bin\n","tokenizer config file saved in BART_reddit/checkpoint-22000/tokenizer_config.json\n","Special tokens file saved in BART_reddit/checkpoint-22000/special_tokens_map.json\n","Deleting older checkpoint [BART_reddit/checkpoint-21500] due to args.save_total_limit\n","Saving model checkpoint to BART_reddit/checkpoint-22500\n","Configuration saved in BART_reddit/checkpoint-22500/config.json\n","Model weights saved in BART_reddit/checkpoint-22500/pytorch_model.bin\n","tokenizer config file saved in BART_reddit/checkpoint-22500/tokenizer_config.json\n","Special tokens file saved in BART_reddit/checkpoint-22500/special_tokens_map.json\n","Deleting older checkpoint [BART_reddit/checkpoint-22000] due to args.save_total_limit\n","The following columns in the evaluation set don't have a corresponding argument in `BartForConditionalGeneration.forward` and have been ignored: summary, subreddit, content. If summary, subreddit, content are not expected by `BartForConditionalGeneration.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 4000\n","  Batch size = 8\n","\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n"]},{"name":"stdout","output_type":"stream","text":["CPU times: user 4h 34min 19s, sys: 10min 17s, total: 4h 44min 36s\n","Wall time: 4h 50min 4s\n"]}],"source":["%%time\n","# note, batch size of 8 seems to almost max out the gpu\n","# so probs dont go any higher than this. \n","args = Seq2SeqTrainingArguments(\n","    f\"BART_reddit\", \n","    evaluation_strategy = \"epoch\",\n","    learning_rate=2e-5,\n","    per_device_train_batch_size=8, # 16\n","    per_device_eval_batch_size=8, #16\n","    weight_decay=0.01,\n","    save_total_limit=1, #3,\n","    num_train_epochs=3,\n","    predict_with_generate=True,\n","    # fp16=True,\n","    # push_to_hub=True,\n",")\n","\n","trainer = Seq2SeqTrainer(\n","    model,\n","    args,\n","    train_dataset=tokenized_datasets[\"train\"],\n","    eval_dataset=tokenized_datasets[\"valid\"],\n","    data_collator=data_collator,\n","    tokenizer=tokenizer,\n","    compute_metrics=compute_metrics\n",")\n","\n","# this should save the model to disk. changing wd so that it saves here:\n","fit_path = '/content/gdrive/MyDrive/w266/w266_reddit_summarization/data/model_outputs/bart_fitted_models/round2'\n","os.chdir(fit_path)\n","trainer.train()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["ce8848308dcf41bc9834cdfa68dce36f","ed396b2aa3ad43a2a8b9a2d579b561c2","8ff9f625841f4cc196401ec2f2620c1b","af3ecda5ac234ebca949eb149a053157","50a634cf333b411ebdfab2245fc192cb","f9a3d7b321c5407c81721bbe60169c38","fb9b217f0b764b659bc55c554d8188f7","377d603b4366429294d21fae7ca86aa2","1add1bc41ca944ceaf3f4def4c68d34e","0e0e0d499d4d4e4d9308a5dec1055699","8651d46c32a24fbba3d1cd35735585b1"]},"id":"3_BIV75eXn-O","outputId":"100e01f9-a355-4ea5-809c-bfd30e7ebb10"},"outputs":[{"name":"stderr","output_type":"stream","text":["PyTorch: setting up devices\n","The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n","Loading model from /content/gdrive/MyDrive/w266/w266_reddit_summarization/data/model_outputs/bart_fitted_models/round2/BART_reddit/checkpoint-15500.\n","The following columns in the training set don't have a corresponding argument in `BartForConditionalGeneration.forward` and have been ignored: subreddit, content, summary. If subreddit, content, summary are not expected by `BartForConditionalGeneration.forward`,  you can safely ignore this message.\n","/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  FutureWarning,\n","***** Running training *****\n","  Num examples = 60000\n","  Num Epochs = 3\n","  Instantaneous batch size per device = 8\n","  Total train batch size (w. parallel, distributed & accumulation) = 8\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 22500\n","  Continuing training from checkpoint, will skip to saved global_step\n","  Continuing training from epoch 2\n","  Continuing training from global step 15500\n","  Will skip the first 2 epochs then the first 500 batches in the first epoch. If this takes a lot of time, you can add the `--ignore_data_skip` flag to your launch command, but you will resume the training on data already seen by your model.\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"ce8848308dcf41bc9834cdfa68dce36f","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/500 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='20417' max='22500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [20417/22500 55:41 < 23:35, 1.47 it/s, Epoch 2.72/3]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Saving model checkpoint to BART_reddit/checkpoint-16000\n","Configuration saved in BART_reddit/checkpoint-16000/config.json\n","Model weights saved in BART_reddit/checkpoint-16000/pytorch_model.bin\n","tokenizer config file saved in BART_reddit/checkpoint-16000/tokenizer_config.json\n","Special tokens file saved in BART_reddit/checkpoint-16000/special_tokens_map.json\n","Deleting older checkpoint [BART_reddit/checkpoint-15000] due to args.save_total_limit\n","Deleting older checkpoint [BART_reddit/checkpoint-15500] due to args.save_total_limit\n","Saving model checkpoint to BART_reddit/checkpoint-16500\n","Configuration saved in BART_reddit/checkpoint-16500/config.json\n","Model weights saved in BART_reddit/checkpoint-16500/pytorch_model.bin\n","tokenizer config file saved in BART_reddit/checkpoint-16500/tokenizer_config.json\n","Special tokens file saved in BART_reddit/checkpoint-16500/special_tokens_map.json\n","Deleting older checkpoint [BART_reddit/checkpoint-16000] due to args.save_total_limit\n","Saving model checkpoint to BART_reddit/checkpoint-17000\n","Configuration saved in BART_reddit/checkpoint-17000/config.json\n","Model weights saved in BART_reddit/checkpoint-17000/pytorch_model.bin\n","tokenizer config file saved in BART_reddit/checkpoint-17000/tokenizer_config.json\n","Special tokens file saved in BART_reddit/checkpoint-17000/special_tokens_map.json\n","Deleting older checkpoint [BART_reddit/checkpoint-16500] due to args.save_total_limit\n","Saving model checkpoint to BART_reddit/checkpoint-17500\n","Configuration saved in BART_reddit/checkpoint-17500/config.json\n","Model weights saved in BART_reddit/checkpoint-17500/pytorch_model.bin\n","tokenizer config file saved in BART_reddit/checkpoint-17500/tokenizer_config.json\n","Special tokens file saved in BART_reddit/checkpoint-17500/special_tokens_map.json\n","Deleting older checkpoint [BART_reddit/checkpoint-17000] due to args.save_total_limit\n","Saving model checkpoint to BART_reddit/checkpoint-18000\n","Configuration saved in BART_reddit/checkpoint-18000/config.json\n","Model weights saved in BART_reddit/checkpoint-18000/pytorch_model.bin\n","tokenizer config file saved in BART_reddit/checkpoint-18000/tokenizer_config.json\n","Special tokens file saved in BART_reddit/checkpoint-18000/special_tokens_map.json\n","Deleting older checkpoint [BART_reddit/checkpoint-17500] due to args.save_total_limit\n","Saving model checkpoint to BART_reddit/checkpoint-18500\n","Configuration saved in BART_reddit/checkpoint-18500/config.json\n","Model weights saved in BART_reddit/checkpoint-18500/pytorch_model.bin\n","tokenizer config file saved in BART_reddit/checkpoint-18500/tokenizer_config.json\n","Special tokens file saved in BART_reddit/checkpoint-18500/special_tokens_map.json\n","Deleting older checkpoint [BART_reddit/checkpoint-18000] due to args.save_total_limit\n","Saving model checkpoint to BART_reddit/checkpoint-19000\n","Configuration saved in BART_reddit/checkpoint-19000/config.json\n","Model weights saved in BART_reddit/checkpoint-19000/pytorch_model.bin\n","tokenizer config file saved in BART_reddit/checkpoint-19000/tokenizer_config.json\n","Special tokens file saved in BART_reddit/checkpoint-19000/special_tokens_map.json\n","Deleting older checkpoint [BART_reddit/checkpoint-18500] due to args.save_total_limit\n","Saving model checkpoint to BART_reddit/checkpoint-19500\n","Configuration saved in BART_reddit/checkpoint-19500/config.json\n","Model weights saved in BART_reddit/checkpoint-19500/pytorch_model.bin\n","tokenizer config file saved in BART_reddit/checkpoint-19500/tokenizer_config.json\n","Special tokens file saved in BART_reddit/checkpoint-19500/special_tokens_map.json\n","Deleting older checkpoint [BART_reddit/checkpoint-19000] due to args.save_total_limit\n","Saving model checkpoint to BART_reddit/checkpoint-20000\n","Configuration saved in BART_reddit/checkpoint-20000/config.json\n","Model weights saved in BART_reddit/checkpoint-20000/pytorch_model.bin\n","tokenizer config file saved in BART_reddit/checkpoint-20000/tokenizer_config.json\n","Special tokens file saved in BART_reddit/checkpoint-20000/special_tokens_map.json\n","Deleting older checkpoint [BART_reddit/checkpoint-19500] due to args.save_total_limit\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='21001' max='22500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [21001/22500 1:02:19 < 16:59, 1.47 it/s, Epoch 2.80/3]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Saving model checkpoint to BART_reddit/checkpoint-20500\n","Configuration saved in BART_reddit/checkpoint-20500/config.json\n","Model weights saved in BART_reddit/checkpoint-20500/pytorch_model.bin\n","tokenizer config file saved in BART_reddit/checkpoint-20500/tokenizer_config.json\n","Special tokens file saved in BART_reddit/checkpoint-20500/special_tokens_map.json\n","Deleting older checkpoint [BART_reddit/checkpoint-20000] due to args.save_total_limit\n","Saving model checkpoint to BART_reddit/checkpoint-21000\n","Configuration saved in BART_reddit/checkpoint-21000/config.json\n","Model weights saved in BART_reddit/checkpoint-21000/pytorch_model.bin\n","tokenizer config file saved in BART_reddit/checkpoint-21000/tokenizer_config.json\n","Special tokens file saved in BART_reddit/checkpoint-21000/special_tokens_map.json\n"]}],"source":["# continue training from checkpoint\n","model_checkpoint = '/content/gdrive/MyDrive/w266/w266_reddit_summarization/data/model_outputs/bart_fitted_models/round2/BART_reddit/checkpoint-15500'\n","args = Seq2SeqTrainingArguments(\n","    f\"BART_reddit\", \n","    evaluation_strategy = \"epoch\",\n","    learning_rate=2e-5,\n","    per_device_train_batch_size=8, # 16\n","    per_device_eval_batch_size=8, #16\n","    weight_decay=0.01,\n","    save_total_limit=1, #3,\n","    num_train_epochs=3, #3\n","    predict_with_generate=True,\n","    # fp16=True,\n","    # push_to_hub=True,\n",")\n","\n","trainer = Seq2SeqTrainer(\n","    model,\n","    args,\n","    train_dataset=tokenized_datasets[\"train\"],\n","    eval_dataset=tokenized_datasets[\"valid\"],\n","    data_collator=data_collator,\n","    tokenizer=tokenizer,\n","    compute_metrics=compute_metrics\n",")\n","fit_path = '/content/gdrive/MyDrive/w266/w266_reddit_summarization/data/model_outputs/bart_fitted_models/round2'\n","os.chdir(fit_path)\n","trainer.train(model_checkpoint)"]},{"cell_type":"code","execution_count":15,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":682},"executionInfo":{"elapsed":20454,"status":"error","timestamp":1658607872137,"user":{"displayName":"Trevor Johnson","userId":"10875792761423186902"},"user_tz":420},"id":"mp2wtfVKt2YF","outputId":"2ad46acc-07c4-4f25-99d0-9f30ac7638ff"},"outputs":[{"name":"stderr","output_type":"stream","text":["PyTorch: setting up devices\n","The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n","Loading model from /content/gdrive/MyDrive/w266/w266_reddit_summarization/data/model_outputs/bart_fitted_models/round2/BART_reddit.\n","The following columns in the training set don't have a corresponding argument in `BartForConditionalGeneration.forward` and have been ignored: summary, subreddit, content. If summary, subreddit, content are not expected by `BartForConditionalGeneration.forward`,  you can safely ignore this message.\n","/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  FutureWarning,\n","***** Running training *****\n","  Num examples = 60000\n","  Num Epochs = 3\n","  Instantaneous batch size per device = 8\n","  Total train batch size (w. parallel, distributed & accumulation) = 8\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 22500\n","Didn't find an RNG file, if you are resuming a training that was launched in a distributed fashion, reproducibility is not guaranteed.\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='17' max='22500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [   17/22500 00:10 < 4:13:39, 1.48 it/s, Epoch 0.00/3]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"ename":"KeyboardInterrupt","evalue":"ignored","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-15-f7e961dac17d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0mfit_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/content/gdrive/MyDrive/w266/w266_reddit_summarization/data/model_outputs/bart_fitted_models/round2'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfit_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_checkpoint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1411\u001b[0m             \u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1412\u001b[0m             \u001b[0mtrial\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1413\u001b[0;31m             \u001b[0mignore_keys_for_eval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mignore_keys_for_eval\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1414\u001b[0m         )\n\u001b[1;32m   1415\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1649\u001b[0m                         \u001b[0mtr_loss_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1650\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1651\u001b[0;31m                     \u001b[0mtr_loss_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1652\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1653\u001b[0m                 if (\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtraining_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   2361\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeepspeed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2362\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2363\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2364\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2365\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    394\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    395\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 396\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    397\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    398\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    173\u001b[0m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[1;32m    174\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 175\u001b[0;31m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    176\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m def grad(\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["# continue training from checkpoint\n","model_checkpoint = '/content/gdrive/MyDrive/w266/w266_reddit_summarization/data/model_outputs/bart_fitted_models/round2/BART_reddit'\n","args = Seq2SeqTrainingArguments(\n","    f\"BART_reddit\", \n","    evaluation_strategy = \"epoch\",\n","    learning_rate=2e-5,\n","    per_device_train_batch_size=8, # 16\n","    per_device_eval_batch_size=8, #16\n","    weight_decay=0.01,\n","    save_total_limit=1, #3,\n","    num_train_epochs=3, #3\n","    predict_with_generate=True,\n","    # fp16=True,\n","    # push_to_hub=True,\n",")\n","\n","trainer = Seq2SeqTrainer(\n","    model,\n","    args,\n","    train_dataset=tokenized_datasets[\"train\"],\n","    eval_dataset=tokenized_datasets[\"valid\"],\n","    data_collator=data_collator,\n","    tokenizer=tokenizer,\n","    compute_metrics=compute_metrics\n",")\n","fit_path = '/content/gdrive/MyDrive/w266/w266_reddit_summarization/data/model_outputs/bart_fitted_models/round2'\n","os.chdir(fit_path)\n","trainer.train(model_checkpoint)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gw64VgdGBNh7"},"outputs":[],"source":["%%time\n","# also save on huggingface\n","trainer.push_to_hub()\n","# fit_path = '/content/gdrive/MyDrive/w266/w266_reddit_summarization/data/model_outputs/bart_fitted_models/'\n","# os.chdir(fit_path)\n","# trainer.push_to_hub('trevorj/BART-reddit-advice_story')\n","\n","\n","# then load model back in\n","# model = AutoModelForSeq2SeqLM.from_pretrained(\"trevorj/model_name\")\n","\n","# push to hub seems to fail with this message:\n","# Dropping the following result as it does not have all the necessary fields:\n","# {'task': {'name': 'Sequence-to-sequence Language Modeling', 'type': 'text2text-generation'}, 'metrics': [{'name': 'Rouge1', 'type': 'rouge', 'value': 20.9206}]}"]},{"cell_type":"markdown","metadata":{"id":"1BvAa_bRA43-"},"source":["Compare Model predictions"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":229,"status":"ok","timestamp":1658380099957,"user":{"displayName":"Trevor Johnson","userId":"10875792761423186902"},"user_tz":420},"id":"HKl8zq8C-VUq","outputId":"fb02e29d-1bcd-4a3e-ad86-73e85567144f"},"outputs":[{"name":"stdout","output_type":"stream","text":["Content:\n","('Living in the Sierra Nevadas, it gets very cold at night. \\n'\n"," ' Took a shower after a cold night at work. Shower with nice hot water. Hot '\n"," 'but not steaming hot. Felt so good and warm. \\n'\n"," ' Drying myself, starting to get cold. \\n'\n"," ' Got out of the shower, so damn cold. \\n'\n"," ' Drying hair(on head) with the blow dryer and it felt so nice and warm.\\n'\n"," 'Balls were cold, penis was cold. All shrivled up, smaller than usual. \\n'\n"," ' Blow dry my private area just as usual during a cold winter day, but since '\n"," 'it was so cold I moved in a little closer. It felt so nice and warm. \\n'\n"," 'Thought \"oh, fuck it, I\\'ll move in a little closer since it was so damn '\n"," 'cold today.\" Moved in too close, tip of penis touches the burning hot iron '\n"," 'grill. Pull dryer away in shock. \\n'\n"," \" It hurts. It burns as I type this. Erections make it burn more. It's red \"\n"," \"where it got burned. Hoping it doesn't turn into a blister\")\n","\n","\n","True Summary\n","('Too damn cold, showered, dry my hair, dry my privates(male parts) moved in '\n"," 'too close, burned the tip of my penis with the iron grill on the blow dryer. '\n"," 'My penis hurts. It burns. \\n'\n"," \" EDIT: I didn't think i would have permanent markings but now I think I \"\n"," 'will. I have 2 very straight, black burn marks on my foreskin where the blow '\n"," 'dryer burned. I will upload a picture soon')\n"]}],"source":["# true:\n","print(\"Content:\")\n","pprint(tokenized_datasets['test_advice_story']['content'][0])\n","print(\"\\n\\nTrue Summary\")\n","pprint(tokenized_datasets['test_advice_story']['summary'][0])"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":591,"status":"ok","timestamp":1658380121697,"user":{"displayName":"Trevor Johnson","userId":"10875792761423186902"},"user_tz":420},"id":"eWNnU8Yy8ibn","outputId":"352d6ffb-070d-43b7-c7f0-522f11ed6e6e"},"outputs":[{"name":"stdout","output_type":"stream","text":["'Got cold, penis burns.'\n"]}],"source":["# results from my trained model above\n","# note, I had to add .cuda() to the end of the input tensor to specify to use gpu i guess. \n","# But dont do this for the original model. just your fine tuned one. \n","output = model.generate(torch.tensor([tokenized_datasets['test_advice_story']['input_ids'][0]]).cuda(), num_beams=2, max_length=60, min_length=2, no_repeat_ngram_size=3)\n","output_decoded = tokenizer.decode(output.squeeze(), skip_special_tokens=True)\n","pprint(output_decoded)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":8387,"status":"ok","timestamp":1658380522515,"user":{"displayName":"Trevor Johnson","userId":"10875792761423186902"},"user_tz":420},"id":"bRvkB7FXBKrZ","outputId":"8943df44-838b-41a5-ffb7-982fe0d0bf3a"},"outputs":[{"name":"stdout","output_type":"stream","text":["'Got cold, penis burns.'\n"]}],"source":["# read model from disk (same prediction. Good) \n","# note, using the latest checkpoint produced the same thing, so the weights are likely almost the same. \n","# final files in the base folder will get overwritten at very end with final weights. \n","# so we can just delete the older checkpoint folders\n","\n","checkpoint_disk = \"/content/gdrive/MyDrive/w266/w266_reddit_summarization/data/model_outputs/bart_fitted_models/round2/BART_reddit_advice_story\"\n","# model_disk = AutoModelForSeq2SeqLM.from_pretrained(checkpoint_disk)\n","model_disk = AutoModelForSeq2SeqLM.from_pretrained(checkpoint_disk, local_files_only=True) # optional to force looking locally\n","\n","clear_output()\n","output = model_disk.generate(torch.tensor([tokenized_datasets['test_advice_story']['input_ids'][0]]), num_beams=2, max_length=60, min_length=2, no_repeat_ngram_size=3)\n","output_decoded = tokenizer.decode(output.squeeze(), skip_special_tokens=True)\n","pprint(output_decoded)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":8100,"status":"ok","timestamp":1658380534874,"user":{"displayName":"Trevor Johnson","userId":"10875792761423186902"},"user_tz":420},"id":"WWnZEMxs94co","outputId":"c97f5c67-b33d-48e4-d5e8-f243a23a6cf1"},"outputs":[{"name":"stdout","output_type":"stream","text":["(' In a series of letters from African journalists, filmmaker and columnist '\n"," 'Adelisa Fonseca looks back at her recent cold weather.')\n"]}],"source":["# compare to original unfit model. (prediction is different, good). \n","model_original = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint)\n","clear_output()\n","output = model_original.generate(torch.tensor([tokenized_datasets['test_advice_story']['input_ids'][0]]), num_beams=2, max_length=60, min_length=2, no_repeat_ngram_size=3)\n","output_decoded = tokenizer.decode(output.squeeze(), skip_special_tokens=True)\n","pprint(output_decoded)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"adkFqhM_57Q_"},"outputs":[],"source":["# Load from huggingface (haven't configured yet)\n","# new_checkpoint = 'trevorj/BART-reddit-advice_story'\n","# model_advice_story = AutoModelForSeq2SeqLM.from_pretrained(new_checkpoint)\n","# tokenizer_advice_story = AutoTokenizer.from_pretrained(new_checkpoint)"]},{"cell_type":"markdown","metadata":{"id":"7Qrav_TyIexv"},"source":["# Train - media_lifestyle_sports\n","- 1:12 hrs"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"elapsed":1644240,"status":"ok","timestamp":1658384948882,"user":{"displayName":"Trevor Johnson","userId":"10875792761423186902"},"user_tz":420},"id":"QTkIL8lxOoan","outputId":"bdae6ccd-a47b-4bdb-cd1b-f5e9c2a69555"},"outputs":[{"name":"stderr","output_type":"stream","text":["PyTorch: setting up devices\n","The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n","Cloning https://huggingface.co/trevorj/BART_reddit_media_lifestyle_sports into local empty directory.\n","WARNING:huggingface_hub.repository:Cloning https://huggingface.co/trevorj/BART_reddit_media_lifestyle_sports into local empty directory.\n","Using cuda_amp half precision backend\n","The following columns in the training set don't have a corresponding argument in `BartForConditionalGeneration.forward` and have been ignored: subreddit_group, subreddit, content, summary. If subreddit_group, subreddit, content, summary are not expected by `BartForConditionalGeneration.forward`,  you can safely ignore this message.\n","/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  FutureWarning,\n","***** Running training *****\n","  Num examples = 15000\n","  Num Epochs = 3\n","  Instantaneous batch size per device = 8\n","  Total train batch size (w. parallel, distributed & accumulation) = 8\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 5625\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='3668' max='5625' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [3668/5625 45:15 < 24:09, 1.35 it/s, Epoch 1.96/3]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Rouge1</th>\n","      <th>Rouge2</th>\n","      <th>Rougel</th>\n","      <th>Rougelsum</th>\n","      <th>Gen Len</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>3.654200</td>\n","      <td>3.548437</td>\n","      <td>15.970400</td>\n","      <td>4.288100</td>\n","      <td>13.462100</td>\n","      <td>13.985700</td>\n","      <td>17.876000</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Saving model checkpoint to BART_reddit_media_lifestyle_sports/checkpoint-500\n","Configuration saved in BART_reddit_media_lifestyle_sports/checkpoint-500/config.json\n","Model weights saved in BART_reddit_media_lifestyle_sports/checkpoint-500/pytorch_model.bin\n","tokenizer config file saved in BART_reddit_media_lifestyle_sports/checkpoint-500/tokenizer_config.json\n","Special tokens file saved in BART_reddit_media_lifestyle_sports/checkpoint-500/special_tokens_map.json\n","tokenizer config file saved in BART_reddit_media_lifestyle_sports/tokenizer_config.json\n","Special tokens file saved in BART_reddit_media_lifestyle_sports/special_tokens_map.json\n","Saving model checkpoint to BART_reddit_media_lifestyle_sports/checkpoint-1000\n","Configuration saved in BART_reddit_media_lifestyle_sports/checkpoint-1000/config.json\n","Model weights saved in BART_reddit_media_lifestyle_sports/checkpoint-1000/pytorch_model.bin\n","tokenizer config file saved in BART_reddit_media_lifestyle_sports/checkpoint-1000/tokenizer_config.json\n","Special tokens file saved in BART_reddit_media_lifestyle_sports/checkpoint-1000/special_tokens_map.json\n","Deleting older checkpoint [BART_reddit_media_lifestyle_sports/checkpoint-500] due to args.save_total_limit\n","Saving model checkpoint to BART_reddit_media_lifestyle_sports/checkpoint-1500\n","Configuration saved in BART_reddit_media_lifestyle_sports/checkpoint-1500/config.json\n","Model weights saved in BART_reddit_media_lifestyle_sports/checkpoint-1500/pytorch_model.bin\n","tokenizer config file saved in BART_reddit_media_lifestyle_sports/checkpoint-1500/tokenizer_config.json\n","Special tokens file saved in BART_reddit_media_lifestyle_sports/checkpoint-1500/special_tokens_map.json\n","Deleting older checkpoint [BART_reddit_media_lifestyle_sports/checkpoint-1000] due to args.save_total_limit\n","The following columns in the evaluation set don't have a corresponding argument in `BartForConditionalGeneration.forward` and have been ignored: subreddit_group, subreddit, content, summary. If subreddit_group, subreddit, content, summary are not expected by `BartForConditionalGeneration.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 1000\n","  Batch size = 8\n","Saving model checkpoint to BART_reddit_media_lifestyle_sports/checkpoint-2000\n","Configuration saved in BART_reddit_media_lifestyle_sports/checkpoint-2000/config.json\n","Model weights saved in BART_reddit_media_lifestyle_sports/checkpoint-2000/pytorch_model.bin\n","tokenizer config file saved in BART_reddit_media_lifestyle_sports/checkpoint-2000/tokenizer_config.json\n","Special tokens file saved in BART_reddit_media_lifestyle_sports/checkpoint-2000/special_tokens_map.json\n","Deleting older checkpoint [BART_reddit_media_lifestyle_sports/checkpoint-1500] due to args.save_total_limit\n","Saving model checkpoint to BART_reddit_media_lifestyle_sports/checkpoint-2500\n","Configuration saved in BART_reddit_media_lifestyle_sports/checkpoint-2500/config.json\n","Model weights saved in BART_reddit_media_lifestyle_sports/checkpoint-2500/pytorch_model.bin\n","tokenizer config file saved in BART_reddit_media_lifestyle_sports/checkpoint-2500/tokenizer_config.json\n","Special tokens file saved in BART_reddit_media_lifestyle_sports/checkpoint-2500/special_tokens_map.json\n","Deleting older checkpoint [BART_reddit_media_lifestyle_sports/checkpoint-2000] due to args.save_total_limit\n","Saving model checkpoint to BART_reddit_media_lifestyle_sports/checkpoint-3000\n","Configuration saved in BART_reddit_media_lifestyle_sports/checkpoint-3000/config.json\n","Model weights saved in BART_reddit_media_lifestyle_sports/checkpoint-3000/pytorch_model.bin\n","tokenizer config file saved in BART_reddit_media_lifestyle_sports/checkpoint-3000/tokenizer_config.json\n","Special tokens file saved in BART_reddit_media_lifestyle_sports/checkpoint-3000/special_tokens_map.json\n","Deleting older checkpoint [BART_reddit_media_lifestyle_sports/checkpoint-2500] due to args.save_total_limit\n","Saving model checkpoint to BART_reddit_media_lifestyle_sports/checkpoint-3500\n","Configuration saved in BART_reddit_media_lifestyle_sports/checkpoint-3500/config.json\n","Model weights saved in BART_reddit_media_lifestyle_sports/checkpoint-3500/pytorch_model.bin\n","tokenizer config file saved in BART_reddit_media_lifestyle_sports/checkpoint-3500/tokenizer_config.json\n","Special tokens file saved in BART_reddit_media_lifestyle_sports/checkpoint-3500/special_tokens_map.json\n","Deleting older checkpoint [BART_reddit_media_lifestyle_sports/checkpoint-3000] due to args.save_total_limit\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='5625' max='5625' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [5625/5625 1:12:40, Epoch 3/3]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Rouge1</th>\n","      <th>Rouge2</th>\n","      <th>Rougel</th>\n","      <th>Rougelsum</th>\n","      <th>Gen Len</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>3.654200</td>\n","      <td>3.548437</td>\n","      <td>15.970400</td>\n","      <td>4.288100</td>\n","      <td>13.462100</td>\n","      <td>13.985700</td>\n","      <td>17.876000</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>3.271900</td>\n","      <td>3.540086</td>\n","      <td>15.973300</td>\n","      <td>4.227100</td>\n","      <td>13.440100</td>\n","      <td>14.055000</td>\n","      <td>18.020000</td>\n","    </tr>\n","    <tr>\n","      <td>3</td>\n","      <td>3.030100</td>\n","      <td>3.562253</td>\n","      <td>16.246300</td>\n","      <td>4.329600</td>\n","      <td>13.772500</td>\n","      <td>14.350400</td>\n","      <td>17.760000</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["The following columns in the evaluation set don't have a corresponding argument in `BartForConditionalGeneration.forward` and have been ignored: subreddit_group, subreddit, content, summary. If subreddit_group, subreddit, content, summary are not expected by `BartForConditionalGeneration.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 1000\n","  Batch size = 8\n","Saving model checkpoint to BART_reddit_media_lifestyle_sports/checkpoint-4000\n","Configuration saved in BART_reddit_media_lifestyle_sports/checkpoint-4000/config.json\n","Model weights saved in BART_reddit_media_lifestyle_sports/checkpoint-4000/pytorch_model.bin\n","tokenizer config file saved in BART_reddit_media_lifestyle_sports/checkpoint-4000/tokenizer_config.json\n","Special tokens file saved in BART_reddit_media_lifestyle_sports/checkpoint-4000/special_tokens_map.json\n","Deleting older checkpoint [BART_reddit_media_lifestyle_sports/checkpoint-3500] due to args.save_total_limit\n","Saving model checkpoint to BART_reddit_media_lifestyle_sports/checkpoint-4500\n","Configuration saved in BART_reddit_media_lifestyle_sports/checkpoint-4500/config.json\n","Model weights saved in BART_reddit_media_lifestyle_sports/checkpoint-4500/pytorch_model.bin\n","tokenizer config file saved in BART_reddit_media_lifestyle_sports/checkpoint-4500/tokenizer_config.json\n","Special tokens file saved in BART_reddit_media_lifestyle_sports/checkpoint-4500/special_tokens_map.json\n","Deleting older checkpoint [BART_reddit_media_lifestyle_sports/checkpoint-4000] due to args.save_total_limit\n","Saving model checkpoint to BART_reddit_media_lifestyle_sports/checkpoint-5000\n","Configuration saved in BART_reddit_media_lifestyle_sports/checkpoint-5000/config.json\n","Model weights saved in BART_reddit_media_lifestyle_sports/checkpoint-5000/pytorch_model.bin\n","tokenizer config file saved in BART_reddit_media_lifestyle_sports/checkpoint-5000/tokenizer_config.json\n","Special tokens file saved in BART_reddit_media_lifestyle_sports/checkpoint-5000/special_tokens_map.json\n","Deleting older checkpoint [BART_reddit_media_lifestyle_sports/checkpoint-4500] due to args.save_total_limit\n","Saving model checkpoint to BART_reddit_media_lifestyle_sports/checkpoint-5500\n","Configuration saved in BART_reddit_media_lifestyle_sports/checkpoint-5500/config.json\n","Model weights saved in BART_reddit_media_lifestyle_sports/checkpoint-5500/pytorch_model.bin\n","tokenizer config file saved in BART_reddit_media_lifestyle_sports/checkpoint-5500/tokenizer_config.json\n","Special tokens file saved in BART_reddit_media_lifestyle_sports/checkpoint-5500/special_tokens_map.json\n","Deleting older checkpoint [BART_reddit_media_lifestyle_sports/checkpoint-5000] due to args.save_total_limit\n","The following columns in the evaluation set don't have a corresponding argument in `BartForConditionalGeneration.forward` and have been ignored: subreddit_group, subreddit, content, summary. If subreddit_group, subreddit, content, summary are not expected by `BartForConditionalGeneration.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 1000\n","  Batch size = 8\n","\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n"]},{"name":"stdout","output_type":"stream","text":["CPU times: user 1h 5min 6s, sys: 4min 42s, total: 1h 9min 48s\n","Wall time: 1h 12min 49s\n"]}],"source":["%%time\n","\n","args = Seq2SeqTrainingArguments(\n","    f\"BART_reddit_media_lifestyle_sports\",\n","    evaluation_strategy = \"epoch\",\n","    learning_rate=2e-5,\n","    per_device_train_batch_size=8, # 16\n","    per_device_eval_batch_size=8, #16\n","    weight_decay=0.01,\n","    save_total_limit=1, #3,\n","    num_train_epochs=3, # 1\n","    predict_with_generate=True,\n","    fp16=True,\n","    push_to_hub=True,\n",")\n","\n","trainer_media = Seq2SeqTrainer(\n","    model,\n","    args,\n","    train_dataset=tokenized_datasets[\"train_media_lifestyle_sports\"],\n","    eval_dataset=tokenized_datasets[\"valid_media_lifestyle_sports\"],\n","    data_collator=data_collator,\n","    tokenizer=tokenizer,\n","    compute_metrics=compute_metrics\n",")\n","\n","# this should save the model to disk. changing wd so that it saves here:\n","fit_path = '/content/gdrive/MyDrive/w266/w266_reddit_summarization/data/model_outputs/bart_fitted_models/round2'\n","os.chdir(fit_path)\n","trainer_media.train()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xzmfDKqjPKca"},"outputs":[],"source":["# create huggingface repo\n","trainer_media.push_to_hub()"]},{"cell_type":"markdown","metadata":{"id":"AWe_p3rDPdWv"},"source":["# Train gaming\n","- 1:13 hrs"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"elapsed":4386844,"status":"ok","timestamp":1658421256589,"user":{"displayName":"Trevor Johnson","userId":"10875792761423186902"},"user_tz":420},"id":"sjt7hC_kPdGc","outputId":"2b4ec6fb-935d-4159-9355-d9c35180e356"},"outputs":[{"name":"stderr","output_type":"stream","text":["PyTorch: setting up devices\n","The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n","Cloning https://huggingface.co/trevorj/BART_reddit_gaming into local empty directory.\n","Using cuda_amp half precision backend\n","The following columns in the training set don't have a corresponding argument in `BartForConditionalGeneration.forward` and have been ignored: subreddit_group, content, summary, subreddit. If subreddit_group, content, summary, subreddit are not expected by `BartForConditionalGeneration.forward`,  you can safely ignore this message.\n","/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  FutureWarning,\n","***** Running training *****\n","  Num examples = 15000\n","  Num Epochs = 3\n","  Instantaneous batch size per device = 8\n","  Total train batch size (w. parallel, distributed & accumulation) = 8\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 5625\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='5625' max='5625' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [5625/5625 1:12:57, Epoch 3/3]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Rouge1</th>\n","      <th>Rouge2</th>\n","      <th>Rougel</th>\n","      <th>Rougelsum</th>\n","      <th>Gen Len</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>3.864000</td>\n","      <td>3.775191</td>\n","      <td>17.375400</td>\n","      <td>4.510000</td>\n","      <td>14.676300</td>\n","      <td>15.220000</td>\n","      <td>16.944000</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>3.475500</td>\n","      <td>3.726480</td>\n","      <td>17.806600</td>\n","      <td>4.418800</td>\n","      <td>14.943200</td>\n","      <td>15.539600</td>\n","      <td>18.104000</td>\n","    </tr>\n","    <tr>\n","      <td>3</td>\n","      <td>3.262900</td>\n","      <td>3.737317</td>\n","      <td>18.120200</td>\n","      <td>4.604500</td>\n","      <td>15.127300</td>\n","      <td>15.760100</td>\n","      <td>18.208000</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Saving model checkpoint to BART_reddit_gaming/checkpoint-500\n","Configuration saved in BART_reddit_gaming/checkpoint-500/config.json\n","Model weights saved in BART_reddit_gaming/checkpoint-500/pytorch_model.bin\n","tokenizer config file saved in BART_reddit_gaming/checkpoint-500/tokenizer_config.json\n","Special tokens file saved in BART_reddit_gaming/checkpoint-500/special_tokens_map.json\n","tokenizer config file saved in BART_reddit_gaming/tokenizer_config.json\n","Special tokens file saved in BART_reddit_gaming/special_tokens_map.json\n","Saving model checkpoint to BART_reddit_gaming/checkpoint-1000\n","Configuration saved in BART_reddit_gaming/checkpoint-1000/config.json\n","Model weights saved in BART_reddit_gaming/checkpoint-1000/pytorch_model.bin\n","tokenizer config file saved in BART_reddit_gaming/checkpoint-1000/tokenizer_config.json\n","Special tokens file saved in BART_reddit_gaming/checkpoint-1000/special_tokens_map.json\n","Deleting older checkpoint [BART_reddit_gaming/checkpoint-500] due to args.save_total_limit\n","Saving model checkpoint to BART_reddit_gaming/checkpoint-1500\n","Configuration saved in BART_reddit_gaming/checkpoint-1500/config.json\n","Model weights saved in BART_reddit_gaming/checkpoint-1500/pytorch_model.bin\n","tokenizer config file saved in BART_reddit_gaming/checkpoint-1500/tokenizer_config.json\n","Special tokens file saved in BART_reddit_gaming/checkpoint-1500/special_tokens_map.json\n","Deleting older checkpoint [BART_reddit_gaming/checkpoint-1000] due to args.save_total_limit\n","The following columns in the evaluation set don't have a corresponding argument in `BartForConditionalGeneration.forward` and have been ignored: subreddit_group, content, summary, subreddit. If subreddit_group, content, summary, subreddit are not expected by `BartForConditionalGeneration.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 1000\n","  Batch size = 8\n","Saving model checkpoint to BART_reddit_gaming/checkpoint-2000\n","Configuration saved in BART_reddit_gaming/checkpoint-2000/config.json\n","Model weights saved in BART_reddit_gaming/checkpoint-2000/pytorch_model.bin\n","tokenizer config file saved in BART_reddit_gaming/checkpoint-2000/tokenizer_config.json\n","Special tokens file saved in BART_reddit_gaming/checkpoint-2000/special_tokens_map.json\n","Deleting older checkpoint [BART_reddit_gaming/checkpoint-1500] due to args.save_total_limit\n","Saving model checkpoint to BART_reddit_gaming/checkpoint-2500\n","Configuration saved in BART_reddit_gaming/checkpoint-2500/config.json\n","Model weights saved in BART_reddit_gaming/checkpoint-2500/pytorch_model.bin\n","tokenizer config file saved in BART_reddit_gaming/checkpoint-2500/tokenizer_config.json\n","Special tokens file saved in BART_reddit_gaming/checkpoint-2500/special_tokens_map.json\n","Deleting older checkpoint [BART_reddit_gaming/checkpoint-2000] due to args.save_total_limit\n","Saving model checkpoint to BART_reddit_gaming/checkpoint-3000\n","Configuration saved in BART_reddit_gaming/checkpoint-3000/config.json\n","Model weights saved in BART_reddit_gaming/checkpoint-3000/pytorch_model.bin\n","tokenizer config file saved in BART_reddit_gaming/checkpoint-3000/tokenizer_config.json\n","Special tokens file saved in BART_reddit_gaming/checkpoint-3000/special_tokens_map.json\n","Deleting older checkpoint [BART_reddit_gaming/checkpoint-2500] due to args.save_total_limit\n","Saving model checkpoint to BART_reddit_gaming/checkpoint-3500\n","Configuration saved in BART_reddit_gaming/checkpoint-3500/config.json\n","Model weights saved in BART_reddit_gaming/checkpoint-3500/pytorch_model.bin\n","tokenizer config file saved in BART_reddit_gaming/checkpoint-3500/tokenizer_config.json\n","Special tokens file saved in BART_reddit_gaming/checkpoint-3500/special_tokens_map.json\n","Deleting older checkpoint [BART_reddit_gaming/checkpoint-3000] due to args.save_total_limit\n","The following columns in the evaluation set don't have a corresponding argument in `BartForConditionalGeneration.forward` and have been ignored: subreddit_group, content, summary, subreddit. If subreddit_group, content, summary, subreddit are not expected by `BartForConditionalGeneration.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 1000\n","  Batch size = 8\n","Saving model checkpoint to BART_reddit_gaming/checkpoint-4000\n","Configuration saved in BART_reddit_gaming/checkpoint-4000/config.json\n","Model weights saved in BART_reddit_gaming/checkpoint-4000/pytorch_model.bin\n","tokenizer config file saved in BART_reddit_gaming/checkpoint-4000/tokenizer_config.json\n","Special tokens file saved in BART_reddit_gaming/checkpoint-4000/special_tokens_map.json\n","Deleting older checkpoint [BART_reddit_gaming/checkpoint-3500] due to args.save_total_limit\n","Saving model checkpoint to BART_reddit_gaming/checkpoint-4500\n","Configuration saved in BART_reddit_gaming/checkpoint-4500/config.json\n","Model weights saved in BART_reddit_gaming/checkpoint-4500/pytorch_model.bin\n","tokenizer config file saved in BART_reddit_gaming/checkpoint-4500/tokenizer_config.json\n","Special tokens file saved in BART_reddit_gaming/checkpoint-4500/special_tokens_map.json\n","Deleting older checkpoint [BART_reddit_gaming/checkpoint-4000] due to args.save_total_limit\n","Saving model checkpoint to BART_reddit_gaming/checkpoint-5000\n","Configuration saved in BART_reddit_gaming/checkpoint-5000/config.json\n","Model weights saved in BART_reddit_gaming/checkpoint-5000/pytorch_model.bin\n","tokenizer config file saved in BART_reddit_gaming/checkpoint-5000/tokenizer_config.json\n","Special tokens file saved in BART_reddit_gaming/checkpoint-5000/special_tokens_map.json\n","Deleting older checkpoint [BART_reddit_gaming/checkpoint-4500] due to args.save_total_limit\n","Saving model checkpoint to BART_reddit_gaming/checkpoint-5500\n","Configuration saved in BART_reddit_gaming/checkpoint-5500/config.json\n","Model weights saved in BART_reddit_gaming/checkpoint-5500/pytorch_model.bin\n","tokenizer config file saved in BART_reddit_gaming/checkpoint-5500/tokenizer_config.json\n","Special tokens file saved in BART_reddit_gaming/checkpoint-5500/special_tokens_map.json\n","Deleting older checkpoint [BART_reddit_gaming/checkpoint-5000] due to args.save_total_limit\n","The following columns in the evaluation set don't have a corresponding argument in `BartForConditionalGeneration.forward` and have been ignored: subreddit_group, content, summary, subreddit. If subreddit_group, content, summary, subreddit are not expected by `BartForConditionalGeneration.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 1000\n","  Batch size = 8\n","\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n"]},{"name":"stdout","output_type":"stream","text":["CPU times: user 1h 5min 19s, sys: 5min 48s, total: 1h 11min 8s\n","Wall time: 1h 13min 6s\n"]}],"source":["%%time\n","\n","args = Seq2SeqTrainingArguments(\n","    f\"BART_reddit_gaming\",\n","    evaluation_strategy = \"epoch\",\n","    learning_rate=2e-5,\n","    per_device_train_batch_size=8, # 16\n","    per_device_eval_batch_size=8, #16\n","    weight_decay=0.01,\n","    save_total_limit=1, #3,\n","    num_train_epochs=3, # 1\n","    predict_with_generate=True,\n","    fp16=True,\n","    push_to_hub=True,\n",")\n","\n","trainer_gaming = Seq2SeqTrainer(\n","    model,\n","    args,\n","    train_dataset=tokenized_datasets[\"train_gaming\"],\n","    eval_dataset=tokenized_datasets[\"valid_gaming\"],\n","    data_collator=data_collator,\n","    tokenizer=tokenizer,\n","    compute_metrics=compute_metrics\n",")\n","\n","# this should save the model to disk. changing wd so that it saves here:\n","fit_path = '/content/gdrive/MyDrive/w266/w266_reddit_summarization/data/model_outputs/bart_fitted_models/round2'\n","os.chdir(fit_path)\n","trainer_gaming.train()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5307,"status":"ok","timestamp":1658421906098,"user":{"displayName":"Trevor Johnson","userId":"10875792761423186902"},"user_tz":420},"id":"rKmWPYgTP4QS","outputId":"3991a711-005a-4499-9024-fc366db36d0c"},"outputs":[{"name":"stderr","output_type":"stream","text":["Saving model checkpoint to BART_reddit_gaming\n","Configuration saved in BART_reddit_gaming/config.json\n","Model weights saved in BART_reddit_gaming/pytorch_model.bin\n","tokenizer config file saved in BART_reddit_gaming/tokenizer_config.json\n","Special tokens file saved in BART_reddit_gaming/special_tokens_map.json\n","Dropping the following result as it does not have all the necessary fields:\n","{'task': {'name': 'Sequence-to-sequence Language Modeling', 'type': 'text2text-generation'}, 'metrics': [{'name': 'Rouge1', 'type': 'rouge', 'value': 18.1202}]}\n"]}],"source":["# create huggingface repo\n","trainer_gaming.push_to_hub()"]},{"cell_type":"markdown","metadata":{"id":"2oZWzrpVPkWa"},"source":["# Train other\n","- 1:17 hrs"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"elapsed":4804173,"status":"ok","timestamp":1658426973253,"user":{"displayName":"Trevor Johnson","userId":"10875792761423186902"},"user_tz":420},"id":"7yAnIoWtPk8b","outputId":"653f9ac5-a634-4be3-947a-49fefaeb1d4f"},"outputs":[{"name":"stderr","output_type":"stream","text":["Cloning https://huggingface.co/trevorj/BART_reddit_other into local empty directory.\n","Using cuda_amp half precision backend\n","The following columns in the training set don't have a corresponding argument in `BartForConditionalGeneration.forward` and have been ignored: content, subreddit_group, subreddit, summary. If content, subreddit_group, subreddit, summary are not expected by `BartForConditionalGeneration.forward`,  you can safely ignore this message.\n","/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  FutureWarning,\n","***** Running training *****\n","  Num examples = 15000\n","  Num Epochs = 3\n","  Instantaneous batch size per device = 8\n","  Total train batch size (w. parallel, distributed & accumulation) = 8\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 5625\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='5625' max='5625' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [5625/5625 1:19:49, Epoch 3/3]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Rouge1</th>\n","      <th>Rouge2</th>\n","      <th>Rougel</th>\n","      <th>Rougelsum</th>\n","      <th>Gen Len</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>3.788700</td>\n","      <td>3.604443</td>\n","      <td>18.466800</td>\n","      <td>5.182000</td>\n","      <td>15.359000</td>\n","      <td>16.169000</td>\n","      <td>19.341000</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>3.381600</td>\n","      <td>3.562785</td>\n","      <td>18.099800</td>\n","      <td>4.893700</td>\n","      <td>15.017900</td>\n","      <td>15.761500</td>\n","      <td>17.789000</td>\n","    </tr>\n","    <tr>\n","      <td>3</td>\n","      <td>3.134000</td>\n","      <td>3.579218</td>\n","      <td>18.570500</td>\n","      <td>5.010700</td>\n","      <td>15.258100</td>\n","      <td>16.082000</td>\n","      <td>19.402000</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Saving model checkpoint to BART_reddit_other/checkpoint-500\n","Configuration saved in BART_reddit_other/checkpoint-500/config.json\n","Model weights saved in BART_reddit_other/checkpoint-500/pytorch_model.bin\n","tokenizer config file saved in BART_reddit_other/checkpoint-500/tokenizer_config.json\n","Special tokens file saved in BART_reddit_other/checkpoint-500/special_tokens_map.json\n","tokenizer config file saved in BART_reddit_other/tokenizer_config.json\n","Special tokens file saved in BART_reddit_other/special_tokens_map.json\n","Saving model checkpoint to BART_reddit_other/checkpoint-1000\n","Configuration saved in BART_reddit_other/checkpoint-1000/config.json\n","Model weights saved in BART_reddit_other/checkpoint-1000/pytorch_model.bin\n","tokenizer config file saved in BART_reddit_other/checkpoint-1000/tokenizer_config.json\n","Special tokens file saved in BART_reddit_other/checkpoint-1000/special_tokens_map.json\n","Deleting older checkpoint [BART_reddit_other/checkpoint-500] due to args.save_total_limit\n","Saving model checkpoint to BART_reddit_other/checkpoint-1500\n","Configuration saved in BART_reddit_other/checkpoint-1500/config.json\n","Model weights saved in BART_reddit_other/checkpoint-1500/pytorch_model.bin\n","tokenizer config file saved in BART_reddit_other/checkpoint-1500/tokenizer_config.json\n","Special tokens file saved in BART_reddit_other/checkpoint-1500/special_tokens_map.json\n","Deleting older checkpoint [BART_reddit_other/checkpoint-1000] due to args.save_total_limit\n","The following columns in the evaluation set don't have a corresponding argument in `BartForConditionalGeneration.forward` and have been ignored: content, subreddit_group, subreddit, summary. If content, subreddit_group, subreddit, summary are not expected by `BartForConditionalGeneration.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 1000\n","  Batch size = 8\n","Saving model checkpoint to BART_reddit_other/checkpoint-2000\n","Configuration saved in BART_reddit_other/checkpoint-2000/config.json\n","Model weights saved in BART_reddit_other/checkpoint-2000/pytorch_model.bin\n","tokenizer config file saved in BART_reddit_other/checkpoint-2000/tokenizer_config.json\n","Special tokens file saved in BART_reddit_other/checkpoint-2000/special_tokens_map.json\n","Deleting older checkpoint [BART_reddit_other/checkpoint-1500] due to args.save_total_limit\n","Saving model checkpoint to BART_reddit_other/checkpoint-2500\n","Configuration saved in BART_reddit_other/checkpoint-2500/config.json\n","Model weights saved in BART_reddit_other/checkpoint-2500/pytorch_model.bin\n","tokenizer config file saved in BART_reddit_other/checkpoint-2500/tokenizer_config.json\n","Special tokens file saved in BART_reddit_other/checkpoint-2500/special_tokens_map.json\n","Deleting older checkpoint [BART_reddit_other/checkpoint-2000] due to args.save_total_limit\n","Saving model checkpoint to BART_reddit_other/checkpoint-3000\n","Configuration saved in BART_reddit_other/checkpoint-3000/config.json\n","Model weights saved in BART_reddit_other/checkpoint-3000/pytorch_model.bin\n","tokenizer config file saved in BART_reddit_other/checkpoint-3000/tokenizer_config.json\n","Special tokens file saved in BART_reddit_other/checkpoint-3000/special_tokens_map.json\n","Deleting older checkpoint [BART_reddit_other/checkpoint-2500] due to args.save_total_limit\n","Saving model checkpoint to BART_reddit_other/checkpoint-3500\n","Configuration saved in BART_reddit_other/checkpoint-3500/config.json\n","Model weights saved in BART_reddit_other/checkpoint-3500/pytorch_model.bin\n","tokenizer config file saved in BART_reddit_other/checkpoint-3500/tokenizer_config.json\n","Special tokens file saved in BART_reddit_other/checkpoint-3500/special_tokens_map.json\n","Deleting older checkpoint [BART_reddit_other/checkpoint-3000] due to args.save_total_limit\n","The following columns in the evaluation set don't have a corresponding argument in `BartForConditionalGeneration.forward` and have been ignored: content, subreddit_group, subreddit, summary. If content, subreddit_group, subreddit, summary are not expected by `BartForConditionalGeneration.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 1000\n","  Batch size = 8\n","Saving model checkpoint to BART_reddit_other/checkpoint-4000\n","Configuration saved in BART_reddit_other/checkpoint-4000/config.json\n","Model weights saved in BART_reddit_other/checkpoint-4000/pytorch_model.bin\n","tokenizer config file saved in BART_reddit_other/checkpoint-4000/tokenizer_config.json\n","Special tokens file saved in BART_reddit_other/checkpoint-4000/special_tokens_map.json\n","Deleting older checkpoint [BART_reddit_other/checkpoint-3500] due to args.save_total_limit\n","Saving model checkpoint to BART_reddit_other/checkpoint-4500\n","Configuration saved in BART_reddit_other/checkpoint-4500/config.json\n","Model weights saved in BART_reddit_other/checkpoint-4500/pytorch_model.bin\n","tokenizer config file saved in BART_reddit_other/checkpoint-4500/tokenizer_config.json\n","Special tokens file saved in BART_reddit_other/checkpoint-4500/special_tokens_map.json\n","Deleting older checkpoint [BART_reddit_other/checkpoint-4000] due to args.save_total_limit\n","Saving model checkpoint to BART_reddit_other/checkpoint-5000\n","Configuration saved in BART_reddit_other/checkpoint-5000/config.json\n","Model weights saved in BART_reddit_other/checkpoint-5000/pytorch_model.bin\n","tokenizer config file saved in BART_reddit_other/checkpoint-5000/tokenizer_config.json\n","Special tokens file saved in BART_reddit_other/checkpoint-5000/special_tokens_map.json\n","Deleting older checkpoint [BART_reddit_other/checkpoint-4500] due to args.save_total_limit\n","Saving model checkpoint to BART_reddit_other/checkpoint-5500\n","Configuration saved in BART_reddit_other/checkpoint-5500/config.json\n","Model weights saved in BART_reddit_other/checkpoint-5500/pytorch_model.bin\n","tokenizer config file saved in BART_reddit_other/checkpoint-5500/tokenizer_config.json\n","Special tokens file saved in BART_reddit_other/checkpoint-5500/special_tokens_map.json\n","Deleting older checkpoint [BART_reddit_other/checkpoint-5000] due to args.save_total_limit\n","The following columns in the evaluation set don't have a corresponding argument in `BartForConditionalGeneration.forward` and have been ignored: content, subreddit_group, subreddit, summary. If content, subreddit_group, subreddit, summary are not expected by `BartForConditionalGeneration.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 1000\n","  Batch size = 8\n","\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n"]},{"name":"stdout","output_type":"stream","text":["CPU times: user 1h 11min 53s, sys: 5min 57s, total: 1h 17min 50s\n","Wall time: 1h 20min 4s\n"]}],"source":["%%time\n","\n","args = Seq2SeqTrainingArguments(\n","    f\"BART_reddit_other\",\n","    evaluation_strategy = \"epoch\",\n","    learning_rate=2e-5,\n","    per_device_train_batch_size=8, # 16\n","    per_device_eval_batch_size=8, #16\n","    weight_decay=0.01,\n","    save_total_limit=1, #3,\n","    num_train_epochs=3, # 1\n","    predict_with_generate=True,\n","    fp16=True,\n","    push_to_hub=True,\n",")\n","\n","trainer_other = Seq2SeqTrainer(\n","    model,\n","    args,\n","    train_dataset=tokenized_datasets[\"train_other\"],\n","    eval_dataset=tokenized_datasets[\"valid_other\"],\n","    data_collator=data_collator,\n","    tokenizer=tokenizer,\n","    compute_metrics=compute_metrics\n",")\n","\n","# this should save the model to disk. changing wd so that it saves here:\n","fit_path = '/content/gdrive/MyDrive/w266/w266_reddit_summarization/data/model_outputs/bart_fitted_models/round2'\n","os.chdir(fit_path)\n","trainer_other.train()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5086,"status":"ok","timestamp":1658426978336,"user":{"displayName":"Trevor Johnson","userId":"10875792761423186902"},"user_tz":420},"id":"HhgkyHmQP5s7","outputId":"d8b32b5d-f00c-4951-94aa-8ebc87b438bb"},"outputs":[{"name":"stderr","output_type":"stream","text":["Saving model checkpoint to BART_reddit_other\n","Configuration saved in BART_reddit_other/config.json\n","Model weights saved in BART_reddit_other/pytorch_model.bin\n","tokenizer config file saved in BART_reddit_other/tokenizer_config.json\n","Special tokens file saved in BART_reddit_other/special_tokens_map.json\n","Dropping the following result as it does not have all the necessary fields:\n","{'task': {'name': 'Sequence-to-sequence Language Modeling', 'type': 'text2text-generation'}, 'metrics': [{'name': 'Rouge1', 'type': 'rouge', 'value': 18.5705}]}\n"]}],"source":["# create huggingface repo\n","trainer_other.push_to_hub()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TNAodjWeQCx6"},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"authorship_tag":"ABX9TyPalUV5A1JddJ+Zof4BvmkR","collapsed_sections":[],"name":"2_BART_training_fullDF.ipynb","provenance":[]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3.10.2 64-bit","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.10.2"},"vscode":{"interpreter":{"hash":"aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"}},"widgets":{"application/vnd.jupyter.widget-state+json":{"06682a88e16a4f50bf231c6207310638":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"0e0e0d499d4d4e4d9308a5dec1055699":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1add1bc41ca944ceaf3f4def4c68d34e":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"232f0a75ffc843218d3ce903bf69a756":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c04b3b3a3d7a403b90b919eb611ee925","placeholder":"","style":"IPY_MODEL_b256eae206544d1bab1282d326debec7","value":"100%"}},"27ee43e81888408aa874a569a0de57e0":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_4a31a7ef703e46b3b562c82148c526b4","placeholder":"","style":"IPY_MODEL_3c2bb074cf394352b6a7c0c5083be919","value":"100%"}},"29b082b7ef2d461ab6884d300990d7a0":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d16c036d8d9d4c3c892edb7551b31e44","placeholder":"","style":"IPY_MODEL_93a26a45055249a99e2b1537a90454d2","value":"100%"}},"334b1cbc12f14eb48b1f906f13eb5d1a":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"334b2b2e0a6f4269b6640cb3456a27e0":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ButtonStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ButtonStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","button_color":null,"font_weight":""}},"377d603b4366429294d21fae7ca86aa2":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3ac7db03816d4f13915b006fce5c1786":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3b4eca3725634087a7840e0a92722c6e":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3c2bb074cf394352b6a7c0c5083be919":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"3d7ab64789e645778443d3b16dd89685":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_29b082b7ef2d461ab6884d300990d7a0","IPY_MODEL_7cbf1b99dc9e4a518a8906f88ed348ba","IPY_MODEL_492de51cec0c484facd2c35c38c31154"],"layout":"IPY_MODEL_84e260ce602e43c8a42b0cc87fc4fefa"}},"3fda19722e59409597470dd0110efc08":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_232f0a75ffc843218d3ce903bf69a756","IPY_MODEL_8ac28d9522f647ab84d3785510f85cfe","IPY_MODEL_ba52e8b08abe41c69cf8904014a12bc6"],"layout":"IPY_MODEL_af94b304158d451f8224240f55ccd2cb"}},"414449a196a44a6094ca6d7496ccd4de":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"492de51cec0c484facd2c35c38c31154":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_9b18efeac7d84f8e9d182129d8787a8d","placeholder":"","style":"IPY_MODEL_97b585d3fdd8480b8789cfc2b30be260","value":" 4/4 [00:02&lt;00:00,  1.55ba/s]"}},"4957975f68a645d8b0b6386c2ec5d873":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4a31a7ef703e46b3b562c82148c526b4":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4d3eefdcf1db43bf9a723d49ebcdf5a6":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"4d515b67918f46158926276af4879f0a":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4ec242f9663b47ab89beaf10d505a22b":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"50a634cf333b411ebdfab2245fc192cb":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5786fb1e5ea041d5a1fd7338487f84f0":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"63f1381b2ad14a05a56178c1d87be274":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_3ac7db03816d4f13915b006fce5c1786","placeholder":"","style":"IPY_MODEL_e3d7cb3140374328af657c177b889807","value":"\n<b>Pro Tip:</b> If you don't already have one, you can create a dedicated\n'notebooks' token with 'write' access, that you can then easily reuse for all\nnotebooks. </center>"}},"77d1121e412246a9add84d16a406384d":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7a4e6315b7514e6289d53eb6bf1a4675":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"7cbf1b99dc9e4a518a8906f88ed348ba":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_77d1121e412246a9add84d16a406384d","max":4,"min":0,"orientation":"horizontal","style":"IPY_MODEL_c82822cd5613489bba88b1ef81ee6b08","value":4}},"7e976547c2aa4a44ac55a6e8907786a0":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_27ee43e81888408aa874a569a0de57e0","IPY_MODEL_9974be5e870e48d181b8b7eb0f37a2e8","IPY_MODEL_a3201ef5d93e4a758bb96d18c84abf44"],"layout":"IPY_MODEL_4ec242f9663b47ab89beaf10d505a22b"}},"804592cb94544a74b1cab4f8d14b6fb3":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_5786fb1e5ea041d5a1fd7338487f84f0","placeholder":"","style":"IPY_MODEL_06682a88e16a4f50bf231c6207310638","value":"<center> <img\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svg\nalt='Hugging Face'> <br> Copy a token from <a\nhref=\"https://huggingface.co/settings/tokens\" target=\"_blank\">your Hugging Face\ntokens page</a> and paste it below. <br> Immediately click login after copying\nyour token or it might be stored in plain text in this notebook file. </center>"}},"84e260ce602e43c8a42b0cc87fc4fefa":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"863712b14c73466aad32c14b8b6d10e0":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":"center","align_self":null,"border":null,"bottom":null,"display":"flex","flex":null,"flex_flow":"column","grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"50%"}},"8651d46c32a24fbba3d1cd35735585b1":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"874161a6ba6447cabac32f7b57e5c9b2":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"PasswordModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"PasswordModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"PasswordView","continuous_update":true,"description":"Token:","description_tooltip":null,"disabled":false,"layout":"IPY_MODEL_3b4eca3725634087a7840e0a92722c6e","placeholder":"","style":"IPY_MODEL_89af5249d84649f0aa2a74032e88da47","value":""}},"89af5249d84649f0aa2a74032e88da47":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"8ac28d9522f647ab84d3785510f85cfe":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_f27234bffb604ebcb08871a1f651a3bc","max":60,"min":0,"orientation":"horizontal","style":"IPY_MODEL_334b1cbc12f14eb48b1f906f13eb5d1a","value":60}},"8ff9f625841f4cc196401ec2f2620c1b":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_377d603b4366429294d21fae7ca86aa2","max":500,"min":0,"orientation":"horizontal","style":"IPY_MODEL_1add1bc41ca944ceaf3f4def4c68d34e","value":500}},"91515a2cf31345e4a8c536bbc30ce423":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ButtonModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ButtonModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ButtonView","button_style":"","description":"Login","disabled":false,"icon":"","layout":"IPY_MODEL_4d515b67918f46158926276af4879f0a","style":"IPY_MODEL_334b2b2e0a6f4269b6640cb3456a27e0","tooltip":""}},"9250295910064effbb9ac75724721d62":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"93a26a45055249a99e2b1537a90454d2":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"97b585d3fdd8480b8789cfc2b30be260":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"9974be5e870e48d181b8b7eb0f37a2e8":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_4957975f68a645d8b0b6386c2ec5d873","max":4,"min":0,"orientation":"horizontal","style":"IPY_MODEL_ec86077a2ff54d23985501daabffe813","value":4}},"9b18efeac7d84f8e9d182129d8787a8d":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a3201ef5d93e4a758bb96d18c84abf44":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_414449a196a44a6094ca6d7496ccd4de","placeholder":"","style":"IPY_MODEL_4d3eefdcf1db43bf9a723d49ebcdf5a6","value":" 4/4 [00:02&lt;00:00,  1.38ba/s]"}},"af3ecda5ac234ebca949eb149a053157":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_0e0e0d499d4d4e4d9308a5dec1055699","placeholder":"","style":"IPY_MODEL_8651d46c32a24fbba3d1cd35735585b1","value":" 500/500 [00:02&lt;00:00, 258.58it/s]"}},"af94b304158d451f8224240f55ccd2cb":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b256eae206544d1bab1282d326debec7":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"ba52e8b08abe41c69cf8904014a12bc6":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_9250295910064effbb9ac75724721d62","placeholder":"","style":"IPY_MODEL_7a4e6315b7514e6289d53eb6bf1a4675","value":" 60/60 [00:46&lt;00:00,  1.52ba/s]"}},"c04b3b3a3d7a403b90b919eb611ee925":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c82822cd5613489bba88b1ef81ee6b08":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"ce8848308dcf41bc9834cdfa68dce36f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_ed396b2aa3ad43a2a8b9a2d579b561c2","IPY_MODEL_8ff9f625841f4cc196401ec2f2620c1b","IPY_MODEL_af3ecda5ac234ebca949eb149a053157"],"layout":"IPY_MODEL_50a634cf333b411ebdfab2245fc192cb"}},"d16c036d8d9d4c3c892edb7551b31e44":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e3d7cb3140374328af657c177b889807":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"e4916d03793e44619a54548216c4381c":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"VBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"VBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"VBoxView","box_style":"","children":["IPY_MODEL_804592cb94544a74b1cab4f8d14b6fb3","IPY_MODEL_874161a6ba6447cabac32f7b57e5c9b2","IPY_MODEL_91515a2cf31345e4a8c536bbc30ce423","IPY_MODEL_63f1381b2ad14a05a56178c1d87be274"],"layout":"IPY_MODEL_863712b14c73466aad32c14b8b6d10e0"}},"ec86077a2ff54d23985501daabffe813":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"ed396b2aa3ad43a2a8b9a2d579b561c2":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f9a3d7b321c5407c81721bbe60169c38","placeholder":"","style":"IPY_MODEL_fb9b217f0b764b659bc55c554d8188f7","value":"Skipping the first batches: 100%"}},"f27234bffb604ebcb08871a1f651a3bc":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f9a3d7b321c5407c81721bbe60169c38":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"fb9b217f0b764b659bc55c554d8188f7":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat":4,"nbformat_minor":0}
