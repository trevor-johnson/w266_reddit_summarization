{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"2_BART_training_fullDF.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyNp+RuIwkxxm4RMJS5qHv2a"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","gpuClass":"standard","widgets":{"application/vnd.jupyter.widget-state+json":{"0debb6780dd947cda703aeea733a7f87":{"model_module":"@jupyter-widgets/controls","model_name":"VBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"VBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"VBoxView","box_style":"","children":["IPY_MODEL_94bd3038d47349fd93395940c61ed832","IPY_MODEL_7761ba6dfed44f1eb5ab0ae6de83342a","IPY_MODEL_cc3da844e5f44b83983635e0c783368c","IPY_MODEL_56a057ff3c344feb978a3badb13a2cb9"],"layout":"IPY_MODEL_267228cb3ff2493ab5835169f7c2e6d7"}},"94bd3038d47349fd93395940c61ed832":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_8d6168b479de4532bf8139d57c850e4f","placeholder":"​","style":"IPY_MODEL_1d6dfd9e98bc44799ca893a22a60efd6","value":"<center> <img\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svg\nalt='Hugging Face'> <br> Copy a token from <a\nhref=\"https://huggingface.co/settings/tokens\" target=\"_blank\">your Hugging Face\ntokens page</a> and paste it below. <br> Immediately click login after copying\nyour token or it might be stored in plain text in this notebook file. </center>"}},"7761ba6dfed44f1eb5ab0ae6de83342a":{"model_module":"@jupyter-widgets/controls","model_name":"PasswordModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"PasswordModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"PasswordView","continuous_update":true,"description":"Token:","description_tooltip":null,"disabled":false,"layout":"IPY_MODEL_7acf92b1c28940ee984d3f5a598dbf24","placeholder":"​","style":"IPY_MODEL_550dc54a673f49f0a466ce348acd8e0c","value":""}},"cc3da844e5f44b83983635e0c783368c":{"model_module":"@jupyter-widgets/controls","model_name":"ButtonModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ButtonModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ButtonView","button_style":"","description":"Login","disabled":false,"icon":"","layout":"IPY_MODEL_89767a9b05d64ab89696a27d3e392365","style":"IPY_MODEL_2d258ac4d7494819ac74181844086847","tooltip":""}},"56a057ff3c344feb978a3badb13a2cb9":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_cb49820e9a584ef1bba06edbb16c4080","placeholder":"​","style":"IPY_MODEL_1e15ffc60ab04886892bc7a2d5d0e70e","value":"\n<b>Pro Tip:</b> If you don't already have one, you can create a dedicated\n'notebooks' token with 'write' access, that you can then easily reuse for all\nnotebooks. </center>"}},"267228cb3ff2493ab5835169f7c2e6d7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":"center","align_self":null,"border":null,"bottom":null,"display":"flex","flex":null,"flex_flow":"column","grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"50%"}},"8d6168b479de4532bf8139d57c850e4f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1d6dfd9e98bc44799ca893a22a60efd6":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"7acf92b1c28940ee984d3f5a598dbf24":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"550dc54a673f49f0a466ce348acd8e0c":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"89767a9b05d64ab89696a27d3e392365":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2d258ac4d7494819ac74181844086847":{"model_module":"@jupyter-widgets/controls","model_name":"ButtonStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ButtonStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","button_color":null,"font_weight":""}},"cb49820e9a584ef1bba06edbb16c4080":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1e15ffc60ab04886892bc7a2d5d0e70e":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"c117b1c63ded4d9287d104b0e63435ce":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_c52ed1731c7e43d79f75c44b85ff0007","IPY_MODEL_8ea6d2934ea943a0923274e156fc626b","IPY_MODEL_e93be9b2629843e78f3aeed4b16224bd"],"layout":"IPY_MODEL_e9e64db3036b4abdae49d91deef33821"}},"c52ed1731c7e43d79f75c44b85ff0007":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_2ee7016823e347f9acdd58cde4ecab49","placeholder":"​","style":"IPY_MODEL_4b5017c0a2074086b067e7eb1224fc1e","value":"100%"}},"8ea6d2934ea943a0923274e156fc626b":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_406f6a83e8994d5684ae9d7ebb3cff36","max":60,"min":0,"orientation":"horizontal","style":"IPY_MODEL_7cafe1c1cdee452889537ac64e36e888","value":60}},"e93be9b2629843e78f3aeed4b16224bd":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_08cad834cf16422088dd50ca5565f492","placeholder":"​","style":"IPY_MODEL_a4d0183a5fff413a8d30f6964448f2f9","value":" 60/60 [00:41&lt;00:00,  1.13ba/s]"}},"e9e64db3036b4abdae49d91deef33821":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2ee7016823e347f9acdd58cde4ecab49":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4b5017c0a2074086b067e7eb1224fc1e":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"406f6a83e8994d5684ae9d7ebb3cff36":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7cafe1c1cdee452889537ac64e36e888":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"08cad834cf16422088dd50ca5565f492":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a4d0183a5fff413a8d30f6964448f2f9":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"0d3c988b37f74c74b880c57bac1d2b52":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_6e049d94786d46c29c93bd89f39a009c","IPY_MODEL_04312b1f328e468e9c2b12d0a899d0d2","IPY_MODEL_9d1a1e64d78b467ca5d6bc6bd1865377"],"layout":"IPY_MODEL_bd7d3c8ee85a42a8826e7f5252586eb9"}},"6e049d94786d46c29c93bd89f39a009c":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_5cb335eea0ff4490b8efc66187870c41","placeholder":"​","style":"IPY_MODEL_b65f0ccf8d1c4dafae7af7467fd8dfcf","value":"100%"}},"04312b1f328e468e9c2b12d0a899d0d2":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_6173daac7efb4a218cf2c6a4167b6236","max":4,"min":0,"orientation":"horizontal","style":"IPY_MODEL_f698ec90dcef4f9eb0d88b817e58b8d3","value":4}},"9d1a1e64d78b467ca5d6bc6bd1865377":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f34ccba7ddcd45f1a605f58da9ede267","placeholder":"​","style":"IPY_MODEL_051fcb1de9204520a99340b12923fcfa","value":" 4/4 [00:04&lt;00:00,  1.31s/ba]"}},"bd7d3c8ee85a42a8826e7f5252586eb9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5cb335eea0ff4490b8efc66187870c41":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b65f0ccf8d1c4dafae7af7467fd8dfcf":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"6173daac7efb4a218cf2c6a4167b6236":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f698ec90dcef4f9eb0d88b817e58b8d3":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"f34ccba7ddcd45f1a605f58da9ede267":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"051fcb1de9204520a99340b12923fcfa":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"196731c016494443aeac593c64cc00dd":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_8aa3b552297b4b949057f83947a0b5e8","IPY_MODEL_c3ba19011c0a4461928b7904db87e8fd","IPY_MODEL_ab221235cb8b4d22bb069db682d12f76"],"layout":"IPY_MODEL_ccd58aa537ff49b9bf3dc341237f9451"}},"8aa3b552297b4b949057f83947a0b5e8":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d516c18e9dc84a61ae5b32b4db519dd8","placeholder":"​","style":"IPY_MODEL_daf638ca0db74a7493d833fa6e8fd082","value":"100%"}},"c3ba19011c0a4461928b7904db87e8fd":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_16891c39e24142dfaab9c47e933ea888","max":4,"min":0,"orientation":"horizontal","style":"IPY_MODEL_8e9b08b022874c7f99bdcb0671af6129","value":4}},"ab221235cb8b4d22bb069db682d12f76":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_89def2e9fe95402b8c1e25f4acb54ebc","placeholder":"​","style":"IPY_MODEL_bf2bb100944042f1870ada08f35eedc0","value":" 4/4 [00:04&lt;00:00,  1.10s/ba]"}},"ccd58aa537ff49b9bf3dc341237f9451":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d516c18e9dc84a61ae5b32b4db519dd8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"daf638ca0db74a7493d833fa6e8fd082":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"16891c39e24142dfaab9c47e933ea888":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8e9b08b022874c7f99bdcb0671af6129":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"89def2e9fe95402b8c1e25f4acb54ebc":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"bf2bb100944042f1870ada08f35eedc0":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"cells":[{"cell_type":"markdown","source":["# About\n","\n","Fine tune single BART by combining all the subgroups together"],"metadata":{"id":"LmCQLmSHDrXg"}},{"cell_type":"markdown","source":["# Setups"],"metadata":{"id":"SaoqggbYD3F_"}},{"cell_type":"code","execution_count":1,"metadata":{"id":"X2__lmSSDmsG","executionInfo":{"status":"ok","timestamp":1658548925656,"user_tz":420,"elapsed":30030,"user":{"displayName":"Trevor Johnson","userId":"10875792761423186902"}}},"outputs":[],"source":["from IPython.display import clear_output\n","\n","!pip install datasets transformers rouge_score rouge-score nltk\n","# rouge-score is the google version\n","!pip install pyarrow\n","!pip install -q sentencepiece\n","\n","clear_output()"]},{"cell_type":"code","source":["import os\n","import re\n","import time\n","from tqdm.notebook import trange, tqdm\n","import pandas as pd\n","import numpy as np\n","from pprint import pprint\n","import matplotlib.pyplot as plt\n","\n","# nlp stuff\n","import nltk\n","nltk.download('punkt')\n","\n","# tf stuff\n","import tensorflow_datasets as tfds \n","import tensorflow as tf\n","from transformers import PegasusTokenizer, TFPegasusForConditionalGeneration # pegasus\n","from transformers import BartTokenizer, TFBartForConditionalGeneration # bart\n","\n","# pytorch dataset types\n","import datasets\n","from datasets.dataset_dict import DatasetDict\n","from datasets import Dataset, load_metric, load_dataset\n","\n","# pytorch bart stuff\n","import torch\n","from transformers import AutoModelForSeq2SeqLM, DataCollatorForSeq2Seq, Seq2SeqTrainingArguments, Seq2SeqTrainer\n","from transformers import AutoTokenizer\n","\n","clear_output()"],"metadata":{"id":"J0pxN5RGD5Zs","executionInfo":{"status":"ok","timestamp":1658548990435,"user_tz":420,"elapsed":11713,"user":{"displayName":"Trevor Johnson","userId":"10875792761423186902"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["# sign into huggingface: https://huggingface.co/settings/tokens\n","from huggingface_hub import notebook_login\n","notebook_login()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":327,"referenced_widgets":["0debb6780dd947cda703aeea733a7f87","94bd3038d47349fd93395940c61ed832","7761ba6dfed44f1eb5ab0ae6de83342a","cc3da844e5f44b83983635e0c783368c","56a057ff3c344feb978a3badb13a2cb9","267228cb3ff2493ab5835169f7c2e6d7","8d6168b479de4532bf8139d57c850e4f","1d6dfd9e98bc44799ca893a22a60efd6","7acf92b1c28940ee984d3f5a598dbf24","550dc54a673f49f0a466ce348acd8e0c","89767a9b05d64ab89696a27d3e392365","2d258ac4d7494819ac74181844086847","cb49820e9a584ef1bba06edbb16c4080","1e15ffc60ab04886892bc7a2d5d0e70e"]},"id":"asC0CF-lGDKW","executionInfo":{"status":"ok","timestamp":1658548990436,"user_tz":420,"elapsed":16,"user":{"displayName":"Trevor Johnson","userId":"10875792761423186902"}},"outputId":"38ac2393-85a6-44ac-acce-52c1c09647c0"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Login successful\n","Your token has been saved to /root/.huggingface/token\n","\u001b[1m\u001b[31mAuthenticated through git-credential store but this isn't the helper defined on your machine.\n","You might have to re-authenticate when pushing to the Hugging Face Hub. Run the following command in your terminal in case you want to set this credential helper as the default\n","\n","git config --global credential.helper store\u001b[0m\n"]}]},{"cell_type":"code","source":["  #!apt install git-lfs"],"metadata":{"id":"Ms8ZCQjjGeV1","executionInfo":{"status":"ok","timestamp":1658548997991,"user_tz":420,"elapsed":225,"user":{"displayName":"Trevor Johnson","userId":"10875792761423186902"}}},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":["# Load data"],"metadata":{"id":"tyI0wrDdD_VW"}},{"cell_type":"code","source":["# specify your path to the repo here:\n","repo_path = '/content/gdrive/MyDrive/w266/w266_reddit_summarization'"],"metadata":{"id":"2Kqkg1SZEKqN","executionInfo":{"status":"ok","timestamp":1658548998981,"user_tz":420,"elapsed":3,"user":{"displayName":"Trevor Johnson","userId":"10875792761423186902"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["%%time\n","from google.colab import drive\n","drive.mount('/content/gdrive')\n","data_path = os.path.join(repo_path, 'data/reddit_parquet/train_test_split_v2')\n","os.chdir(data_path)\n","files = [i for i in os.listdir(data_path) if re.search(\"reddit\", i)]\n","\n","train = pd.read_parquet('reddit_train.parquet')\n","test = pd.read_parquet('reddit_test.parquet')\n","valid = pd.read_parquet('reddit_validation.parquet')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"r6MWx5d_D_7b","executionInfo":{"status":"ok","timestamp":1658549025464,"user_tz":420,"elapsed":24808,"user":{"displayName":"Trevor Johnson","userId":"10875792761423186902"}},"outputId":"b6e33d63-4d90-43d0-80c4-b005c8724f83"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/gdrive\n","CPU times: user 1.25 s, sys: 400 ms, total: 1.65 s\n","Wall time: 24.7 s\n"]}]},{"cell_type":"code","source":["print(\"train\")\n","print(train['subreddit_group'].value_counts())\n","\n","print(\"\\n\\ntest:\")\n","print(test['subreddit_group'].value_counts())\n","\n","print(\"\\n\\nvalid:\")\n","valid['subreddit_group'].value_counts()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZPGpW4IKE-1Y","executionInfo":{"status":"ok","timestamp":1658549030352,"user_tz":420,"elapsed":243,"user":{"displayName":"Trevor Johnson","userId":"10875792761423186902"}},"outputId":"a967abb4-8a03-4ac5-db63-1158c77845d8"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["train\n","advice/story              15000\n","gaming                    15000\n","media/lifestyle/sports    15000\n","other                     15000\n","Name: subreddit_group, dtype: int64\n","\n","\n","test:\n","advice/story              1000\n","gaming                    1000\n","media/lifestyle/sports    1000\n","other                     1000\n","Name: subreddit_group, dtype: int64\n","\n","\n","valid:\n"]},{"output_type":"execute_result","data":{"text/plain":["advice/story              1000\n","gaming                    1000\n","media/lifestyle/sports    1000\n","other                     1000\n","Name: subreddit_group, dtype: int64"]},"metadata":{},"execution_count":7}]},{"cell_type":"markdown","source":["# Modeling"],"metadata":{"id":"ZV5-6hQiGi-B"}},{"cell_type":"code","source":["# bunch of diff checkpoints to consider\n","\n","# bart checkpoints\n","# model_checkpoint = 'facebook/bart-base' # keep returning the first sentence for me, extractive.\n","# model_checkpoint = 'facebook/bart-large-mnli' # same as above, only returns first sentences. extractive.\n","# model_checkpoint = 'sshleifer/distilbart-cnn-12-6' # works a bit better, but seems to produce extractive summaries still. \n","# model_checkpoint = 'sshleifer/distilbart-xsum-6-6' # was recommended. produces abstractive summaries p well. so far works the best of the above. \n","model_checkpoint = 'sshleifer/distilbart-xsum-6-6' # trained on both xsum and cnn/dm\n","\n","# pegasus checkpoints:\n","# model_checkpoint = \"google/pegasus-xsum\" # works really well\n","# model_checkpoint = 'google/pegasus-reddit_tifu' # also works really well"],"metadata":{"id":"fgwErjJDZ70J","executionInfo":{"status":"ok","timestamp":1658549037914,"user_tz":420,"elapsed":222,"user":{"displayName":"Trevor Johnson","userId":"10875792761423186902"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","source":["# load model, tokenizer, and rouge metric\n","tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n","model = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint)\n","metric = load_metric(\"rouge\")\n","\n","clear_output()"],"metadata":{"id":"mdeOrmfCFM02","executionInfo":{"status":"ok","timestamp":1658549059197,"user_tz":420,"elapsed":20350,"user":{"displayName":"Trevor Johnson","userId":"10875792761423186902"}}},"execution_count":9,"outputs":[]},{"cell_type":"code","source":["# convert data to torch Dataset.\n","raw_datasets = DatasetDict({\n","    'train': Dataset.from_dict({\n","        'content': train['content'],\n","        'summary': train['summary'],\n","        'subreddit': train['subreddit']\n","    }), \n","    'test': Dataset.from_dict({\n","        'content': test['content'],\n","        'summary': test['summary'],\n","        'subreddit': test['subreddit']\n","    }), \n","    'valid': Dataset.from_dict({\n","        'content': valid['content'],\n","        'summary': valid['summary'],\n","        'subreddit': valid['subreddit']\n","    })\n","})\n","\n","raw_datasets"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JaH79IneFNXJ","executionInfo":{"status":"ok","timestamp":1658549163981,"user_tz":420,"elapsed":529,"user":{"displayName":"Trevor Johnson","userId":"10875792761423186902"}},"outputId":"f8b5415a-9376-4e54-f6ae-6147b3f952a5"},"execution_count":11,"outputs":[{"output_type":"execute_result","data":{"text/plain":["DatasetDict({\n","    train: Dataset({\n","        features: ['content', 'summary', 'subreddit'],\n","        num_rows: 60000\n","    })\n","    test: Dataset({\n","        features: ['content', 'summary', 'subreddit'],\n","        num_rows: 4000\n","    })\n","    valid: Dataset({\n","        features: ['content', 'summary', 'subreddit'],\n","        num_rows: 4000\n","    })\n","})"]},"metadata":{},"execution_count":11}]},{"cell_type":"code","source":["# tokenize everything\n","max_input_length = 1024\n","max_target_length = 128\n","\n","def preprocess_function(examples):\n","    inputs = [doc for doc in examples[\"content\"]]\n","    model_inputs = tokenizer(inputs, max_length=max_input_length, truncation=True)\n","\n","    # Setup the tokenizer for targets\n","    with tokenizer.as_target_tokenizer():\n","        labels = tokenizer(examples[\"summary\"], max_length=max_target_length, truncation=True)\n","\n","    model_inputs[\"labels\"] = labels[\"input_ids\"]\n","    return model_inputs\n","\n","tokenized_datasets = raw_datasets.map(preprocess_function, batched=True)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":151,"referenced_widgets":["c117b1c63ded4d9287d104b0e63435ce","c52ed1731c7e43d79f75c44b85ff0007","8ea6d2934ea943a0923274e156fc626b","e93be9b2629843e78f3aeed4b16224bd","e9e64db3036b4abdae49d91deef33821","2ee7016823e347f9acdd58cde4ecab49","4b5017c0a2074086b067e7eb1224fc1e","406f6a83e8994d5684ae9d7ebb3cff36","7cafe1c1cdee452889537ac64e36e888","08cad834cf16422088dd50ca5565f492","a4d0183a5fff413a8d30f6964448f2f9","0d3c988b37f74c74b880c57bac1d2b52","6e049d94786d46c29c93bd89f39a009c","04312b1f328e468e9c2b12d0a899d0d2","9d1a1e64d78b467ca5d6bc6bd1865377","bd7d3c8ee85a42a8826e7f5252586eb9","5cb335eea0ff4490b8efc66187870c41","b65f0ccf8d1c4dafae7af7467fd8dfcf","6173daac7efb4a218cf2c6a4167b6236","f698ec90dcef4f9eb0d88b817e58b8d3","f34ccba7ddcd45f1a605f58da9ede267","051fcb1de9204520a99340b12923fcfa","196731c016494443aeac593c64cc00dd","8aa3b552297b4b949057f83947a0b5e8","c3ba19011c0a4461928b7904db87e8fd","ab221235cb8b4d22bb069db682d12f76","ccd58aa537ff49b9bf3dc341237f9451","d516c18e9dc84a61ae5b32b4db519dd8","daf638ca0db74a7493d833fa6e8fd082","16891c39e24142dfaab9c47e933ea888","8e9b08b022874c7f99bdcb0671af6129","89def2e9fe95402b8c1e25f4acb54ebc","bf2bb100944042f1870ada08f35eedc0"]},"id":"3vyhmnSZIIjS","executionInfo":{"status":"ok","timestamp":1658549222706,"user_tz":420,"elapsed":51325,"user":{"displayName":"Trevor Johnson","userId":"10875792761423186902"}},"outputId":"c81ad4ed-0746-4c23-f40f-3698effd984c"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stderr","text":["Parameter 'function'=<function preprocess_function at 0x7f1eb17638c0> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n"]},{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/60 [00:00<?, ?ba/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c117b1c63ded4d9287d104b0e63435ce"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/4 [00:00<?, ?ba/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0d3c988b37f74c74b880c57bac1d2b52"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/4 [00:00<?, ?ba/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"196731c016494443aeac593c64cc00dd"}},"metadata":{}}]},{"cell_type":"code","source":["tokenized_datasets"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xWqYi08uwanD","executionInfo":{"status":"ok","timestamp":1658549222707,"user_tz":420,"elapsed":16,"user":{"displayName":"Trevor Johnson","userId":"10875792761423186902"}},"outputId":"76dfa2c3-b798-4f30-eab8-2c3a9918bef0"},"execution_count":13,"outputs":[{"output_type":"execute_result","data":{"text/plain":["DatasetDict({\n","    train: Dataset({\n","        features: ['content', 'summary', 'subreddit', 'input_ids', 'attention_mask', 'labels'],\n","        num_rows: 60000\n","    })\n","    test: Dataset({\n","        features: ['content', 'summary', 'subreddit', 'input_ids', 'attention_mask', 'labels'],\n","        num_rows: 4000\n","    })\n","    valid: Dataset({\n","        features: ['content', 'summary', 'subreddit', 'input_ids', 'attention_mask', 'labels'],\n","        num_rows: 4000\n","    })\n","})"]},"metadata":{},"execution_count":13}]},{"cell_type":"code","source":["data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)\n","\n","def compute_metrics(eval_pred):\n","    predictions, labels = eval_pred\n","    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n","    # Replace -100 in the labels as we can't decode them.\n","    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n","    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n","    \n","    # Rouge expects a newline after each sentence\n","    decoded_preds = [\"\\n\".join(nltk.sent_tokenize(pred.strip())) for pred in decoded_preds]\n","    decoded_labels = [\"\\n\".join(nltk.sent_tokenize(label.strip())) for label in decoded_labels]\n","    \n","    result = metric.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)\n","    # Extract a few results\n","    result = {key: value.mid.fmeasure * 100 for key, value in result.items()}\n","    \n","    # Add mean generated length\n","    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in predictions]\n","    result[\"gen_len\"] = np.mean(prediction_lens)\n","    \n","    return {k: round(v, 4) for k, v in result.items()}"],"metadata":{"id":"PLMagSErJLmh","executionInfo":{"status":"ok","timestamp":1658549222707,"user_tz":420,"elapsed":14,"user":{"displayName":"Trevor Johnson","userId":"10875792761423186902"}}},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":["# Train full dataset\n","- epochs=3, obs=60k, batch_size=8. Took __ hrs"],"metadata":{"id":"khYnVUR0_rBZ"}},{"cell_type":"code","source":["%%time\n","# note, batch size of 8 seems to almost max out the gpu\n","# so probs dont go any higher than this. \n","args = Seq2SeqTrainingArguments(\n","    f\"BART_reddit\", \n","    evaluation_strategy = \"epoch\",\n","    learning_rate=2e-5,\n","    per_device_train_batch_size=8, # 16\n","    per_device_eval_batch_size=8, #16\n","    weight_decay=0.01,\n","    save_total_limit=1, #3,\n","    num_train_epochs=3,\n","    predict_with_generate=True,\n","    fp16=True,\n","    push_to_hub=True,\n",")\n","\n","trainer = Seq2SeqTrainer(\n","    model,\n","    args,\n","    train_dataset=tokenized_datasets[\"train\"],\n","    eval_dataset=tokenized_datasets[\"valid\"],\n","    data_collator=data_collator,\n","    tokenizer=tokenizer,\n","    compute_metrics=compute_metrics\n",")\n","\n","# this should save the model to disk. changing wd so that it saves here:\n","fit_path = '/content/gdrive/MyDrive/w266/w266_reddit_summarization/data/model_outputs/bart_fitted_models/round2'\n","os.chdir(fit_path)\n","trainer.train()"],"metadata":{"id":"10C1FSESKUt7","colab":{"base_uri":"https://localhost:8080/","height":649},"outputId":"8a4b6529-5ab3-4647-c4c2-bb4b8cd0e922"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["Cloning https://huggingface.co/trevorj/BART_reddit into local empty directory.\n","Using cuda_amp half precision backend\n","The following columns in the training set don't have a corresponding argument in `BartForConditionalGeneration.forward` and have been ignored: summary, content, subreddit. If summary, content, subreddit are not expected by `BartForConditionalGeneration.forward`,  you can safely ignore this message.\n","/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  FutureWarning,\n","***** Running training *****\n","  Num examples = 60000\n","  Num Epochs = 3\n","  Instantaneous batch size per device = 8\n","  Total train batch size (w. parallel, distributed & accumulation) = 8\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 22500\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='1900' max='22500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [ 1900/22500 23:17 < 4:12:46, 1.36 it/s, Epoch 0.25/3]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Saving model checkpoint to BART_reddit/checkpoint-500\n","Configuration saved in BART_reddit/checkpoint-500/config.json\n","Model weights saved in BART_reddit/checkpoint-500/pytorch_model.bin\n","tokenizer config file saved in BART_reddit/checkpoint-500/tokenizer_config.json\n","Special tokens file saved in BART_reddit/checkpoint-500/special_tokens_map.json\n","tokenizer config file saved in BART_reddit/tokenizer_config.json\n","Special tokens file saved in BART_reddit/special_tokens_map.json\n","Saving model checkpoint to BART_reddit/checkpoint-1000\n","Configuration saved in BART_reddit/checkpoint-1000/config.json\n","Model weights saved in BART_reddit/checkpoint-1000/pytorch_model.bin\n","tokenizer config file saved in BART_reddit/checkpoint-1000/tokenizer_config.json\n","Special tokens file saved in BART_reddit/checkpoint-1000/special_tokens_map.json\n","Deleting older checkpoint [BART_reddit/checkpoint-500] due to args.save_total_limit\n","Saving model checkpoint to BART_reddit/checkpoint-1500\n","Configuration saved in BART_reddit/checkpoint-1500/config.json\n","Model weights saved in BART_reddit/checkpoint-1500/pytorch_model.bin\n","tokenizer config file saved in BART_reddit/checkpoint-1500/tokenizer_config.json\n","Special tokens file saved in BART_reddit/checkpoint-1500/special_tokens_map.json\n","Deleting older checkpoint [BART_reddit/checkpoint-1000] due to args.save_total_limit\n"]}]},{"cell_type":"code","source":["%%time\n","# also save on huggingface\n","trainer.push_to_hub()\n","# fit_path = '/content/gdrive/MyDrive/w266/w266_reddit_summarization/data/model_outputs/bart_fitted_models/'\n","# os.chdir(fit_path)\n","# trainer.push_to_hub('trevorj/BART-reddit-advice_story')\n","\n","\n","# then load model back in\n","# model = AutoModelForSeq2SeqLM.from_pretrained(\"trevorj/model_name\")\n","\n","# push to hub seems to fail with this message:\n","# Dropping the following result as it does not have all the necessary fields:\n","# {'task': {'name': 'Sequence-to-sequence Language Modeling', 'type': 'text2text-generation'}, 'metrics': [{'name': 'Rouge1', 'type': 'rouge', 'value': 20.9206}]}"],"metadata":{"id":"gw64VgdGBNh7"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Compare Model predictions"],"metadata":{"id":"1BvAa_bRA43-"}},{"cell_type":"code","source":["# true:\n","print(\"Content:\")\n","pprint(tokenized_datasets['test_advice_story']['content'][0])\n","print(\"\\n\\nTrue Summary\")\n","pprint(tokenized_datasets['test_advice_story']['summary'][0])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HKl8zq8C-VUq","executionInfo":{"status":"ok","timestamp":1658380099957,"user_tz":420,"elapsed":229,"user":{"displayName":"Trevor Johnson","userId":"10875792761423186902"}},"outputId":"fb02e29d-1bcd-4a3e-ad86-73e85567144f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Content:\n","('Living in the Sierra Nevadas, it gets very cold at night. \\n'\n"," ' Took a shower after a cold night at work. Shower with nice hot water. Hot '\n"," 'but not steaming hot. Felt so good and warm. \\n'\n"," ' Drying myself, starting to get cold. \\n'\n"," ' Got out of the shower, so damn cold. \\n'\n"," ' Drying hair(on head) with the blow dryer and it felt so nice and warm.\\n'\n"," 'Balls were cold, penis was cold. All shrivled up, smaller than usual. \\n'\n"," ' Blow dry my private area just as usual during a cold winter day, but since '\n"," 'it was so cold I moved in a little closer. It felt so nice and warm. \\n'\n"," 'Thought \"oh, fuck it, I\\'ll move in a little closer since it was so damn '\n"," 'cold today.\" Moved in too close, tip of penis touches the burning hot iron '\n"," 'grill. Pull dryer away in shock. \\n'\n"," \" It hurts. It burns as I type this. Erections make it burn more. It's red \"\n"," \"where it got burned. Hoping it doesn't turn into a blister\")\n","\n","\n","True Summary\n","('Too damn cold, showered, dry my hair, dry my privates(male parts) moved in '\n"," 'too close, burned the tip of my penis with the iron grill on the blow dryer. '\n"," 'My penis hurts. It burns. \\n'\n"," \" EDIT: I didn't think i would have permanent markings but now I think I \"\n"," 'will. I have 2 very straight, black burn marks on my foreskin where the blow '\n"," 'dryer burned. I will upload a picture soon')\n"]}]},{"cell_type":"code","source":["# results from my trained model above\n","# note, I had to add .cuda() to the end of the input tensor to specify to use gpu i guess. \n","# But dont do this for the original model. just your fine tuned one. \n","output = model.generate(torch.tensor([tokenized_datasets['test_advice_story']['input_ids'][0]]).cuda(), num_beams=2, max_length=60, min_length=2, no_repeat_ngram_size=3)\n","output_decoded = tokenizer.decode(output.squeeze(), skip_special_tokens=True)\n","pprint(output_decoded)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"eWNnU8Yy8ibn","executionInfo":{"status":"ok","timestamp":1658380121697,"user_tz":420,"elapsed":591,"user":{"displayName":"Trevor Johnson","userId":"10875792761423186902"}},"outputId":"352d6ffb-070d-43b7-c7f0-522f11ed6e6e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["'Got cold, penis burns.'\n"]}]},{"cell_type":"code","source":["# read model from disk (same prediction. Good) \n","# note, using the latest checkpoint produced the same thing, so the weights are likely almost the same. \n","# final files in the base folder will get overwritten at very end with final weights. \n","# so we can just delete the older checkpoint folders\n","\n","checkpoint_disk = \"/content/gdrive/MyDrive/w266/w266_reddit_summarization/data/model_outputs/bart_fitted_models/round2/BART_reddit_advice_story\"\n","# model_disk = AutoModelForSeq2SeqLM.from_pretrained(checkpoint_disk)\n","model_disk = AutoModelForSeq2SeqLM.from_pretrained(checkpoint_disk, local_files_only=True) # optional to force looking locally\n","\n","clear_output()\n","output = model_disk.generate(torch.tensor([tokenized_datasets['test_advice_story']['input_ids'][0]]), num_beams=2, max_length=60, min_length=2, no_repeat_ngram_size=3)\n","output_decoded = tokenizer.decode(output.squeeze(), skip_special_tokens=True)\n","pprint(output_decoded)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bRvkB7FXBKrZ","executionInfo":{"status":"ok","timestamp":1658380522515,"user_tz":420,"elapsed":8387,"user":{"displayName":"Trevor Johnson","userId":"10875792761423186902"}},"outputId":"8943df44-838b-41a5-ffb7-982fe0d0bf3a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["'Got cold, penis burns.'\n"]}]},{"cell_type":"code","source":["# compare to original unfit model. (prediction is different, good). \n","model_original = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint)\n","clear_output()\n","output = model_original.generate(torch.tensor([tokenized_datasets['test_advice_story']['input_ids'][0]]), num_beams=2, max_length=60, min_length=2, no_repeat_ngram_size=3)\n","output_decoded = tokenizer.decode(output.squeeze(), skip_special_tokens=True)\n","pprint(output_decoded)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WWnZEMxs94co","executionInfo":{"status":"ok","timestamp":1658380534874,"user_tz":420,"elapsed":8100,"user":{"displayName":"Trevor Johnson","userId":"10875792761423186902"}},"outputId":"c97f5c67-b33d-48e4-d5e8-f243a23a6cf1"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["(' In a series of letters from African journalists, filmmaker and columnist '\n"," 'Adelisa Fonseca looks back at her recent cold weather.')\n"]}]},{"cell_type":"code","source":["# Load from huggingface (haven't configured yet)\n","# new_checkpoint = 'trevorj/BART-reddit-advice_story'\n","# model_advice_story = AutoModelForSeq2SeqLM.from_pretrained(new_checkpoint)\n","# tokenizer_advice_story = AutoTokenizer.from_pretrained(new_checkpoint)"],"metadata":{"id":"adkFqhM_57Q_"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Train - media_lifestyle_sports\n","- 1:12 hrs"],"metadata":{"id":"7Qrav_TyIexv"}},{"cell_type":"code","source":["%%time\n","\n","args = Seq2SeqTrainingArguments(\n","    f\"BART_reddit_media_lifestyle_sports\",\n","    evaluation_strategy = \"epoch\",\n","    learning_rate=2e-5,\n","    per_device_train_batch_size=8, # 16\n","    per_device_eval_batch_size=8, #16\n","    weight_decay=0.01,\n","    save_total_limit=1, #3,\n","    num_train_epochs=3, # 1\n","    predict_with_generate=True,\n","    fp16=True,\n","    push_to_hub=True,\n",")\n","\n","trainer_media = Seq2SeqTrainer(\n","    model,\n","    args,\n","    train_dataset=tokenized_datasets[\"train_media_lifestyle_sports\"],\n","    eval_dataset=tokenized_datasets[\"valid_media_lifestyle_sports\"],\n","    data_collator=data_collator,\n","    tokenizer=tokenizer,\n","    compute_metrics=compute_metrics\n",")\n","\n","# this should save the model to disk. changing wd so that it saves here:\n","fit_path = '/content/gdrive/MyDrive/w266/w266_reddit_summarization/data/model_outputs/bart_fitted_models/round2'\n","os.chdir(fit_path)\n","trainer_media.train()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"QTkIL8lxOoan","executionInfo":{"status":"ok","timestamp":1658384948882,"user_tz":420,"elapsed":1644240,"user":{"displayName":"Trevor Johnson","userId":"10875792761423186902"}},"outputId":"bdae6ccd-a47b-4bdb-cd1b-f5e9c2a69555"},"execution_count":null,"outputs":[{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["PyTorch: setting up devices\n","The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n","Cloning https://huggingface.co/trevorj/BART_reddit_media_lifestyle_sports into local empty directory.\n","WARNING:huggingface_hub.repository:Cloning https://huggingface.co/trevorj/BART_reddit_media_lifestyle_sports into local empty directory.\n","Using cuda_amp half precision backend\n","The following columns in the training set don't have a corresponding argument in `BartForConditionalGeneration.forward` and have been ignored: subreddit_group, subreddit, content, summary. If subreddit_group, subreddit, content, summary are not expected by `BartForConditionalGeneration.forward`,  you can safely ignore this message.\n","/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  FutureWarning,\n","***** Running training *****\n","  Num examples = 15000\n","  Num Epochs = 3\n","  Instantaneous batch size per device = 8\n","  Total train batch size (w. parallel, distributed & accumulation) = 8\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 5625\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='3668' max='5625' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [3668/5625 45:15 < 24:09, 1.35 it/s, Epoch 1.96/3]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Rouge1</th>\n","      <th>Rouge2</th>\n","      <th>Rougel</th>\n","      <th>Rougelsum</th>\n","      <th>Gen Len</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>3.654200</td>\n","      <td>3.548437</td>\n","      <td>15.970400</td>\n","      <td>4.288100</td>\n","      <td>13.462100</td>\n","      <td>13.985700</td>\n","      <td>17.876000</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["Saving model checkpoint to BART_reddit_media_lifestyle_sports/checkpoint-500\n","Configuration saved in BART_reddit_media_lifestyle_sports/checkpoint-500/config.json\n","Model weights saved in BART_reddit_media_lifestyle_sports/checkpoint-500/pytorch_model.bin\n","tokenizer config file saved in BART_reddit_media_lifestyle_sports/checkpoint-500/tokenizer_config.json\n","Special tokens file saved in BART_reddit_media_lifestyle_sports/checkpoint-500/special_tokens_map.json\n","tokenizer config file saved in BART_reddit_media_lifestyle_sports/tokenizer_config.json\n","Special tokens file saved in BART_reddit_media_lifestyle_sports/special_tokens_map.json\n","Saving model checkpoint to BART_reddit_media_lifestyle_sports/checkpoint-1000\n","Configuration saved in BART_reddit_media_lifestyle_sports/checkpoint-1000/config.json\n","Model weights saved in BART_reddit_media_lifestyle_sports/checkpoint-1000/pytorch_model.bin\n","tokenizer config file saved in BART_reddit_media_lifestyle_sports/checkpoint-1000/tokenizer_config.json\n","Special tokens file saved in BART_reddit_media_lifestyle_sports/checkpoint-1000/special_tokens_map.json\n","Deleting older checkpoint [BART_reddit_media_lifestyle_sports/checkpoint-500] due to args.save_total_limit\n","Saving model checkpoint to BART_reddit_media_lifestyle_sports/checkpoint-1500\n","Configuration saved in BART_reddit_media_lifestyle_sports/checkpoint-1500/config.json\n","Model weights saved in BART_reddit_media_lifestyle_sports/checkpoint-1500/pytorch_model.bin\n","tokenizer config file saved in BART_reddit_media_lifestyle_sports/checkpoint-1500/tokenizer_config.json\n","Special tokens file saved in BART_reddit_media_lifestyle_sports/checkpoint-1500/special_tokens_map.json\n","Deleting older checkpoint [BART_reddit_media_lifestyle_sports/checkpoint-1000] due to args.save_total_limit\n","The following columns in the evaluation set don't have a corresponding argument in `BartForConditionalGeneration.forward` and have been ignored: subreddit_group, subreddit, content, summary. If subreddit_group, subreddit, content, summary are not expected by `BartForConditionalGeneration.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 1000\n","  Batch size = 8\n","Saving model checkpoint to BART_reddit_media_lifestyle_sports/checkpoint-2000\n","Configuration saved in BART_reddit_media_lifestyle_sports/checkpoint-2000/config.json\n","Model weights saved in BART_reddit_media_lifestyle_sports/checkpoint-2000/pytorch_model.bin\n","tokenizer config file saved in BART_reddit_media_lifestyle_sports/checkpoint-2000/tokenizer_config.json\n","Special tokens file saved in BART_reddit_media_lifestyle_sports/checkpoint-2000/special_tokens_map.json\n","Deleting older checkpoint [BART_reddit_media_lifestyle_sports/checkpoint-1500] due to args.save_total_limit\n","Saving model checkpoint to BART_reddit_media_lifestyle_sports/checkpoint-2500\n","Configuration saved in BART_reddit_media_lifestyle_sports/checkpoint-2500/config.json\n","Model weights saved in BART_reddit_media_lifestyle_sports/checkpoint-2500/pytorch_model.bin\n","tokenizer config file saved in BART_reddit_media_lifestyle_sports/checkpoint-2500/tokenizer_config.json\n","Special tokens file saved in BART_reddit_media_lifestyle_sports/checkpoint-2500/special_tokens_map.json\n","Deleting older checkpoint [BART_reddit_media_lifestyle_sports/checkpoint-2000] due to args.save_total_limit\n","Saving model checkpoint to BART_reddit_media_lifestyle_sports/checkpoint-3000\n","Configuration saved in BART_reddit_media_lifestyle_sports/checkpoint-3000/config.json\n","Model weights saved in BART_reddit_media_lifestyle_sports/checkpoint-3000/pytorch_model.bin\n","tokenizer config file saved in BART_reddit_media_lifestyle_sports/checkpoint-3000/tokenizer_config.json\n","Special tokens file saved in BART_reddit_media_lifestyle_sports/checkpoint-3000/special_tokens_map.json\n","Deleting older checkpoint [BART_reddit_media_lifestyle_sports/checkpoint-2500] due to args.save_total_limit\n","Saving model checkpoint to BART_reddit_media_lifestyle_sports/checkpoint-3500\n","Configuration saved in BART_reddit_media_lifestyle_sports/checkpoint-3500/config.json\n","Model weights saved in BART_reddit_media_lifestyle_sports/checkpoint-3500/pytorch_model.bin\n","tokenizer config file saved in BART_reddit_media_lifestyle_sports/checkpoint-3500/tokenizer_config.json\n","Special tokens file saved in BART_reddit_media_lifestyle_sports/checkpoint-3500/special_tokens_map.json\n","Deleting older checkpoint [BART_reddit_media_lifestyle_sports/checkpoint-3000] due to args.save_total_limit\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='5625' max='5625' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [5625/5625 1:12:40, Epoch 3/3]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Rouge1</th>\n","      <th>Rouge2</th>\n","      <th>Rougel</th>\n","      <th>Rougelsum</th>\n","      <th>Gen Len</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>3.654200</td>\n","      <td>3.548437</td>\n","      <td>15.970400</td>\n","      <td>4.288100</td>\n","      <td>13.462100</td>\n","      <td>13.985700</td>\n","      <td>17.876000</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>3.271900</td>\n","      <td>3.540086</td>\n","      <td>15.973300</td>\n","      <td>4.227100</td>\n","      <td>13.440100</td>\n","      <td>14.055000</td>\n","      <td>18.020000</td>\n","    </tr>\n","    <tr>\n","      <td>3</td>\n","      <td>3.030100</td>\n","      <td>3.562253</td>\n","      <td>16.246300</td>\n","      <td>4.329600</td>\n","      <td>13.772500</td>\n","      <td>14.350400</td>\n","      <td>17.760000</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["The following columns in the evaluation set don't have a corresponding argument in `BartForConditionalGeneration.forward` and have been ignored: subreddit_group, subreddit, content, summary. If subreddit_group, subreddit, content, summary are not expected by `BartForConditionalGeneration.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 1000\n","  Batch size = 8\n","Saving model checkpoint to BART_reddit_media_lifestyle_sports/checkpoint-4000\n","Configuration saved in BART_reddit_media_lifestyle_sports/checkpoint-4000/config.json\n","Model weights saved in BART_reddit_media_lifestyle_sports/checkpoint-4000/pytorch_model.bin\n","tokenizer config file saved in BART_reddit_media_lifestyle_sports/checkpoint-4000/tokenizer_config.json\n","Special tokens file saved in BART_reddit_media_lifestyle_sports/checkpoint-4000/special_tokens_map.json\n","Deleting older checkpoint [BART_reddit_media_lifestyle_sports/checkpoint-3500] due to args.save_total_limit\n","Saving model checkpoint to BART_reddit_media_lifestyle_sports/checkpoint-4500\n","Configuration saved in BART_reddit_media_lifestyle_sports/checkpoint-4500/config.json\n","Model weights saved in BART_reddit_media_lifestyle_sports/checkpoint-4500/pytorch_model.bin\n","tokenizer config file saved in BART_reddit_media_lifestyle_sports/checkpoint-4500/tokenizer_config.json\n","Special tokens file saved in BART_reddit_media_lifestyle_sports/checkpoint-4500/special_tokens_map.json\n","Deleting older checkpoint [BART_reddit_media_lifestyle_sports/checkpoint-4000] due to args.save_total_limit\n","Saving model checkpoint to BART_reddit_media_lifestyle_sports/checkpoint-5000\n","Configuration saved in BART_reddit_media_lifestyle_sports/checkpoint-5000/config.json\n","Model weights saved in BART_reddit_media_lifestyle_sports/checkpoint-5000/pytorch_model.bin\n","tokenizer config file saved in BART_reddit_media_lifestyle_sports/checkpoint-5000/tokenizer_config.json\n","Special tokens file saved in BART_reddit_media_lifestyle_sports/checkpoint-5000/special_tokens_map.json\n","Deleting older checkpoint [BART_reddit_media_lifestyle_sports/checkpoint-4500] due to args.save_total_limit\n","Saving model checkpoint to BART_reddit_media_lifestyle_sports/checkpoint-5500\n","Configuration saved in BART_reddit_media_lifestyle_sports/checkpoint-5500/config.json\n","Model weights saved in BART_reddit_media_lifestyle_sports/checkpoint-5500/pytorch_model.bin\n","tokenizer config file saved in BART_reddit_media_lifestyle_sports/checkpoint-5500/tokenizer_config.json\n","Special tokens file saved in BART_reddit_media_lifestyle_sports/checkpoint-5500/special_tokens_map.json\n","Deleting older checkpoint [BART_reddit_media_lifestyle_sports/checkpoint-5000] due to args.save_total_limit\n","The following columns in the evaluation set don't have a corresponding argument in `BartForConditionalGeneration.forward` and have been ignored: subreddit_group, subreddit, content, summary. If subreddit_group, subreddit, content, summary are not expected by `BartForConditionalGeneration.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 1000\n","  Batch size = 8\n","\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n"]},{"output_type":"stream","name":"stdout","text":["CPU times: user 1h 5min 6s, sys: 4min 42s, total: 1h 9min 48s\n","Wall time: 1h 12min 49s\n"]}]},{"cell_type":"code","source":["# create huggingface repo\n","trainer_media.push_to_hub()"],"metadata":{"id":"xzmfDKqjPKca"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Train gaming\n","- 1:13 hrs"],"metadata":{"id":"AWe_p3rDPdWv"}},{"cell_type":"code","source":["%%time\n","\n","args = Seq2SeqTrainingArguments(\n","    f\"BART_reddit_gaming\",\n","    evaluation_strategy = \"epoch\",\n","    learning_rate=2e-5,\n","    per_device_train_batch_size=8, # 16\n","    per_device_eval_batch_size=8, #16\n","    weight_decay=0.01,\n","    save_total_limit=1, #3,\n","    num_train_epochs=3, # 1\n","    predict_with_generate=True,\n","    fp16=True,\n","    push_to_hub=True,\n",")\n","\n","trainer_gaming = Seq2SeqTrainer(\n","    model,\n","    args,\n","    train_dataset=tokenized_datasets[\"train_gaming\"],\n","    eval_dataset=tokenized_datasets[\"valid_gaming\"],\n","    data_collator=data_collator,\n","    tokenizer=tokenizer,\n","    compute_metrics=compute_metrics\n",")\n","\n","# this should save the model to disk. changing wd so that it saves here:\n","fit_path = '/content/gdrive/MyDrive/w266/w266_reddit_summarization/data/model_outputs/bart_fitted_models/round2'\n","os.chdir(fit_path)\n","trainer_gaming.train()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"sjt7hC_kPdGc","executionInfo":{"status":"ok","timestamp":1658421256589,"user_tz":420,"elapsed":4386844,"user":{"displayName":"Trevor Johnson","userId":"10875792761423186902"}},"outputId":"2b4ec6fb-935d-4159-9355-d9c35180e356"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["PyTorch: setting up devices\n","The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n","Cloning https://huggingface.co/trevorj/BART_reddit_gaming into local empty directory.\n","Using cuda_amp half precision backend\n","The following columns in the training set don't have a corresponding argument in `BartForConditionalGeneration.forward` and have been ignored: subreddit_group, content, summary, subreddit. If subreddit_group, content, summary, subreddit are not expected by `BartForConditionalGeneration.forward`,  you can safely ignore this message.\n","/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  FutureWarning,\n","***** Running training *****\n","  Num examples = 15000\n","  Num Epochs = 3\n","  Instantaneous batch size per device = 8\n","  Total train batch size (w. parallel, distributed & accumulation) = 8\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 5625\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='5625' max='5625' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [5625/5625 1:12:57, Epoch 3/3]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Rouge1</th>\n","      <th>Rouge2</th>\n","      <th>Rougel</th>\n","      <th>Rougelsum</th>\n","      <th>Gen Len</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>3.864000</td>\n","      <td>3.775191</td>\n","      <td>17.375400</td>\n","      <td>4.510000</td>\n","      <td>14.676300</td>\n","      <td>15.220000</td>\n","      <td>16.944000</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>3.475500</td>\n","      <td>3.726480</td>\n","      <td>17.806600</td>\n","      <td>4.418800</td>\n","      <td>14.943200</td>\n","      <td>15.539600</td>\n","      <td>18.104000</td>\n","    </tr>\n","    <tr>\n","      <td>3</td>\n","      <td>3.262900</td>\n","      <td>3.737317</td>\n","      <td>18.120200</td>\n","      <td>4.604500</td>\n","      <td>15.127300</td>\n","      <td>15.760100</td>\n","      <td>18.208000</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Saving model checkpoint to BART_reddit_gaming/checkpoint-500\n","Configuration saved in BART_reddit_gaming/checkpoint-500/config.json\n","Model weights saved in BART_reddit_gaming/checkpoint-500/pytorch_model.bin\n","tokenizer config file saved in BART_reddit_gaming/checkpoint-500/tokenizer_config.json\n","Special tokens file saved in BART_reddit_gaming/checkpoint-500/special_tokens_map.json\n","tokenizer config file saved in BART_reddit_gaming/tokenizer_config.json\n","Special tokens file saved in BART_reddit_gaming/special_tokens_map.json\n","Saving model checkpoint to BART_reddit_gaming/checkpoint-1000\n","Configuration saved in BART_reddit_gaming/checkpoint-1000/config.json\n","Model weights saved in BART_reddit_gaming/checkpoint-1000/pytorch_model.bin\n","tokenizer config file saved in BART_reddit_gaming/checkpoint-1000/tokenizer_config.json\n","Special tokens file saved in BART_reddit_gaming/checkpoint-1000/special_tokens_map.json\n","Deleting older checkpoint [BART_reddit_gaming/checkpoint-500] due to args.save_total_limit\n","Saving model checkpoint to BART_reddit_gaming/checkpoint-1500\n","Configuration saved in BART_reddit_gaming/checkpoint-1500/config.json\n","Model weights saved in BART_reddit_gaming/checkpoint-1500/pytorch_model.bin\n","tokenizer config file saved in BART_reddit_gaming/checkpoint-1500/tokenizer_config.json\n","Special tokens file saved in BART_reddit_gaming/checkpoint-1500/special_tokens_map.json\n","Deleting older checkpoint [BART_reddit_gaming/checkpoint-1000] due to args.save_total_limit\n","The following columns in the evaluation set don't have a corresponding argument in `BartForConditionalGeneration.forward` and have been ignored: subreddit_group, content, summary, subreddit. If subreddit_group, content, summary, subreddit are not expected by `BartForConditionalGeneration.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 1000\n","  Batch size = 8\n","Saving model checkpoint to BART_reddit_gaming/checkpoint-2000\n","Configuration saved in BART_reddit_gaming/checkpoint-2000/config.json\n","Model weights saved in BART_reddit_gaming/checkpoint-2000/pytorch_model.bin\n","tokenizer config file saved in BART_reddit_gaming/checkpoint-2000/tokenizer_config.json\n","Special tokens file saved in BART_reddit_gaming/checkpoint-2000/special_tokens_map.json\n","Deleting older checkpoint [BART_reddit_gaming/checkpoint-1500] due to args.save_total_limit\n","Saving model checkpoint to BART_reddit_gaming/checkpoint-2500\n","Configuration saved in BART_reddit_gaming/checkpoint-2500/config.json\n","Model weights saved in BART_reddit_gaming/checkpoint-2500/pytorch_model.bin\n","tokenizer config file saved in BART_reddit_gaming/checkpoint-2500/tokenizer_config.json\n","Special tokens file saved in BART_reddit_gaming/checkpoint-2500/special_tokens_map.json\n","Deleting older checkpoint [BART_reddit_gaming/checkpoint-2000] due to args.save_total_limit\n","Saving model checkpoint to BART_reddit_gaming/checkpoint-3000\n","Configuration saved in BART_reddit_gaming/checkpoint-3000/config.json\n","Model weights saved in BART_reddit_gaming/checkpoint-3000/pytorch_model.bin\n","tokenizer config file saved in BART_reddit_gaming/checkpoint-3000/tokenizer_config.json\n","Special tokens file saved in BART_reddit_gaming/checkpoint-3000/special_tokens_map.json\n","Deleting older checkpoint [BART_reddit_gaming/checkpoint-2500] due to args.save_total_limit\n","Saving model checkpoint to BART_reddit_gaming/checkpoint-3500\n","Configuration saved in BART_reddit_gaming/checkpoint-3500/config.json\n","Model weights saved in BART_reddit_gaming/checkpoint-3500/pytorch_model.bin\n","tokenizer config file saved in BART_reddit_gaming/checkpoint-3500/tokenizer_config.json\n","Special tokens file saved in BART_reddit_gaming/checkpoint-3500/special_tokens_map.json\n","Deleting older checkpoint [BART_reddit_gaming/checkpoint-3000] due to args.save_total_limit\n","The following columns in the evaluation set don't have a corresponding argument in `BartForConditionalGeneration.forward` and have been ignored: subreddit_group, content, summary, subreddit. If subreddit_group, content, summary, subreddit are not expected by `BartForConditionalGeneration.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 1000\n","  Batch size = 8\n","Saving model checkpoint to BART_reddit_gaming/checkpoint-4000\n","Configuration saved in BART_reddit_gaming/checkpoint-4000/config.json\n","Model weights saved in BART_reddit_gaming/checkpoint-4000/pytorch_model.bin\n","tokenizer config file saved in BART_reddit_gaming/checkpoint-4000/tokenizer_config.json\n","Special tokens file saved in BART_reddit_gaming/checkpoint-4000/special_tokens_map.json\n","Deleting older checkpoint [BART_reddit_gaming/checkpoint-3500] due to args.save_total_limit\n","Saving model checkpoint to BART_reddit_gaming/checkpoint-4500\n","Configuration saved in BART_reddit_gaming/checkpoint-4500/config.json\n","Model weights saved in BART_reddit_gaming/checkpoint-4500/pytorch_model.bin\n","tokenizer config file saved in BART_reddit_gaming/checkpoint-4500/tokenizer_config.json\n","Special tokens file saved in BART_reddit_gaming/checkpoint-4500/special_tokens_map.json\n","Deleting older checkpoint [BART_reddit_gaming/checkpoint-4000] due to args.save_total_limit\n","Saving model checkpoint to BART_reddit_gaming/checkpoint-5000\n","Configuration saved in BART_reddit_gaming/checkpoint-5000/config.json\n","Model weights saved in BART_reddit_gaming/checkpoint-5000/pytorch_model.bin\n","tokenizer config file saved in BART_reddit_gaming/checkpoint-5000/tokenizer_config.json\n","Special tokens file saved in BART_reddit_gaming/checkpoint-5000/special_tokens_map.json\n","Deleting older checkpoint [BART_reddit_gaming/checkpoint-4500] due to args.save_total_limit\n","Saving model checkpoint to BART_reddit_gaming/checkpoint-5500\n","Configuration saved in BART_reddit_gaming/checkpoint-5500/config.json\n","Model weights saved in BART_reddit_gaming/checkpoint-5500/pytorch_model.bin\n","tokenizer config file saved in BART_reddit_gaming/checkpoint-5500/tokenizer_config.json\n","Special tokens file saved in BART_reddit_gaming/checkpoint-5500/special_tokens_map.json\n","Deleting older checkpoint [BART_reddit_gaming/checkpoint-5000] due to args.save_total_limit\n","The following columns in the evaluation set don't have a corresponding argument in `BartForConditionalGeneration.forward` and have been ignored: subreddit_group, content, summary, subreddit. If subreddit_group, content, summary, subreddit are not expected by `BartForConditionalGeneration.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 1000\n","  Batch size = 8\n","\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n"]},{"output_type":"stream","name":"stdout","text":["CPU times: user 1h 5min 19s, sys: 5min 48s, total: 1h 11min 8s\n","Wall time: 1h 13min 6s\n"]}]},{"cell_type":"code","source":["# create huggingface repo\n","trainer_gaming.push_to_hub()"],"metadata":{"id":"rKmWPYgTP4QS","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1658421906098,"user_tz":420,"elapsed":5307,"user":{"displayName":"Trevor Johnson","userId":"10875792761423186902"}},"outputId":"3991a711-005a-4499-9024-fc366db36d0c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["Saving model checkpoint to BART_reddit_gaming\n","Configuration saved in BART_reddit_gaming/config.json\n","Model weights saved in BART_reddit_gaming/pytorch_model.bin\n","tokenizer config file saved in BART_reddit_gaming/tokenizer_config.json\n","Special tokens file saved in BART_reddit_gaming/special_tokens_map.json\n","Dropping the following result as it does not have all the necessary fields:\n","{'task': {'name': 'Sequence-to-sequence Language Modeling', 'type': 'text2text-generation'}, 'metrics': [{'name': 'Rouge1', 'type': 'rouge', 'value': 18.1202}]}\n"]}]},{"cell_type":"markdown","source":["# Train other\n","- 1:17 hrs"],"metadata":{"id":"2oZWzrpVPkWa"}},{"cell_type":"code","source":["%%time\n","\n","args = Seq2SeqTrainingArguments(\n","    f\"BART_reddit_other\",\n","    evaluation_strategy = \"epoch\",\n","    learning_rate=2e-5,\n","    per_device_train_batch_size=8, # 16\n","    per_device_eval_batch_size=8, #16\n","    weight_decay=0.01,\n","    save_total_limit=1, #3,\n","    num_train_epochs=3, # 1\n","    predict_with_generate=True,\n","    fp16=True,\n","    push_to_hub=True,\n",")\n","\n","trainer_other = Seq2SeqTrainer(\n","    model,\n","    args,\n","    train_dataset=tokenized_datasets[\"train_other\"],\n","    eval_dataset=tokenized_datasets[\"valid_other\"],\n","    data_collator=data_collator,\n","    tokenizer=tokenizer,\n","    compute_metrics=compute_metrics\n",")\n","\n","# this should save the model to disk. changing wd so that it saves here:\n","fit_path = '/content/gdrive/MyDrive/w266/w266_reddit_summarization/data/model_outputs/bart_fitted_models/round2'\n","os.chdir(fit_path)\n","trainer_other.train()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"7yAnIoWtPk8b","executionInfo":{"status":"ok","timestamp":1658426973253,"user_tz":420,"elapsed":4804173,"user":{"displayName":"Trevor Johnson","userId":"10875792761423186902"}},"outputId":"653f9ac5-a634-4be3-947a-49fefaeb1d4f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["Cloning https://huggingface.co/trevorj/BART_reddit_other into local empty directory.\n","Using cuda_amp half precision backend\n","The following columns in the training set don't have a corresponding argument in `BartForConditionalGeneration.forward` and have been ignored: content, subreddit_group, subreddit, summary. If content, subreddit_group, subreddit, summary are not expected by `BartForConditionalGeneration.forward`,  you can safely ignore this message.\n","/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  FutureWarning,\n","***** Running training *****\n","  Num examples = 15000\n","  Num Epochs = 3\n","  Instantaneous batch size per device = 8\n","  Total train batch size (w. parallel, distributed & accumulation) = 8\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 5625\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='5625' max='5625' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [5625/5625 1:19:49, Epoch 3/3]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Rouge1</th>\n","      <th>Rouge2</th>\n","      <th>Rougel</th>\n","      <th>Rougelsum</th>\n","      <th>Gen Len</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>3.788700</td>\n","      <td>3.604443</td>\n","      <td>18.466800</td>\n","      <td>5.182000</td>\n","      <td>15.359000</td>\n","      <td>16.169000</td>\n","      <td>19.341000</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>3.381600</td>\n","      <td>3.562785</td>\n","      <td>18.099800</td>\n","      <td>4.893700</td>\n","      <td>15.017900</td>\n","      <td>15.761500</td>\n","      <td>17.789000</td>\n","    </tr>\n","    <tr>\n","      <td>3</td>\n","      <td>3.134000</td>\n","      <td>3.579218</td>\n","      <td>18.570500</td>\n","      <td>5.010700</td>\n","      <td>15.258100</td>\n","      <td>16.082000</td>\n","      <td>19.402000</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Saving model checkpoint to BART_reddit_other/checkpoint-500\n","Configuration saved in BART_reddit_other/checkpoint-500/config.json\n","Model weights saved in BART_reddit_other/checkpoint-500/pytorch_model.bin\n","tokenizer config file saved in BART_reddit_other/checkpoint-500/tokenizer_config.json\n","Special tokens file saved in BART_reddit_other/checkpoint-500/special_tokens_map.json\n","tokenizer config file saved in BART_reddit_other/tokenizer_config.json\n","Special tokens file saved in BART_reddit_other/special_tokens_map.json\n","Saving model checkpoint to BART_reddit_other/checkpoint-1000\n","Configuration saved in BART_reddit_other/checkpoint-1000/config.json\n","Model weights saved in BART_reddit_other/checkpoint-1000/pytorch_model.bin\n","tokenizer config file saved in BART_reddit_other/checkpoint-1000/tokenizer_config.json\n","Special tokens file saved in BART_reddit_other/checkpoint-1000/special_tokens_map.json\n","Deleting older checkpoint [BART_reddit_other/checkpoint-500] due to args.save_total_limit\n","Saving model checkpoint to BART_reddit_other/checkpoint-1500\n","Configuration saved in BART_reddit_other/checkpoint-1500/config.json\n","Model weights saved in BART_reddit_other/checkpoint-1500/pytorch_model.bin\n","tokenizer config file saved in BART_reddit_other/checkpoint-1500/tokenizer_config.json\n","Special tokens file saved in BART_reddit_other/checkpoint-1500/special_tokens_map.json\n","Deleting older checkpoint [BART_reddit_other/checkpoint-1000] due to args.save_total_limit\n","The following columns in the evaluation set don't have a corresponding argument in `BartForConditionalGeneration.forward` and have been ignored: content, subreddit_group, subreddit, summary. If content, subreddit_group, subreddit, summary are not expected by `BartForConditionalGeneration.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 1000\n","  Batch size = 8\n","Saving model checkpoint to BART_reddit_other/checkpoint-2000\n","Configuration saved in BART_reddit_other/checkpoint-2000/config.json\n","Model weights saved in BART_reddit_other/checkpoint-2000/pytorch_model.bin\n","tokenizer config file saved in BART_reddit_other/checkpoint-2000/tokenizer_config.json\n","Special tokens file saved in BART_reddit_other/checkpoint-2000/special_tokens_map.json\n","Deleting older checkpoint [BART_reddit_other/checkpoint-1500] due to args.save_total_limit\n","Saving model checkpoint to BART_reddit_other/checkpoint-2500\n","Configuration saved in BART_reddit_other/checkpoint-2500/config.json\n","Model weights saved in BART_reddit_other/checkpoint-2500/pytorch_model.bin\n","tokenizer config file saved in BART_reddit_other/checkpoint-2500/tokenizer_config.json\n","Special tokens file saved in BART_reddit_other/checkpoint-2500/special_tokens_map.json\n","Deleting older checkpoint [BART_reddit_other/checkpoint-2000] due to args.save_total_limit\n","Saving model checkpoint to BART_reddit_other/checkpoint-3000\n","Configuration saved in BART_reddit_other/checkpoint-3000/config.json\n","Model weights saved in BART_reddit_other/checkpoint-3000/pytorch_model.bin\n","tokenizer config file saved in BART_reddit_other/checkpoint-3000/tokenizer_config.json\n","Special tokens file saved in BART_reddit_other/checkpoint-3000/special_tokens_map.json\n","Deleting older checkpoint [BART_reddit_other/checkpoint-2500] due to args.save_total_limit\n","Saving model checkpoint to BART_reddit_other/checkpoint-3500\n","Configuration saved in BART_reddit_other/checkpoint-3500/config.json\n","Model weights saved in BART_reddit_other/checkpoint-3500/pytorch_model.bin\n","tokenizer config file saved in BART_reddit_other/checkpoint-3500/tokenizer_config.json\n","Special tokens file saved in BART_reddit_other/checkpoint-3500/special_tokens_map.json\n","Deleting older checkpoint [BART_reddit_other/checkpoint-3000] due to args.save_total_limit\n","The following columns in the evaluation set don't have a corresponding argument in `BartForConditionalGeneration.forward` and have been ignored: content, subreddit_group, subreddit, summary. If content, subreddit_group, subreddit, summary are not expected by `BartForConditionalGeneration.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 1000\n","  Batch size = 8\n","Saving model checkpoint to BART_reddit_other/checkpoint-4000\n","Configuration saved in BART_reddit_other/checkpoint-4000/config.json\n","Model weights saved in BART_reddit_other/checkpoint-4000/pytorch_model.bin\n","tokenizer config file saved in BART_reddit_other/checkpoint-4000/tokenizer_config.json\n","Special tokens file saved in BART_reddit_other/checkpoint-4000/special_tokens_map.json\n","Deleting older checkpoint [BART_reddit_other/checkpoint-3500] due to args.save_total_limit\n","Saving model checkpoint to BART_reddit_other/checkpoint-4500\n","Configuration saved in BART_reddit_other/checkpoint-4500/config.json\n","Model weights saved in BART_reddit_other/checkpoint-4500/pytorch_model.bin\n","tokenizer config file saved in BART_reddit_other/checkpoint-4500/tokenizer_config.json\n","Special tokens file saved in BART_reddit_other/checkpoint-4500/special_tokens_map.json\n","Deleting older checkpoint [BART_reddit_other/checkpoint-4000] due to args.save_total_limit\n","Saving model checkpoint to BART_reddit_other/checkpoint-5000\n","Configuration saved in BART_reddit_other/checkpoint-5000/config.json\n","Model weights saved in BART_reddit_other/checkpoint-5000/pytorch_model.bin\n","tokenizer config file saved in BART_reddit_other/checkpoint-5000/tokenizer_config.json\n","Special tokens file saved in BART_reddit_other/checkpoint-5000/special_tokens_map.json\n","Deleting older checkpoint [BART_reddit_other/checkpoint-4500] due to args.save_total_limit\n","Saving model checkpoint to BART_reddit_other/checkpoint-5500\n","Configuration saved in BART_reddit_other/checkpoint-5500/config.json\n","Model weights saved in BART_reddit_other/checkpoint-5500/pytorch_model.bin\n","tokenizer config file saved in BART_reddit_other/checkpoint-5500/tokenizer_config.json\n","Special tokens file saved in BART_reddit_other/checkpoint-5500/special_tokens_map.json\n","Deleting older checkpoint [BART_reddit_other/checkpoint-5000] due to args.save_total_limit\n","The following columns in the evaluation set don't have a corresponding argument in `BartForConditionalGeneration.forward` and have been ignored: content, subreddit_group, subreddit, summary. If content, subreddit_group, subreddit, summary are not expected by `BartForConditionalGeneration.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 1000\n","  Batch size = 8\n","\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n"]},{"output_type":"stream","name":"stdout","text":["CPU times: user 1h 11min 53s, sys: 5min 57s, total: 1h 17min 50s\n","Wall time: 1h 20min 4s\n"]}]},{"cell_type":"code","source":["# create huggingface repo\n","trainer_other.push_to_hub()"],"metadata":{"id":"HhgkyHmQP5s7","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1658426978336,"user_tz":420,"elapsed":5086,"user":{"displayName":"Trevor Johnson","userId":"10875792761423186902"}},"outputId":"d8b32b5d-f00c-4951-94aa-8ebc87b438bb"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["Saving model checkpoint to BART_reddit_other\n","Configuration saved in BART_reddit_other/config.json\n","Model weights saved in BART_reddit_other/pytorch_model.bin\n","tokenizer config file saved in BART_reddit_other/tokenizer_config.json\n","Special tokens file saved in BART_reddit_other/special_tokens_map.json\n","Dropping the following result as it does not have all the necessary fields:\n","{'task': {'name': 'Sequence-to-sequence Language Modeling', 'type': 'text2text-generation'}, 'metrics': [{'name': 'Rouge1', 'type': 'rouge', 'value': 18.5705}]}\n"]}]},{"cell_type":"code","source":[""],"metadata":{"id":"TNAodjWeQCx6"},"execution_count":null,"outputs":[]}]}