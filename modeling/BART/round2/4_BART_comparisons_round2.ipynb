{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"4_BART_comparisons_round2.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyNYJ1saYBCVoJmYwB78xB6f"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Compare metrics between the models"],"metadata":{"id":"GOCLkBdHUlx8"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"z-n0CLDAUixY"},"outputs":[],"source":["from IPython.display import clear_output\n","\n","!pip install datasets transformers rouge_score rouge-score nltk\n","# rouge-score is the google version\n","!pip install pyarrow\n","!pip install -q sentencepiece\n","\n","clear_output()"]},{"cell_type":"code","source":["import os\n","import re\n","import time\n","from tqdm import tqdm\n","import pandas as pd\n","import numpy as np\n","from pprint import pprint\n","import matplotlib.pyplot as plt\n","\n","# nlp stuff\n","import nltk\n","nltk.download('punkt')\n","\n","# tf stuff\n","import tensorflow_datasets as tfds \n","import tensorflow as tf\n","from transformers import PegasusTokenizer, TFPegasusForConditionalGeneration # pegasus\n","from transformers import BartTokenizer, TFBartForConditionalGeneration # bart\n","\n","# pytorch dataset types\n","import datasets\n","from datasets.dataset_dict import DatasetDict\n","from datasets import Dataset, load_metric, load_dataset\n","\n","# pytorch bart stuff\n","import torch\n","from transformers import AutoModelForSeq2SeqLM, DataCollatorForSeq2Seq, Seq2SeqTrainingArguments, Seq2SeqTrainer\n","from transformers import AutoTokenizer\n","\n","clear_output()"],"metadata":{"id":"FZFTPxtRUrj9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["%%time\n","\n","# specify your path to the repo here:\n","repo_path = '/content/gdrive/MyDrive/w266/w266_reddit_summarization'\n","\n","from google.colab import drive\n","drive.mount('/content/gdrive')\n","\n","# baseline bart\n","baseline_preds = pd.read_parquet(os.path.join(repo_path, 'data/model_outputs/bart_preds/round2/bart_baseline_preds.parquet'))\n","\n","# finetuned bart\n","finetuned_preds = pd.concat([\n","  pd.read_parquet(os.path.join(repo_path, 'data/model_outputs/bart_preds/round2/bart_preds_advice.parquet')),\n","  pd.read_parquet(os.path.join(repo_path, 'data/model_outputs/bart_preds/round2/bart_preds_media.parquet')),\n","  pd.read_parquet(os.path.join(repo_path, 'data/model_outputs/bart_preds/round2/bart_preds_gaming.parquet')),\n","  pd.read_parquet(os.path.join(repo_path, 'data/model_outputs/bart_preds/round2/bart_preds_other.parquet'))], ignore_index=True)\n"],"metadata":{"id":"gMEdS4VXUsCC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# join them\n","baseline_preds.columns = ['content', 'y', 'yhat_baseline']\n","finetuned_preds.columns = ['content', 'y', 'yhat_finetune']\n","\n","all_preds = pd.merge(finetuned_preds, baseline_preds, on = ['content', 'y'], how='left')"],"metadata":{"id":"V5NvYAC2V7LX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# compute metrics\n","baseline_metrics = metric.compute(predictions=all_preds['yhat_baseline'].tolist(), references=all_preds['y'].tolist())\n","finetune_metrics = metric.compute(predictions=all_preds['yhat_finetune'].tolist(), references=all_preds['y'].tolist())\n","\n","print(\"Baseline:\")\n","print(baseline_metrics)\n","\n","print(\"\\n\\nBaseline:\")\n","print(finetune_metrics)"],"metadata":{"id":"iN_C1f4PVhEz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# compute metrics for each genre"],"metadata":{"id":"nKqYbqsiV2Xl"},"execution_count":null,"outputs":[]}]}